var relearn_searchindex = [
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "Blogs How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR)",
    "description": "Blogs How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR)",
    "tags": [],
    "title": "My Blogs",
    "uri": "/blogs/index.html"
  },
  {
    "breadcrumb": "",
    "content": "My Engineering Notes and Blogs Welcome on my Engineering Notes and Blogs site!\nMy Blogs How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR) My Notes Apache Kafka (WIP) Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured Lombok Java Library MongoDB Python and PyTest TypeScript",
    "description": "My Engineering Notes and Blogs Welcome on my Engineering Notes and Blogs site!\nMy Blogs How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR) My Notes Apache Kafka (WIP) Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured Lombok Java Library MongoDB Python and PyTest TypeScript",
    "tags": [],
    "title": "Alain Bouchard's Engineering Notes and Blogs",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "My Engineering Notes Apache Kafka (WIP) Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured Lombok Java Library MongoDB Python and PyTest TypeScript",
    "description": "My Engineering Notes Apache Kafka (WIP) Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured Lombok Java Library MongoDB Python and PyTest TypeScript",
    "tags": [],
    "title": "My Notes",
    "uri": "/notes/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Warning 2022-07-11: This is a WORK IN PROGRESS document (WIP) and need to be reviewed.\nApache Kafka Topics, partitions and offsets Brokers Brokers and topics Topic replication factor Producers Consumers Consumer offsets Kafka broker discovery Zookeeper Kafka guarantees Install Kafka using Docker images Use Kafka Topics CLI Topics CLI Kafka console producer Kafka console consumer Kafka-Client with Java Kafka Connect and Kafka Stream Apache Kafka Why Apache Kafka?\nCreated by LinkedIn, now Open Source Project Maintained by Confluent (Apache Stewardship) Distributedm resilient architecture, fault tolerant scales Horizontaly (100s of borkers, millions of messages per seconds, etc.) Use Cases?\nMessaging system Activity tracking Gather metrics from many different locations Application logs gathering Stream processing (e.g., kafka stream API, Apache Spark, etc.) De-coupliung system dependicies Integration with Spark, Flink, Storm, Hadoop, and other big Data technologies Topics, partitions and offsets Topics: a particular stream of data\nsimilar to a table in a DB (but without the constraints) you can have as many topics as you want a topic is identified by its name Partitions: spliting topics\neach partiction is orderied each message within a partiction gets an incremental id, called offset | Partition 0 -\u003e offset: | 0 | 1 | 2 | 3 | 4 | 5 | ... | Kafka Topic + Partition 1 -\u003e offset: | 0 | 1 | 2 | ... writes | | Partition 2 -\u003e offset: | 0 | 1 | 2 | 3 | 4 | ... offset only have a meaning for a specific partiction (e.g., ofsset 3 in partition 0 != offset 3 in partition 1) order is guaranteed only within a partition (not acresoo partitions) data is kept only for a limited time (default is one week) once the data is written to a partition, it can’t be changed (immutability) data is assigned randomly to a partition unless a key is provided (more on this later) Brokers a kafka cluster in composed of multiple brokers (broker = servers) each broker is identified with its ID (integer) each broker contains certain topic partitions after connecting to any broker (called a bootstrap broker), you will be connected to the intire cluster a good number to get started is 3 brokers, but may go over 100 brokers Brokers and topics Example of Topic-A with 3 partitions Exmaple of Topic-B with 2 partitions Broker 101 Broker 102 Broker 103 Topic A partition 0 Topic A partition 2 Topic A partition 1 Topic A partition 1 Topic A partition 0 Topic replication factor topics should have a replication factor \u003e 1 (usually between 2 and 3) this way if a broker is down, another broker can serve the data example: topic-A with 2 partitions and replications factor of 2 Broker 101 Broker 102 Broker 103 Topic-A Partition 0 Topic-A Partition 1 Topic-A Partition 1 Topic-A Partition 0 example: broker 102 goes down; broker 101 and 103 still up, both partitions still work at any time only one broker can be a leader for a given partition only that leader can receive and serve data for a partition the other brokers will synchronize the data therefore each partion has one leader and multiple ISR (in-sync replica) Producers producers write data to topics (which is made of partitions) producers automatically know to which broker and partition to write to in case of broker failures, producers will automatically recover producers can choose to receive acknowledgement of data writes acks=0: producers won’t wait for acknowledgment (possible data loss) (default) acks=1: producer will wait for leader acknowledgment (limited data loss) acks=all: leaders and replicas acknowledgment (no data loss) Message keys producers can choose to send a key with the message (string, number, etc.) if key=null, data is sent round robin (broker 101, 102 then 103, and 101 again…) if a key is sent, then all messages for that key will always goto the same partition a key is basically sent if you need a message ordering for specific field (e.g., truck_id, etc.) we get this guarantee due to key hashing, which depends on the number of partitions Consumers consumers read data from a tipic (identified by name) consumers know which broker to read from in case of broker failures, consumer know how to recover data is read in order within each partitions Consumers groups consumers read data in consumer groups each consumer within a group reads from exclusinve partitions if youy have more consumer than partitions, some consumers with be icative if you have more consumers than partitions, some consumers will be inactive Consumer offsets kafka stores the offsets at which a consumer group has beed reading the offset committed live in a kafka topic named __consumer_offsets when a consumer in a group has processed data received from kafka, it should be committing the offsets if a consumer dies, it will be able to read back from where it left off (due to the committed consumer offsets) Delivery semantics for consumers consumers choose when to commit offsets there are 3 delivery semantics at most once: offsets are committed as soon as the message is received offsets are commited as soon as the message is received if the processing goes wring, the message will be lost (it won’t be read again) at least once (usually preferred) offsets are committed after the message is processed if the processing goes wring, the message will be read again this can reults in duplicate processing of messages, so make sure the processing is idempotent exactly once can be achieved for kafta-to-kafka workflows using kafka streams API for kafka-to-external-system workflows, it requires the consumer to be idempotent Kafka broker discovery every kafka broker is also called a bootstrap server that means that you only need to connect to one broker and you will be connected to the entire cluster each broker knows about all brokers, topics and partitions (metadata) Zookeeper zookeeper manages brokers (keeps a list of them) zookeeper helps in performing leader election partitions zookeeper sends notifications to kafka in case of changes (e.g., new topic, broker dies, brker comes up, delete topics, etc.) kafka can’t work without zookeeper zookeeper by design operates with an odd number of servers (3, 5, 7, …) zookeeper has a leader (handle writes) and the rest if the servers are followers (handle reads) zookeeper does not store consumer offsets with kafka \u003e v0.10 Kafka guarantees Messages are appended to a topic-partition in othe order they are sent consumers read messages in the order stored in a topic-partition with a replication factor of N, producers and consumers can tolerate up to N-1 brokers being down this is why a replicator factor of 4 is a good idea: allows for one broker to be taken down for maintenance allows for another broker to be taken down unexpectedly as long as the number of partitions remains constant for a topic (no new partitions), the same key will always go to the same partition (i.e., hashed key) Install Kafka using Docker images Create the docker-compose.yaml file:\nversion: '3.5' services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 ports: - 22181:2181 kafka: image: confluentinc/cp-kafka:latest depends_on: - zookeeper ports: - 29092:29092 environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 From the CLI:\n\u003e docker-compose up -d \u003e docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ab09b5c8bc7b confluentinc/cp-kafka:latest \"/etc/confluent/dock…\" 11 minutes ago Up 11 minutes 9092/tcp, 0.0.0.0:29092-\u003e29092/tcp kafka-cluster_kafka_1 8e7725b7874b confluentinc/cp-zookeeper:latest \"/etc/confluent/dock…\" 11 minutes ago Up 11 minutes 2888/tcp, 3888/tcp, 0.0.0.0:22181-\u003e2181/tcp kafka-cluster_zookeeper_1 \u003e $ netstat -aon | grep 22181 TCP 0.0.0.0:22181 0.0.0.0:0 LISTENING 23792 TCP [::]:22181 [::]:0 LISTENING 23792 TCP [::1]:22181 [::]:0 LISTENING 27612 \u003e netstat -aon | grep 29092 TCP 0.0.0.0:29092 0.0.0.0:0 LISTENING 23792 TCP [::]:29092 [::]:0 LISTENING 23792 TCP [::1]:29092 [::]:0 LISTENING 27612 \u003e docker-compose logs kafka | grep -i started kafka_1 | [2022-05-31 14:49:34,438] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 14:49:34,441] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 14:49:34,445] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,450] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,450] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started socket server acceptors and processors (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,459] INFO [KafkaServer id=1] started (kafka.server.KafkaServer) kafka_1 | [2022-05-31 18:51:50,791] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 18:51:50,791] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 19:52:52,922] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 19:52:52,923] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 21:02:51,738] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 21:02:51,739] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 23:24:34,226] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 23:24:34,226] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) Note: nc -z localhost \u003cport\u003e can be used on MacOS and linux.\nFollow the Guide to Setting Up Apache Kafka Using Docker instructions.\nInstall the Kafka Offset Explorer to connect to Kafka Cluster and make sure to configure the bootstrap server: localhost:29092\nUse Kafka Topics CLI Use docker to execute the kafka-topics command:\n\u003e docker-compose ps Name Command State Ports ----------------------------------------------------------------------------------------------------------- kafka-cluster_kafka_1 /etc/confluent/docker/run Up 0.0.0.0:29092-\u003e29092/tcp, 9092/tcp kafka-cluster_zookeeper_1 /etc/confluent/docker/run Up 0.0.0.0:22181-\u003e2181/tcp, 2888/tcp, 3888/tcp \u003e docker exec -t kafka-cluster_kafka_1 kafka-topics Create, delete, describe, or change a topic. Option Description ------ ----------- --alter Alter the number of partitions, replica assignment, and/or configuration for the topic. . . . To use Kafka Container shell:\n\u003e docker exec -it kafka-cluster_kafka_1 sh sh-4.4$ Topics CLI Reference: Apache Kafka CLI commands cheat sheet\nCreating a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --create --topic first_topic --partitions 3 --replication-factor 1 WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic first_topic. sh-4.4$ Note: the --bootstrap-server localhost:9092 replaces the kafka-topics command --zookeeper localhost:9092\nListing current topics sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first_topic sh-4.4$ Describe a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --describe --topic first_topic Topic: first_topic TopicId: vQ6o8fX1Sx-qNQuUQ5vbAg PartitionCount: 3 ReplicationFactor: 1 Configs: Topic: first_topic Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: first_topic Partition: 1 Leader: 1 Replicas: 1 Isr: 1 Topic: first_topic Partition: 2 Leader: 1 Replicas: 1 Isr: 1 sh-4.4$ Delete a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first-topic first_topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --delete --topic first-topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first_topic sh-4.4$ Kafka console producer Create messages Creating 4 messages using Kafka Broker using the kafka-console-producer command. The CTRL-C command will make the kafka-console-producer command to stop.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic \u003emessage1 \u003emessage2 \u003emessage3 \u003emessage4 \u003e^C sh-4.4$ Create messages in a non-existing topic It is possible to create a new topic on-the-fly when adding but it is not recommended since the default values for both PartitionCount and ReplicationFactor are set to 1. Best practices require more partitions and replications.\nDefault replication value can be changed from /etc/kafka/server.properties value num.partitions, default is 1.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic new_topic \u003eThis is a message to a new topic [2022-06-02 18:56:53,185] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 3 : {new_topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient) \u003eanother message sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first_topic new_topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --topic new_topic --describe Topic: new_topic TopicId: FuC1JLRETNCrgUWaEPZCOg PartitionCount: 1 ReplicationFactor: 1 Configs: Topic: new_topic Partition: 0 Leader: 1 Replicas: 1 Isr: 1 sh-4.4$ Change the producer-property Changing the acks property. Refer to producer above sections for more information about acks=all property.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic --producer-property acks=all \u003eacks message1 \u003e^C sh-4.4$ Kafka console consumer Consuming messages from a topic The kafka-console-consumer won’t consume topic messages from offset=0 by default to avoid consuming millions of existing message. It will consume the upcomming messages only (from now on). To get all messages from offest:0, the --from-beginning option must be specified.\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --from-beginning message2 acks message1 message3 message1 message4 The order isn’t garanteed when the number of topic partitions is greater than 1, as explained in Topics, partitions and offsets section.\nConsuming messages from a topic with groups The --group define a group of kafka consumers that will share the consumption load for one given topic.\nThe kafka-console-producer example:\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic \u003epatate \u003ecarotte \u003epomme \u003eorange \u003e The first kafka-console-consumer example:\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-group carotte orange The second kafka-console-consumer example:\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-group patate pomme Consumer groups command Get all the available consumer groups:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --list my-group sh-4.4$ Get consumer group information:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 3 3 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 1 2 2 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 2 4 4 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer sh-4.4$ Consumer groups: reseting offsets All messages are currently consumed: current-offsets=log-end-offsets:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 3 3 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 1 2 2 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 2 4 4 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer sh-4.4$ Reseting the current-offsets to 0 using --to-earliest option:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --reset-offsets --to-earliest --execute --topic first_topic GROUP TOPIC PARTITION NEW-OFFSET my-group first_topic 0 0 my-group first_topic 1 0 my-group first_topic 2 0 sh-4.4$ Verify the group offsets is not 0:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group Consumer group 'my-group' has no active members. GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 0 3 3 - - - my-group first_topic 1 0 2 2 - - - my-group first_topic 2 0 4 4 - - - sh-4.4$ Shift the offset by 1 using --shift-by option:\nkafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --reset-offsets --shift-by 1 -- execute --topic first_topic GROUP TOPIC PARTITION NEW-OFFSET my-group first_topic 0 1 my-group first_topic 1 1 my-group first_topic 2 1 sh-4.4$ Available --reset-offsets options are :\n--to-datetime --by-period --to-earliest --to-latest --shift-by --from-file --to-current Kafka-Client with Java Use Kafka Documentation as much as possible.\nCreate Java project Using IntelliJ:\ncreate a project search for kafka-client in https://mvnrepository.com/ select the desired version (i.e., the following will use 3.2.0) copy the Gradle implementation information in the dependencies section of the build.gradle file download the dependencies using IntelliJ Gradle window -\u003e Reload... button repeat same steps for slf4j-simple package and set scope to implementation instead of testImplementation create a new package, e.g., com.github.alainbouchard.kafka-demo.demo1 Create a simple producer The following are needed when creating a producer:\nkafka producer properties (or Properties object) kafka producter creation Send data (verification) Close the producer Reference for Kafka Producer Properties.\npublic class KafkaProducerDemo { public static void main(String[] args) { Properties properties = new Properties(); // Set Producer Properties properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\"); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // Create Kafka Producer KafkaProducer\u003cString, String\u003e producer = new KafkaProducer\u003cString, String\u003e(properties); ProducerRecord\u003cString, String\u003e record = new ProducerRecord\u003c\u003e(\"first_topic\", \"hello world from Java!\"); // Send data producer.send(record); // asynchronous, need to flush data producer.flush(); // Tear down Producer producer.close(); } } Create a producer with a callback public class KafkaProducerDemoWithCallback { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(KafkaProducerDemoWithCallback.class); Properties properties = new Properties(); // Set Producer Properties properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\"); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // Create Kafka Producer KafkaProducer\u003cString, String\u003e producer = new KafkaProducer\u003cString, String\u003e(properties); ProducerRecord\u003cString, String\u003e record = new ProducerRecord\u003c\u003e(\"first_topic\", \"hello world from Java!\"); // Send data producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { // execute everytime a message is sent OR an exception is thrown. if (exception == null) { // successfully sent message logger.info(\"Received metadata - Topic: \" + metadata.topic() + \" Partition: \" + metadata.partition() + \" Offset: \" + metadata.offset() + \" Timestamp: \" + metadata.timestamp()); } else { logger.error(\"Error while producer sent a message\", exception); } } }); // asynchronous, need to flush data producer.flush(); // Tear down Producer producer.close(); } } Create a simple consumer The following are needed when creating a consumer:\nkafka consumer properties (or Properties object) kafka consumer creation poll data in a loop Reference for Kafka Consumer Properties.\npublic class KafkaConsumerDemo { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(KafkaConsumerDemo.class.getName()); // Configure the consumer Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\"); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, \"my_group\"); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest // Create consumer KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003cString, String\u003e(properties); // Subscribe the consumer to the Topic or Topics consumer.subscribe(Arrays.asList(\"first_topic\")); // Poll for new data while(true) { // Avoid in production - demo purpose only. ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100)); records.forEach(r -\u003e logger.info(\"Key: \" + r.key() + \" Value: \" + r.value() + \" Partition: \" + r.partition() + \" Offset: \" + r.offset() + \" Timestamp: \" + r.timestamp())); } } } Create a consumer in a thread Note: It is only working if the application does stop gracefully. A Break/Kill signal won’t trigger the shutdown properly. IntelliJ don’t have the exit button on Windows.\npublic class KafkaConsumerWithThreadsDemo { Logger logger = LoggerFactory.getLogger(KafkaConsumerWithThreadsDemo.class.getName()); public KafkaConsumerWithThreadsDemo() { } public void run() { String bootstrapServer = \"localhost:29092\"; String groupId = \"my_group\"; String topic = \"first_topic\"; // Latch for dealing with multiple threads; CountDownLatch latch = new CountDownLatch(1); // Create Consumer Runnable; Runnable consumerRunnable = new ConsumerRunnable(bootstrapServer, groupId, topic, latch); // Starting Consumer Runnable Thread; Thread thread = new Thread(consumerRunnable); thread.start(); // Add a shutdown hook; Runtime.getRuntime().addShutdownHook(new Thread( () -\u003e { logger.info(\"Received a shutdown hook...\"); ((ConsumerRunnable) consumerRunnable).shutdown(); try { latch.await(); } catch (InterruptedException e) { throw new RuntimeException(e); } logger.info(\"Consumer application has exited...\"); })); try { latch.await(); } catch (InterruptedException e) { logger.error(\"Consumer application got interrupted\", e); } finally { logger.info(\"Consumer application is closing...\"); } } public class ConsumerRunnable implements Runnable { private CountDownLatch latch; KafkaConsumer\u003cString, String\u003e consumer; public ConsumerRunnable(String bootstrapServer, String groupId, String topic, CountDownLatch latch) { this.latch = latch; // Configure the consumer Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest // Create consumer consumer = new KafkaConsumer\u003cString, String\u003e(properties); // Subscribe the consumer to the Topic or Topics consumer.subscribe(Arrays.asList(topic)); } @Override public void run() { // Poll for new data try { while (true) { // Avoid in production - demo purpose only. ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100)); records.forEach(r -\u003e logger.info(\"Key: \" + r.key() + \" Value: \" + r.value() + \" Partition: \" + r.partition() + \" Offset: \" + r.offset() + \" Timestamp: \" + r.timestamp())); } } catch ( WakeupException exception) { logger.info(\"Received shutdown signal...\"); } finally { consumer.close(); latch.countDown(); // telling caller code that this thread is done. } } public void shutdown() { // to interrupt the consumer.poll() method // and will make consumer.poll() to throw an exception WakeupException consumer.wakeup(); } } public static void main(String[] args) { new KafkaConsumerWithThreadsDemo().run(); } } Assign and seek consumer The Assign and Seek is mostly used to replay data or fetch a specific message.\npublic class KafkaConsumerWithAssignAndSeekDemo { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(KafkaConsumerWithAssignAndSeekDemo.class.getName()); // Configure the consumer Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\"); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, \"my_group\"); No needs for group with assign and seek... properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest // Create consumer KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003cString, String\u003e(properties); // The Assign and Seek is mostly used to replay data or fetch a specific message. // Assign TopicPartition partition = new TopicPartition(\"first_topic\", 0); consumer.assign(Arrays.asList(partition)); // Seek consumer.seek(partition, 1L); // Poll for new data while(true) { // Avoid in production - demo purpose only. ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100)); records.forEach(r -\u003e logger.info(\"Key: \" + r.key() + \" Value: \" + r.value() + \" Partition: \" + r.partition() + \" Offset: \" + r.offset() + \" Timestamp: \" + r.timestamp())); } } } Kafka Bidirectional Compatibility As Kafka 0.10.2 (July 2017), the client and brokers hava a vapability called bi-directional compatibility (because API calls are nov versioned).\nIt means that the latest client library version should always be used as documented in the confluent documentation Upgrading Apache Kafka Clients Just Got Easier\nCreating a Producer for Twitter messages Needed packages:\ndependencies { implementation group: 'com.twitter', name: 'twitter-api-java-sdk', version: '1.2.4' } Creating the topic:\nkafka-topics --bootstrap-server localhost:9092 --create --topic twitter_tweets --partitions 6 --replication-factor 1 Adding a TwitterKafkaProducerInterface Interface:\npackage com.github.alainbouchard.kafka_demo.demo2; public interface TwitterKafkaProducerInterface { void send(String topic, String message); } Implementing the interface with TwitterKafkaProducer Class:\npublic class TwitterKafkaProducer implements TwitterKafkaProducerInterface { private final Logger logger = LoggerFactory.getLogger(TwitterKafkaProducer.class.getName()); private KafkaProducer\u003cString, String\u003e producer; public TwitterKafkaProducer() { Properties properties = new Properties(); // Set Producer Properties properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\"); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // Create Kafka Producer producer = new KafkaProducer\u003cString, String\u003e(properties); // Create topic: // kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_tweets --partitions 6 --replication-factor 1 } public Logger getLogger() { return logger; } @Override public void send(String topic, String message) { ProducerRecord producerRecord = new ProducerRecord\u003c\u003e(topic, null, message); producer.send(producerRecord, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (exception != null) { getLogger().error(\"Could not send the message to the producer.\", exception); } } }); } } Adding the TwitterListerner Class:\nSome TwitterApi v2 SDK methods have issues at the moment that this document is written.\npublic class TwitterListener { /*** * Ref: * https://developer.axonivy.com/api-browser?url=/market-cache/twitter/twitter-connector-product/9.3.0/openapi.json#/Tweets/addOrDeleteRules * https://github.com/twitterdev/twitter-api-java-sdk/tree/d0d6a8ce8db16faf4e3e1841c3a43bd5a56aa069 * https://developer.twitter.com/en/docs/twitter-api */ private Logger logger = LoggerFactory.getLogger(TwitterListener.class.getName()); // API V2 uses BEARER token. // TODO: Use environment variable for BEARER_TOKEN. private final String BEARER_TOKEN = \"\"; protected TwitterApi twitterApi; private TwitterKafkaProducerInterface twitterKafkaProducer; Function\u003cList\u003cRule\u003e, List\u003cString\u003e\u003e GetIdsFromRules = r -\u003e r.stream().map(Rule::getId).collect(Collectors.toList()); public TwitterListener() { twitterApi = new TwitterApi(); TwitterCredentialsBearer credentials = new TwitterCredentialsBearer(BEARER_TOKEN); twitterApi.setTwitterCredentials(credentials); } public Logger getLogger() { return logger; } public TwitterApi getTwitterApi() { return twitterApi; } public void setTwitterKafkaProducer(TwitterKafkaProducerInterface twitterKafkaProducer) { this.twitterKafkaProducer = twitterKafkaProducer; }; private void logApiExceptionToString(String description, ApiException e) { String text = description + \" Status code: \" + e.getCode() + \" Reason: \" + e.getResponseBody() + \" Response headers: \" + e.getResponseHeaders(); getLogger().error(text, e); } private List\u003cRule\u003e addRule(String value, String tag, boolean dryRun) { // Create rule RuleNoId ruleNoId = new RuleNoId(); ruleNoId.setValue(value); ruleNoId.setTag(tag); // Add rule to list of rules List\u003cRuleNoId\u003e ruleNoIds = new ArrayList\u003c\u003e(); ruleNoIds.add(ruleNoId); // Add the list of rules to the request AddRulesRequest addRulesRequest = new AddRulesRequest(); addRulesRequest.add(ruleNoIds); AddOrDeleteRulesRequest addOrDeleteRulesRequest = new AddOrDeleteRulesRequest(); addOrDeleteRulesRequest.setActualInstance(addRulesRequest); List\u003cRule\u003e rules = null; try { AddOrDeleteRulesResponse result = getTwitterApi().tweets().addOrDeleteRules(addOrDeleteRulesRequest, dryRun); getLogger().info(result.toString()); rules = result.getData(); } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#addOrDeleteRules\", e); } return rules; } private AddOrDeleteRulesResponse deleteRules(List\u003cRule\u003e rules, boolean dryRun) { DeleteRulesRequestDelete deleteRulesRequestDelete = new DeleteRulesRequestDelete(); deleteRulesRequestDelete.ids(GetIdsFromRules.apply(rules)); DeleteRulesRequest deleteRulesRequest = new DeleteRulesRequest(); deleteRulesRequest.setDelete(deleteRulesRequestDelete); AddOrDeleteRulesRequest addOrDeleteRulesRequestForDelete = new AddOrDeleteRulesRequest(); addOrDeleteRulesRequestForDelete.setActualInstance(deleteRulesRequest); AddOrDeleteRulesResponse result = null; try { result = this.getTwitterApi().tweets().addOrDeleteRules(addOrDeleteRulesRequestForDelete, dryRun); getLogger().info(result.toString()); } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#addOrDeleteRules\", e); } return result; } private GetRulesResponse getRules(String paginationToken, List\u003cRule\u003e rules, Integer maxResults) { List\u003cString\u003e ruleIds = rules != null? GetIdsFromRules.apply(rules) : null; GetRulesResponse result = null; try { result = getTwitterApi().tweets().getRules(ruleIds, maxResults, paginationToken); getLogger().info(result.toString()); } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#getRules\", e); } return result; } private GetRulesResponse getRules(String paginationToken, List\u003cRule\u003e rules) { return this.getRules(paginationToken, rules, 1000); } private GetRulesResponse getRules(String paginationToken) { return this.getRules(paginationToken, null); } private void searchStream() { Set\u003cString\u003e expansions = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e tweetFields = new HashSet\u003c\u003e(); tweetFields.add(\"id\"); tweetFields.add(\"author_id\"); tweetFields.add(\"created_at\"); tweetFields.add(\"text\"); Set\u003cString\u003e userFields = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e mediaFields = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e placeFields = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e pollFields = new HashSet\u003c\u003e(Arrays.asList()); Integer backfillMinutes = null; // There is a bug in the Twitter API v2 where any specified value will cause an error. InputStream result = null; try { result = getTwitterApi().tweets().searchStream(expansions, tweetFields, userFields, mediaFields, placeFields, pollFields, backfillMinutes); try { JSON json = new JSON(); Type localVarReturnType = new TypeToken\u003cFilteredStreamingTweet\u003e(){}.getType(); BufferedReader reader = new BufferedReader(new InputStreamReader(result)); String line = reader.readLine(); while (line != null) { if (line.isEmpty()) { getLogger().info(\"==\u003e Empty line\"); line = reader.readLine(); continue; } Object jsonObject = json.getGson().fromJson(line, localVarReturnType); String message = jsonObject != null ? jsonObject.toString() : \"Null object\"; getLogger().info(message); twitterKafkaProducer.send(\"twitter_tweets\", message); line = reader.readLine(); } } catch (Exception e) { e.printStackTrace(); } } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#searchStream\", e); } } public static void main(String[] args) { TwitterListener twitterListener = new TwitterListener(); twitterListener.setTwitterKafkaProducer(new TwitterKafkaProducer()); boolean dryRun = false; // Delete all existing Rules; try { GetRulesResponse rulesResponse = twitterListener.getRules(null); AddOrDeleteRulesResponse result = twitterListener.deleteRules(rulesResponse.getData(), dryRun); twitterListener.getLogger().info(\"Deleted rules: \" + twitterListener.GetIdsFromRules.apply(result.getData())); } catch (Exception e) { twitterListener.getLogger().error(\"oops!\"); // bug in the TwitterApi SDK. e.printStackTrace(); } // Adding Rules; twitterListener.addRule( \"potus -is:retweet\", \"Non-retweeted potus tweets\", dryRun); twitterListener.addRule( \"hockey -is:retweet\", \"Non-retweeted hockey tweets\", dryRun); twitterListener.addRule( \"baseball -is:retweet\", \"Non-retweeted baseball tweets\", dryRun); twitterListener.addRule( \"bitcoin -is:retweet\", \"Non-retweeted bitcoin tweets\", dryRun); // Filter twitter stream; twitterListener.searchStream(); } } Fine-tuning the Kafka producer Producers Acks acks=0 no response is requested may loose data if broker goes offline it is okay when lost of data is acceptable: metrics collection log collection Producer Broker 101 partition 0 (leader) -------- ------------------------------- | | +-----[send data to leader]----\u003e+ | | acks=1 (default as Kafka v2.0) leader response is requestion (no guarantee of replication) the producer may retry if the tack isn’t received data loss is possible if leader broker goes offline and replcas haven’t replicated the data yet Producer Broker 101 partition 0 (leader) -------- ------------------------------- | | +----[send data to leader]-----\u003e+ | | +\u003c---[respond write reqs]-------+ | | acks=all (replicas acks) leader and replicas acks are requested adding latency and safety no data loss if enough replicas needed setting to avoid losing data Producer Broker 101 part0 (leader) Broker 102 part0 (replica) Broker 103 part0 (replica) -------- ------------------------- -------------------------- -------------------------- | | | | +---[send data to leader]---\u003e+ | | | | | | | +-----[send to replica]-----\u003e+ | | | | | | +-----[send to replicas]---------------------------------\u003e+ | | | | | +\u003c--------[ack write]--------+ | | | | | | +\u003c--------[ack write]-------------------------------------+ | | | | +\u003c---[respond write reqs]----+ | | | | min.insync.replicas the acks=all must be used along with min.insync.replicas it can be set at the proker or topic level (override) min.insync.replicas=2 means that at least 2 brokers that are ISR (incuding leader) must do the write aknowledgement e.g., with replication.factor=3, the min.insync.replicas=2 and acks=all then only 1 broker can go down or the producer will receive an exception on the send operation Producer retries developers are expected to handle the exceptions or the data may be lost: e.g., transcient failure: NotEnoughReplicasException a retries setting is available: default is 0 no limits to the retries, i.e., Interger.MAX_VALUE in case of retries, there is a change that the messages will be sent in wrong order relying on key-based ordering may be an issue max.in.flight.requests.per.connection (default=5) can be used to help fixing message ordering issue, where max.in.flight.requests.per.connection=1 will ensure ordering, and slow down the throughput. Idempotent producer can be used to help with ordering issues if Kafka version is \u003e= 1.0.0 Idempotent producer using kafka \u003e= 0.11 - an idempotent producer can be defined, which will solve the duplicates due to network issues (i.e., if ack is lost on the network) idempotent producers are great to guarantee a stable and safe pipeline it comes with when using producerProps.put(\"enable.idempotence\", true): retries=Integer.MAX_VALUE max.in.flight.requests=1 with Kafka \u003e= 0.11 and \u003c 1.1 max.in.flight.requests=5 with Kafka \u003e= 1.1 for higher performance acks=all The min.insync.replicas=2 must also be specified since enable.idempotence=true property doesn’t imply this configuration Note: running safe producer might impact the throughput or lantency, and therefore the use case must be verified to make sure the producer is within the NFR expectations An example of Producer settings to improve safeness:\n// Make a safer producer properties.setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\"); properties.setProperty(ProducerConfig.ACKS_CONFIG, \"all\"); // default value with Idempotence=true properties.setProperty(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE)); // default value with Idempotence=true properties.setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"5\"); // with kafka \u003e= 2.0, otherwise use 1. Message compression Some compression benchmarks can be found the this blog: Squeezing the firehose: getting the most from Kafka compression\nproducer usually send data text-based (json) messages compression is set at the producer configuration only no needs for consumer or broker configuration the compression.type can be none (default), gzip, lz4 and snappy compression will improve the throughput and performance +----------------+ +--------------------+ +---------------+ | Producer Batch |------\u003e| Compression of the |-----\u003e| Kafka Cluster | +----------------+ | batch of messages | +---------------+ +--------------------+ Compression advantages:\nsmaller producer request size (compression up to 4x) faster to tranfer data over the network better throughput better disk utilisation in Kafka cluster Compression disavantages:\nproducers and consumers must commit CPU time for compression/decompression Overall:\nsnappy and lz4 are optimal for speed/compression ratio recommendations are to try the algorithms in a given use case and environment always use compression in prod should use along with linger.ms and batch.size configuration settings Configuration: linger.ms and batch.size Kafka producer default behavior is to send records as soon as possible it will have up to 5 requests in flight (batch) batching messages are done simultaneously with messages are in-flight (no time wasted) smart batching allows kafka to increase throughput while maintaining very low latency linger.ms is the number of ms (default=0) that the producer is willing to wait to fully get a batch of messages by introducing some lag (e.g., linger.ms=5) then we increase the cahnges of messages being sent together in a batch at the cost of introducing a small delay (e.g., 5 ms) then the throughput can be increased, the compression is more efficient and the producer efficiency is better batch.size is the maximum mumber of bytes (default=16KB) that will be included in a batch increasing a batch size to higher number (32KB or 64KB) can help increasing the ompression, throughput, and efficiency of requests any message tha is bigger than a batch size will not be batched the producer will make or allocate a batch per partition the average batch size metric can be monitored by using the Kafka Producer Metrics An example of Producer settings to improve efficiency:\n// Improve throughput efficiency of the producer properties.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"snappy\"); properties.setProperty(ProducerConfig.LINGER_MS_CONFIG, \"20\"); properties.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, \"32768\"); // 32 KB Producer default partition and key hashing The default key are hashed using murmur2 algorithm it is possible - but maybe not suggested - to override the partitioner behavior using partitioner.class the formula from Kafka code: targetPartition = Utils.abs(Utils.murmur2(record.key())) % numPartitions; therfore the same key will always go to the same partition changing the number of partition will cause key vs partition issues and should be avoided Max.blocks.ms and buffer.memory These are advanced settings and it is probably better to avoid tweaking them unless necessary.\nwhen the producer prodeuces faster tahn the broker can handle then the records will be memory buffered the buffer.memory is the size of the buffer (default is 32MB) a full buffer (e.g., full 32MB) will cause the producer.send() method to block (or wait) the max.block.ms (default=60000ms) is the waiting time befre the producer.send() method throw an exception, and causes are: the producer has filled up the buffer the broker is not accepting any new data 60s has elapsed An exception may mean that the broker is down or overloaded as it can’t handle the requests Elastic Search Adding elasticsearch docker container The following must be added in order to add an elastic search container or server to the kafka-cluster:\ndocker-compose.yaml:\nelasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 ports: - 9200:9200 - 9300:9300 environment: discovery.type: single-node node.name: es01 cluster.name: kafka-cluster To start the service from docker-compose:\n\u003e docker-compose up elasticsearch -d \u003e docker-compose ps NAME COMMAND SERVICE STATUS PORTS kafka-cluster-elasticsearch-1 \"/usr/local/bin/dock…\" elasticsearch running 0.0.0.0:9200-\u003e9200/tcp, 0.0.0.0:9300-\u003e9300/tcp kafka-cluster-kafka-1 \"/etc/confluent/dock…\" kafka running 0.0.0.0:29092-\u003e29092/tcp kafka-cluster-zookeeper-1 \"/etc/confluent/dock…\" zookeeper running 0.0.0.0:22181-\u003e2181/tcp \u003e curl localhost:9200/ { \"name\" : \"es01\", \"cluster_name\" : \"kafka-cluster\", \"cluster_uuid\" : \"-w05UbWdSeOhc4nRGcy8yA\", \"version\" : { \"number\" : \"7.5.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"8bec50e1e0ad29dad5653712cf3bb580cd1afcdf\", \"build_date\" : \"2020-01-15T12:11:52.313576Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.3.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Java elasticsearch client The following packages are required:\nimplementation group: 'org.elasticsearch.client', name: 'elasticsearch-rest-high-level-client', version: '7.14.0' implementation group: 'org.apache.logging.log4j', name: 'log4j-core', version: '2.17.2' // Elastic search dependency public class ElasticsearchClient { Logger logger = LoggerFactory.getLogger(ElasticsearchClient.class.getName()); private RestHighLevelClient restHighLevelClient; public ElasticsearchClient() { restHighLevelClient = new RestHighLevelClient( // TODO: use configuration file or environment variables to set ip and ports, now configured for local docker containers. RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9300, \"http\"))); } public RestHighLevelClient getRestHighLevelClient() { return restHighLevelClient; } public boolean ping() { boolean result = false; try { result = getRestHighLevelClient().ping(RequestOptions.DEFAULT); } catch (IOException e) { logger.error(\"Elasticsearch client received an exception.\", e); } finally { logger.info(\"Elasticsearch aliveness: \" + result); } return result; } public IndexResponse toIndex(String index, String jsonSource, String id) { IndexRequest indexRequest = new IndexRequest(index) .id(id) // Make the entry idempotent. .source(jsonSource, XContentType.JSON); IndexResponse indexResponse = null; try { indexResponse = getRestHighLevelClient().index(indexRequest, RequestOptions.DEFAULT); logger.info(indexResponse.getId()); } catch (IOException e) { logger.error(\"Caught and exception.\", e); } return indexResponse; } public void close() { try { getRestHighLevelClient().close(); } catch (IOException e) { logger.error(\"Caught and exception.\", e); } } public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(ElasticsearchClient.class.getName()); // Setup Elasticsearch ElasticsearchClient elasticsearchClient = new ElasticsearchClient(); elasticsearchClient.ping(); // Setup Kafka Consumer TwitterKafkaConsumer twitterKafkaConsumer = new TwitterKafkaConsumer(); twitterKafkaConsumer.subscribe(\"twitter_tweets\"); // Get data using Kafka Consumer and insert data to the elasticsearch while(true) { // TODO: replace with better logic. ConsumerRecords\u003cString, String\u003e records = twitterKafkaConsumer.poll(Duration.ofMillis(100)); records.forEach(record -\u003e { FilteredStreamingTweetResponse tweet = twitterKafkaConsumer.mapTweetToObject(record.value()); elasticsearchClient.toIndex(\"twitter\", record.value(), tweet.getData().getId()); logger.debug(\"Key: \" + record.key() + \" Value: \" + record.value() + \" Partition: \" + record.partition() + \" Offset: \" + record.offset() + \" Timestamp: \" + record.timestamp()); try { Thread.sleep(1000); } catch (InterruptedException e) { logger.error(\"Error while waiting for sleep\", e); } }); } // Tear-down the Elasticsearch // elasticsearchClient.close(); // Tear-down the Kafka Consumer } } Kafka delivery semantics The at least once processing should used for most applications along with idempotent strategy.\nAt most once Offsets are committed as soon as the message batch is received. If the processing oes wrong, the message will be lost (it won’t be read again)\nAt least once Offsets are committed after the message is processed. If the processing goes wrong, the message will be read again. This can resilt in duplicate processing of messages. Idempotence of the messages is needed to avoid duplicates.\nExactly once Can be achieved for kafka-to-kafka workflows using kafka streams APIs. For the Kafka-to-Sink workflows, idempotent consumer is needed.\nIdempotentce and offset auto-commit The default configuration (ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG=\"enable.auto.commit\") setting is at-least-once if not specified otherwise.\nTo avoid duplicate entries, the usage of a unique key will be required. An example would be the Twitter ID.\nConsumer Poll Behaviours Default values should be correct in most cases but the following can be adjuested.\nfetch.min.bytes (default=1):\ncontrols the minimum data to pull on each request help improving throughput and decreasing request numbr cost is latency max.poll.records (default=500):\ncontrols the number of records to receive per poll request increase if the messages are very small and if RAM is available best practices tell to monitor the number of records per poll request and to adjust to increase the value if default value is often reached max-partitions.fetch.bytes (default=1MB):\nmaximum data returned by the broker per partition reading from many partions will require a lot of memory fetch.max.bytes (default 50MB):\nmax data returned for each fetch request (covers multiple partitions) the consumer performs multiple fetches in parallel Consumer Offset Commits Strategies The two strategies:\nenable.auto.commit=true and syncrhonous processing of record batches (default)\npseudocode:\nwhile(true) { List\u003cRecords\u003e records = consumer.poll(Duration.ofMillis(100)); doSomethingSunchronous(records); } offsets get automatically commited at regular interval auto.commit.interval.ms=5000 (default) Note: using asyncrhonous processing would make the delivery sementic to at-most-once since the offset will be committed before the data is processes enable.auto.commit=false and manual commit of offsets\npseudocode:\nwhile(true) { records += consumer.poll(Duration.ofMillis(100)); if isReady(records) { doSomethingSynchronous(records); consumer.commitSync(); } } offsets commit is controlled according to the expected conditions\nE.g., accumulation recortds into a buffer and then flushing the buffer to a DB, then offsets are committed\nManual commits Settings to be configured to avoid offsets auto commits:\nproperties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,\"false\"); // Will require manual offsets commits properties.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"10\"); // Only retrieve 10 records at the time The syncronous commit command for the consumer - pseudocode:\nwhile(true) { ConsumerRecords\u003cString, String\u003e records = twitterKafkaConsumer.poll(Duration.ofMillis(100)); logger.info(String.format(\"Received %s records\", records.count())); records.forEach(record -\u003e { // do something synchronous here... }); logger.info(\"Committing offsets\"); twitterKafkaConsumer.commitSync(); logger.info(\"Offsets were committed\"); } Using kafka-console-consumer command:\nlooking at the offsets for given group kafka-java-demo-elasticsearch: kafka-consumer-groups --bootstrap-server localhost:9092 --group kafka-java-demo-elasticsearch --describe Consumer group 'kafka-java-demo-elasticsearch' has no active members. GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID kafka-java-demo-elasticsearch twitter_tweets 3 0 39 39 - - - kafka-java-demo-elasticsearch twitter_tweets 0 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 4 0 63 63 - - - kafka-java-demo-elasticsearch twitter_tweets 5 0 43 43 - - - kafka-java-demo-elasticsearch twitter_tweets 2 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 1 0 58 58 - - - sh-4.4$ consuming records using Java consumer and verifying the offsets: sh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --group kafka-java-demo-elasticsearch --describe Consumer group 'kafka-java-demo-elasticsearch' has no active members. GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID kafka-java-demo-elasticsearch twitter_tweets 3 0 39 39 - - - kafka-java-demo-elasticsearch twitter_tweets 0 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 4 0 63 63 - - - kafka-java-demo-elasticsearch twitter_tweets 5 30 43 13 - - - kafka-java-demo-elasticsearch twitter_tweets 2 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 1 0 58 58 - - - sh-4.4$ Note: the current offset for the partition 5 (above example) should be a multiple of ConsumerConfig.MAX_POLL_RECORDS_CONFIG property value, any record (modulo) will be consumed again since the offset wasn’t committed Consumer offset reset behaviour The offset reset available behaviours are:\nauto.offset.reset=latest which will read from the end of the log auto.offset.reset=earliest which will read from start of the log auto.offset.reset=none which will throw exception if no offset is found The consumer offsets can be lost:\nif a consumer hasn’t read new data for 1 day (kafka \u003c 2.0) if a consumner hasn’t read new data for 7 days (kafka \u003e= 2.0) The retention delay can be set using the broker setting offset.retention.minutes. Proper data and offset retention period must be set to ensure no data loss if a server can go down for a while, e.g., 30 days.\nReplaying the data for a consumner group To reset kafka server offsets and replay messages from beginning:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --topic twitter_tweets --group kafka-java-demo-elasticse arch --reset-offsets --to-earliest --execute GROUP TOPIC PARTITION NEW-OFFSET kafka-java-demo-elasticsearch twitter_tweets 3 0 kafka-java-demo-elasticsearch twitter_tweets 0 0 kafka-java-demo-elasticsearch twitter_tweets 4 0 kafka-java-demo-elasticsearch twitter_tweets 5 0 kafka-java-demo-elasticsearch twitter_tweets 2 0 kafka-java-demo-elasticsearch twitter_tweets 1 0 sh-4.4$ This allows to restart the consumer and replay the messages; having an idempotent server will make replays safe.\nControlling Consumer Liveliness consumers in a group talk to a Consumer Group Coordinator there is a heartbeat and a pool mechanism to detect if consumers are down best practices tell that a process should process data fast and poll often Consumer Heartbeat Thread:\nsession.timeout.ms (default=10s)\nheardbeets are sent periodically to broker if no heartbeat is sent during that period, the consumer is considered dead set low value to faster consumer rebalancing heartbeat.interval.ms (default=3s)\nwait period between 2 heartbeats best pratices tell to set 1/3rd of the session.timeout.ms value both settings are set together to detect a dead consumer application (down)\nConsumer Poll Thread:\nmax.poll.interval.ms (default=5m)\nmaximum time between 2 poll() alls before declaring the consumer is dead relevant for Big Data frameworks (e.g., Spark) in case processing takes time mecahnism to detect a data processing issue with the consumer\nKafka Connect and Kafka Stream There are 4 Kafka use cases: Source to Kafka =\u003e Producer API =\u003e Kafka Connect Source Kafka to Kafka =\u003e Consumer API/Producer API =\u003e Kafka Streams Kafka =\u003e Sink =\u003e Consumer API =\u003e Kafka Connect Sink Kafka =\u003e Application =\u003e Consumer API =\u003e Kafka connect and Kafka stream will simplify and improve the in/out of Kafka Kafka connect and Kafka stream will simplify transforming data within kafka withou relying on external libraries The Kafka connectors can be found on the Confluent Kafka Connectors page.\nWhy Kafka Connect? developpers always want to import data from the same sources: DB, JDBC, Couchbase, Goldergate, SAP HANA, Blockchain, Cassandra, DynamoDB, FTP, IOT, MongoDB, MQTT, RethinkDB, Salesforce, Solr, SQS, Twitter, etc. developpers always want to store data in the same sinks: S3, ElasticSearch, HDFS, JDBC, SAP HANA, DocumentDB, Cassandra, DynamoDB, HBase, MongoDB, Redis, Solr, Splunk, Twitter, etc. an unexperimented developper may struggle to achieve fault tolerance, idempotence, distribution, ordering, etc. an experimented developper already did the work for others. Sources/Destinations Connect Cluster Kafka Cluster Stream Apps -------------------- --------------- ------------- ----------- [source1]-------------\u003e[worker] [broker]--------------\u003e[app1] [worker]------------------\u003e[broker]\u003c-------------- [source2] [worker] [broker] [worker]\u003c------------------[broker]--------------\u003e[app2] [sinks]\u003c--------------[worker] \u003c-------------- Kafka Connect: High level source connectors to get data from Common Data Sources sink connectors to publish data in common data stores make it easy for non-experienced developpers to quickly get data in kafka part of the ETL pipeline (Extract, Transform and Load) scaling made easy from small pipelines to company-wide pipelines re-usable code Adding a connector to a Java project The connector kafka-connect-twitter will be used as an example to show file structure:\n+ [my-java-project] +-+ [kafka-connect] (to be created with sub folders) | +-+ [connectors] | | +-+ [kafka-connect-twitter] (downloaded connector project from github) | | | +-+ *.jar | | +-+ connector-standalone.properties | | | + run.sh | | | + twitter.properties Connect standalone CLI command sh-4.4$ connect-standalone USAGE: /usr/bin/connect-standalone [-daemon] connect-standalone.properties sh-4.4$ Creating a new topic for the kafka connector:\n$ kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_status_connect --parti tions 3 --replication-factor 1 WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic twitter_status_connect. $ kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_deletes_connect --part itions 3 --replication-factor 1 WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic twitter_deletes_connect. $",
    "description": "Warning 2022-07-11: This is a WORK IN PROGRESS document (WIP) and need to be reviewed.\nApache Kafka Topics, partitions and offsets Brokers Brokers and topics Topic replication factor Producers Consumers Consumer offsets Kafka broker discovery Zookeeper Kafka guarantees Install Kafka using Docker images Use Kafka Topics CLI Topics CLI Kafka console producer Kafka console consumer Kafka-Client with Java Kafka Connect and Kafka Stream Apache Kafka Why Apache Kafka?",
    "tags": [],
    "title": "Apache Kafka (WIP)",
    "uri": "/notes/apache-kafka/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Data Quality and Data Management 101 What is Data Quality Definition? What is Data Quality Management (DQM) and its pillars? What is the Impact of Poor Data Quality Real-life Examples Causes of Bad Data Data Quality Dimensions Overview Example of Accuracy Issue Example of Completeness Issue Example of Consistency Issue Example of Timelessness Issue Example of Validity Issue Example for Uniqueness Issue Example of Integrity Issues Multiple Data Quality Dimensions Issues Data Quality Rules Data Quality Techniques Data Quality Roles Data Quality Process Data Quality Tools Data Governance or Data Quality Management Best Practices Data Quality and Data Management 101 What is Data Quality Definition? In the Data and Business Intelligence domain, Data Quality refers to the overall accuracy, completeness, reliability, and relevance of data, ensuring that it is fit for its intended use In essence, Data Quality ensures that the data used in business intelligence efforts is trustworthy, allowing for accurate analysis, reporting, and decision-making Data quality is defined by how well a given dataset meets a user’s need. Data quality is an important criteria for ensuring that data-driven decisions are made as accurately as possible What is Data Quality Management (DQM) and its pillars? People: the involvement of the data stewards, analyst and business users who are responsible for setting data standards, monitoring quality, and resolving issues. These roles ensure alignment alignment between business needs and the data used to support them. Data Profiling: a critical step that involves analyzing the current state of the data by examining its structure, patterns, and anomalies. Data profiling helps uncover quality issues such as duplicates, missing values, and inconsistencies, enabling organizations to identify areas needing improvement. It is initiated to understand the current state of existing data by comparing data to data standards as set by the DQM, used to define the benchmarks to evaluate the improvements. Defining Data Quality: establishing clear, measurable data quality dimensions such as accuracy, completeness, timeliness and consistency. These criteria are developed based on business needs and objectives, ensuring that data supports decision-making and operations effectively. What the data should look like, and it is based on the business goals Data Reporting: providing regular insights and metrics on the state of data quality through dashboards audits, and scorecards. This reporting enable stakeholders to monitor progress, identify trends, and make informed decisions about improvements and corrective actions. It will return the DQM “return on the investment” (ROI), and how data compares to the defined data quality benchmarks. Data Fixing: implementing corrective actions to resolve data quality issues, including data cleansing, standardization, and deduplication. It also involves root cause analysis to prevent recurring issues by addressing underlying process or system flaws. It is intended to repair the data that doesn’t meet the defined data quality benchmarks and standards. Improving data to the required standards. Most important pillar is probably the People one.\nWhat is the Impact of Poor Data Quality Poor data quality can have significant negative impacts on a business across various dimensions:\ndecision-making, operational efficiency, financial performance/missed opportunities, and customer trust/reputation risk. Key impacts include:\nInaccurate Decision-Making: poor data quality leads to faulty analysis and reporting, causing executives and managers to base strategic and operation decisions on incorrect or incomplete information. This can result in missed opportunities, flawed strategies, or inappropriate response to market conditions. The data based decisions and policies are only as good as the data they are based on. Reduced Operational Efficiency: when data is inaccurate or inconsistent, processes such ad data integration, analysis, and reporting take longer and require more manual intervention. This leads to inefficiencies, increased labor costs, and delayed operations, affecting productivity. Financial Losses: poor data quality can directly impact revenue by causing billing errors, incorrect pricing, or inventory mismanagement. It also increases costs due to rework compliance fines, or failed marking campaigns based on bad data. For example, failure to deliver service to customers or failure to sale the relevant contacts. Customer dissatisfaction: errors in customer data, such as wrong addresses or incorrect orders, can result in poor customer experiences, damaged relationships, and loss of trust. This can lead to increased customer churn and negative brand reputation. Regulatory and Compliance risks:: in industries with strict regulatory requirements (e.g., finance, healthcare, etc), poor data quality can lead to non-compliance, legal penalties, and damage to the organization’s credibility. For example, GDPR issues and negative media coverage. Loss of competitive advantage: businesses that fail to manage data quality effectively may fall behind competitors that leverage clean, accurate data for better market insights, customer targeting, and innovation. In conclusion, poor data quality undermines the reliability and value of business intelligence (BI)., impacting nearly every aspect of the business, from dat-to-day operations to long-term growth and competitiveness.\nReal-life Examples There are several real-life examples where poor data quality had a significant impact on businesses:\nKnight Capital Group: in 2012, a software glitch caused the firm to make unintended stock trades, resulting in a $ 440 million loss within 45 minutes. This error was linked to poor data handling and led to the firm’s eventual bankruptcy. [GetRightData] **Boeing 747 Max Crashes: Faulty sensor data triggered the automated flight control system, contributing to two fatal crashes in 2018 and 2019. Boeing lost billions, and the incidents resulted in the grounding of all 737 Max planes. [GetRightData] UK Passport Agency: in 1999, data migration errors during a system upgrade delayed the insurance of 500K passports. The fallout from these data issues led to public outrage and a cost of around £12.6 millions to resolve. [GetRightData] NASA Mars Orbiter: NASA lost a $125 million Mars orbiter because a Lockheed Martin engineering team used English units of measurement while the agency’s team used the more conventional metric system for a key spacecraft operation. [CNN_NASA] These examples demonstrate how critical accurate data is in preventing financial losses, reputation damage, and even human safety risks.\nMore cases are documented on [GetRightData].\nCauses of Bad Data Bad data can arise due to due to various reasons, typically stemming from issues in data collection, management, and governance. Here are some reasons why we encounter poor data quality:\nHuman Error Manual Data Entry: people can make mistakes when entering data, leading to inaccuracies, misspellings, and incomplete records. For example, typographical errors or incorrect formatting can lead to inconsistencies in databases. Lack of Training: when data entry personnel aren’t adequately trained, they may unintentionally introduce systemic errors. This happens when employees unknowingly enter incomplete or incorrect data, which accumulates and impacts the overall quality of the dataset. Inadequate Data Validation Missing Validation Rules: without proper validation checks, systems can let through invalid data like incorrect formats or data types (for instance, entering letters when numbers are expected). These gaps can lead to faulty or unusable data being stored. Inconsistent Standards: when different departments or systems use varying data formats and standards, it results in inconsistencies across the organization, making it difficult to maintain clean and reliable data. Integration Errors Data from Multiple Sources: merging data from various sources, such as legacy systems or third-party vendors, often introduces discrepancies. Differences in data structures or formats can lead to duplicated records, missing information, or incorrect data being captured. Incorrect Mapping or Transformation: during migrations or updates, if data fields aren’t mapped properly, or if transformation rules are incorrect, it can lead to distorted or corrupted data. Outdated Information Stale Data: information can become outdated if not updated regularly. This is especially common with customer records like contact details, which change frequently over time. Lag in Data Updates: some systems don’t update data in real time, leading to discrepancies between what’s recorded and actual events, which can cause issues in decision-making. Lack of Data Governance No Clear Ownership: when there’s no assigned responsibility for data management, inconsistent practices across departments can result in poor data quality. Without ownership, it’s hard to maintain data standards. Unstructured Data Management: without clear policies and structured processes for managing storing, and cleaning data, its quality tends to degrade, leading to inaccurate or complete data. Incomplete Data Partial Data Entry: sometimes, important data fields are left out during entry, leading to incomplete records. This can happen when systems don’t enforce required fields, making the data unreliable. Legacy Systems: older systems might not capture all necessary data, leaving gaps when this information i used in modern analytics or business intelligence platforms. Technical Errors System Failures: technical issues like system crashes, corrupted files, or failed data transfers can lead to incomplete or inaccurate data. Software Bugs: bugs in data processing tools or software can introduce errors, reading faulty data that affects overall insights and decision-making. In conclusion, each of these factors highlights the need for solid DQM practices, such as robust validation, regular data cleaning, and strong governance to keep data accurate and reliable.\nData Quality Dimensions Overview Data Quality Dimensions refer to the different ways we measure how good our data is.\nflowchart TB DQD(Data Quality Dimensions) \u003c--\u003e Validity DQD \u003c--\u003e Timeliness DQD \u003c--\u003e Completeness DQD \u003c--\u003e Uniqueness DQD \u003c--\u003e Consistency DQD \u003c--\u003e Accuracy DQD \u003c--\u003e Integrity Here’s some of the most recognized dimensions:\nAccuracy: this is all about how closely the data matches real-world facts. If your data doesn’t reflect reality, it can lead to bad decisions. Completeness: it checks if all the necessary information is present. Missing data can leave important gaps, making the data unreliable. Consistency: data should be the same across all systems. For example, if one database says a customer lives in New York and another says they live in California, that’s inconsistent and can cause confusion. Timelessness: how current is the data? Old or delayed information can lead to missed opportunities or wrong conclusions. Validity: this looks at whether the data follows the required rules or formats. For instance, you wouldn’t want letters in a field that should only have numbers. Uniqueness: there shouldn’t be duplicates. If the same person or thing is entered twice, it skews the results and creates inefficiencies. Integrity: this ensures that relationships within the data are properly maintained. For example, if a customer is linked to an order, that link should always stay intact. In summary, these dimensions help ensure that your data is trustworthy and usable for making informed business decisions.\nExample of Accuracy Issue Here’s an example of a data accuracy issue using a customer list:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih 123 Elm Street Springfield 1105 104 Emma Johnson 789 Maple Blvd Capital City 54321 Issue: In this table, Customer ID 103 has multiple accuracy errors:\nFirst Name: “Jonh” should be “John.” Last Name: “Smtih” should be “Smith.” Address: “123 Elm Street” is an incorrect variation of “123 Elm St.” Zip Code: The entry “1105” is missing a digit; it should be “01105.” These inaccuracies could lead to problems like sending mail to the wrong address, duplicate entries for the same person, or errors in customer service interactions.\nThis highlights how inaccurate data can cause operational inefficiencies and poor customer experiences.\nExample of Completeness Issue Here’s an example of a data completeness issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih Springfield 1105 104 Emma 789 Maple Blvd Capital City 54321 Completeness Issues:\nCustomer 103: The Address field is missing, leaving out critical information needed for communications or shipments. Customer 104: The Last Name is missing, making it impossible to fully identify this customer, especially if there are multiple people named “Emma.” Incomplete data like this can cause issues such as failed deliveries, incorrect customer segmentation, or difficulties in contacting the customer.\nExample of Consistency Issue Here’s an example of a data consistency issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih 123 Elm Street Springfield 1105 104 Emma Johnson 789 Maple Blvd Capital City 54321 Consistency Issues:\nCustomer 103: The Address is listed as “123 Elm Street” instead of “123 Elm St,” which is an inconsistency in the format of the same address as Customer 101. Customer 103: The Zip Code is “1105” instead of “01105,” showing inconsistency in the zip code format (missing leading zero). Inconsistent data like this creates confusion and inefficiencies, as different systems may not recognize the same entity when information is presented in various formats. This can lead to duplicate records or incorrect data processing.\nExample of Timelessness Issue Here’s an example of a data timeliness issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code Last Updated 101 John Smith 123 Elm St Springfield 01105 2024-09-01 102 Jane Doe 456 Oak Ave Shelbyville 12345 2024-09-01 103 Jonh Smtih 123 Elm Street Springfield 01105 2021-06-15 104 Emma Johnson 789 Maple Blvd Capital City 54321 2022-03-10 Timeliness Issues:\nCustomer 103: The Last Updated date is from 2021, making this record outdated. Since the address and contact details haven’t been verified or updated in over two years, it’s possible that the data is no longer accurate. Customer 104: The data was last updated in early 2022, which also makes it outdated for a customer who might have changed address or contact information since then. Outdated records like these could lead to problems such as sending communications or deliveries to the wrong address, missing customer preferences, or failing to capture changes that impact business decisions. Timely data updates are crucial to maintaining the reliability of customer information.\nExample of Validity Issue Here’s an example of a data validity issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code Email 101 John Smith 123 Elm St Springfield 01105 john.smith@email.com 102 Jane Doe 456 Oak Ave Shelbyville 12345 jane.doe@email.com 103 Jonh Smtih 123 Elm Street Springfield 1105 jonh.smtih@email..com 104 Emma Johnson 789 Maple Blvd Capital City 54321 emma@invalidemail@com Validity Issues:\nCustomer 103: The email address “jonh.smtih@email..com” contains a formatting error (extra period before “.com”), which is invalid. Customer 104: The email “emma@invalidemail@com” has two “@” symbols, which is not allowed in email formatting, making it invalid. In cases like these, data validity rules (e.g., proper email formats, correct zip code length, or valid data types) must be enforced. Invalid data such as incorrect email formats could result in failed communications and missed customer interactions, highlighting the importance of ensuring data conforms to required standards.\nExample for Uniqueness Issue Here’s an example of a data uniqueness issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code Email 101 John Smith 123 Elm St Springfield 01105 john.smith@email.com 102 Jane Doe 456 Oak Ave Shelbyville 12345 jane.doe@email.com 103 Jonh Smtih 123 Elm Street Springfield 01105 jonh.smtih@email.com 104 Emma Johnson 789 Maple Blvd Capital City 54321 emma.johnson@email.com 105 John Smith 123 Elm St Springfield 01105 john.smith@email.com Uniqueness Issues:\nCustomer 101 and Customer 105: These entries are duplicates, representing the same individual (John Smith) with identical information (address, email, etc.). However, they have different Customer IDs, which indicates a lack of uniqueness in the dataset. Duplicate records like this can lead to inefficiencies, such as multiple mailings to the same person, errors in customer segmentation, and skewed analytics. Ensuring uniqueness, especially in key fields like Customer ID or email, is crucial to maintaining data integrity.\nExample of Integrity Issues Here’s an example of a data integrity issue using a customer list and an associated order table:\nCustomer Table:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih 123 Elm Street Springfield 01105 104 Emma Johnson 789 Maple Blvd Capital City 54321 Order Table:\nOrder ID Customer ID Order Date Order Amount 5001 101 2024-09-01 $150 5002 105 2024-09-02 $200 5003 104 2024-09-03 $250 Integrity Issues:\nCustomer ID in Order Table: In the Order Table, Order 5002 refers to Customer ID 105, which does not exist in the Customer Table. This indicates a referential integrity problem, where a record in the Order Table is pointing to a non-existent customer. Mismatch of Addresses: In the Customer Table, Customer 103 has the same address as Customer 101 but with variations (e.g., “Elm Street” vs. “Elm St.”), which could also indicate an integrity issue in maintaining consistent data relationships. Data integrity ensures that relationships between tables (e.g., customer and order data) are accurate and complete. Integrity issues like these can lead to broken reporting, incorrect data analysis, and operational inefficiencies.\nMultiple Data Quality Dimensions Issues A single issue, like an address problem, can involve multiple data quality dimensions at once. Here’s how:\nAccuracy: if the address is wrong (e.g., “123 Elm St” instead of “321 Elm St”), it’s an accuracy issue. This can cause real problems, like sending packages to the wrong place or communicating with the customer at the wrong address. Completeness: if part of the address is missing, such as no zip code or an incomplete street name (e.g., “Elm St” without the house number), that’s a completeness issue. Without the full details, the address may not be useful, and the business can’t function properly. Integrity: data integrity is about making sure information is consistent across systems. If one system shows “123 Elm St” and another has “456 Oak Ave” for the same person, it’s an integrity problem, as these records don’t match. So, a simple issue like an incorrect or incomplete address can affect accuracy, completeness, and integrity all at the same time, making it crucial to fix the issue from multiple angles to ensure the data is reliable.\nData Quality Rules Data Quality Techniques Data Quality Roles Data Quality Process Data Quality Tools Data Governance or Data Quality Management Best Practices",
    "description": "Data Quality and Data Management 101 What is Data Quality Definition? What is Data Quality Management (DQM) and its pillars? What is the Impact of Poor Data Quality Real-life Examples Causes of Bad Data Data Quality Dimensions Overview Example of Accuracy Issue Example of Completeness Issue Example of Consistency Issue Example of Timelessness Issue Example of Validity Issue Example for Uniqueness Issue Example of Integrity Issues Multiple Data Quality Dimensions Issues Data Quality Rules Data Quality Techniques Data Quality Roles Data Quality Process Data Quality Tools Data Governance or Data Quality Management Best Practices Data Quality and Data Management 101 What is Data Quality Definition? In the Data and Business Intelligence domain, Data Quality refers to the overall accuracy, completeness, reliability, and relevance of data, ensuring that it is fit for its intended use In essence, Data Quality ensures that the data used in business intelligence efforts is trustworthy, allowing for accurate analysis, reporting, and decision-making Data quality is defined by how well a given dataset meets a user’s need. Data quality is an important criteria for ensuring that data-driven decisions are made as accurately as possible What is Data Quality Management (DQM) and its pillars? People: the involvement of the data stewards, analyst and business users who are responsible for setting data standards, monitoring quality, and resolving issues. These roles ensure alignment alignment between business needs and the data used to support them. Data Profiling: a critical step that involves analyzing the current state of the data by examining its structure, patterns, and anomalies. Data profiling helps uncover quality issues such as duplicates, missing values, and inconsistencies, enabling organizations to identify areas needing improvement. It is initiated to understand the current state of existing data by comparing data to data standards as set by the DQM, used to define the benchmarks to evaluate the improvements. Defining Data Quality: establishing clear, measurable data quality dimensions such as accuracy, completeness, timeliness and consistency. These criteria are developed based on business needs and objectives, ensuring that data supports decision-making and operations effectively. What the data should look like, and it is based on the business goals Data Reporting: providing regular insights and metrics on the state of data quality through dashboards audits, and scorecards. This reporting enable stakeholders to monitor progress, identify trends, and make informed decisions about improvements and corrective actions. It will return the DQM “return on the investment” (ROI), and how data compares to the defined data quality benchmarks. Data Fixing: implementing corrective actions to resolve data quality issues, including data cleansing, standardization, and deduplication. It also involves root cause analysis to prevent recurring issues by addressing underlying process or system flaws. It is intended to repair the data that doesn’t meet the defined data quality benchmarks and standards. Improving data to the required standards. Most important pillar is probably the People one.",
    "tags": [],
    "title": "Data Quality",
    "uri": "/notes/data-quality/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Gradle (6.6.1) Terminology Actions Files structure for a “Multi-module build” Commands Example of Typed Task Plugins Gradle (6.6.1) requires JDK download from [https://gradle.org/releases/] can use either Groovy or Kotlin DSL (Domain-Specific Language) Terminology Project: models a software component Build script: contains automation instructions for a project Task: defines executable automation instructions Ad hoc task: implements one-off, simplistic action code by defining doFirst or doLast, automatically extends DefaultTaslk without having to declare it Typed task: Explicitly declares type (for example, Copy); does not need to define actions as they are already provided by type Wrapper: set of files checked into SCM alongside source code standardizes compatible Gradle version for a project automatically downloads the Gradle distribution with defined version Actions doLast, doFirst, … Files structure for a “Multi-module build” root + build.gradle + moduleA/ | + build.gradle | + src/ + moduleB/ + build.gradle + src/ Commands gradle \u003ctask name\u003e -\u003e run the task from the build.gradle\ngradle wrapper -\u003e create the wrapper files\ncreates gradlew, gradlew.bat and the gradle directories…\ngradle/wrapper/gradle-wrapper.properties\nexample:\ndistributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https\\://services.gradle.org/distributions/gradle-6.6.1-bin.zip zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists ./gradlew \u003ctask name\u003e -\u003e download the wrapper gradle version and execute the task with it\ngradle projects -\u003e create settings for the project\nUse settings.gradle do define information, example:\nrootProject.name = \"gradle-training\" gradle \u003ctask name\u003e --dry-run -\u003e show the tasks from the build.gradle but does not execute\ngradle tasks --all -\u003e show the full list of available tasks\nExample of Typed Task example:\ntask copyFiles(type: Copy) { from \"sourceFiles\" into \"target\" include \"**/*md\" includeEmptyDirs = false } task createZip(type: Zip) { from \"build/docs\" archiveFileName = \"docs.zip\" destinationDirectory = file(\"build/dist\") dependsOn CopyFiles } Plugins create a reusable or sharable \u003cplugin-name\u003e.gradle file with defined tasks apply the plugin to the local project build.gradle file, example: plugin with tasks: myPlugin.gradle in build.gradle: apply from: \"myPlugin.gradle\" Notes:\navailable plugins are Core Plugins (from Gradle) and Community Plugins (not from Gradle) to apply Core plugin Base (for example), in build.gradle: apply plugin: 'base'",
    "description": "Gradle (6.6.1) Terminology Actions Files structure for a “Multi-module build” Commands Example of Typed Task Plugins Gradle (6.6.1) requires JDK download from [https://gradle.org/releases/] can use either Groovy or Kotlin DSL (Domain-Specific Language) Terminology Project: models a software component Build script: contains automation instructions for a project Task: defines executable automation instructions Ad hoc task: implements one-off, simplistic action code by defining doFirst or doLast, automatically extends DefaultTaslk without having to declare it Typed task: Explicitly declares type (for example, Copy); does not need to define actions as they are already provided by type Wrapper: set of files checked into SCM alongside source code standardizes compatible Gradle version for a project automatically downloads the Gradle distribution with defined version Actions doLast, doFirst, … Files structure for a “Multi-module build” root + build.gradle + moduleA/ | + build.gradle | + src/ + moduleB/ + build.gradle + src/ Commands gradle \u003ctask name\u003e -\u003e run the task from the build.gradle",
    "tags": [],
    "title": "Gradle",
    "uri": "/notes/gradle/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Project structure for gradle project in Java In build.gradle JAVA plugin APPLICATION plugin Commands Maven Dependencies Testing with Gradle JUnit 5 dependencies Project structure for gradle project in Java single Java project with Gradle\nexample: root + src + main + java + resources + test + java + resources + build + classes -\u003e compiled class files + libs -\u003e generated JAR files A multi-modules project file structure:\nexample: root/ + appA/ + build.gradle + src/ + main/ + java/ + resources/ + test/ + java/ + resources/ + build/ + classes/ -\u003e compiled class files + libs/ -\u003e generated JAR files + appB/ + build.gradle + src/ ... (same as above subproject) In build.gradle JAVA plugin add plugins:\nplugins { id 'java' } set the Java compatibility:\njava { sourceCompatibility = JavaVersion.VERSION_11 targetCompatibility = JavaVersion.VERSION_11 } compiler arguments:\ncompileJava { // example: to terminate compilation if a warning occures) options.compilerArgs \u003c\u003c '-Werror' } set JAR filename explicitly:\nversion = '1.0.0' jar { archiveBaseName = '\u003cyour-project-name\u003e' } Set javadoc options:\njavadoc { options.header = '\u003cyour java doc title here\u003e' options.verbose() } APPLICATION plugin in build.gradle file:\nplugins { id 'application' } set base class to run for the application as needed by plugin:\napplication { mainClass = 'com.\u003corganization\u003e.\u003cproject\u003e.Main' } in settings.gradle file:\nrootProject.name = '\u003cproject_name\u003e' include ':appA', ':appB' Commands gradle wrapper -\u003e Create Gradle Wrapper\ngradle clean -\u003e Clean dist output?\nwith java plugin:\n./gradlew compileJava --console=verbose ./gradlew processResources --console=verbose or both above command agragated command:\n./gradlew classes --console=verbose see dependencies tree (and if it can be found)\n./gradlew dependencies generate JAR file\n./gradlew jar -\u003e dropped JAR in build/libs directory with application plugin:\n./gradlew run --args=\"add 1 2\" -\u003e specifies arguments ./gradlew installDist -\u003e generate shippable application with scripts ./gradlew distZip distTar -\u003e bundle distributhe appliation ./gradlew javadocs -\u003e genetate java doc in build/docs/javadoc (index.html) project or multi-modules project:\n./gradlew project -\u003e show project and show projects structure Maven Dependencies where to find? [https://search.maven.org]\ndependency coordinates:\nexample: commons-cli:commons-cli:1.4 -\u003e \u003cgroup\u003e:\u003cartifact\u003e:\u003cversion\u003e or GAV set the repositories (where to get from) and dependencies:\nrepositories { mavenCentral() } dependencies { implementation 'commons-cli:commons-cli:1.4' // get from maven repo search results implementation project(':appA') // add project dependencies (other modules from this project) } Testing with Gradle JUnit 5 dependencies from search.maven.org:\nsearch org.junit.jupiter (latest version is 5)\nminimum needed is junit-jupiter-api and junit-jupiter-engine declaring test dependencies:\ntestImplementation -\u003e work on compilation and test execution testRuntime -\u003e work on runtime only adding dependencies:\ncopy \u0026 paste Gradle Groovy DSL path for both depenencies dependencies { implementation 'commons-cli:commons-cli:1.4' // not a testImplementation! testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.0' // test dependency testRuntime 'org.junit.jupiter:junit-jupiter-engine:5.7.0' // test dependency on run time only } commands:\n./gradlew compileTestJava -\u003e compile tests ./gradlew test -\u003e run the tests (see useJUnitPlatform comment in order to use JUnit 5) go to build/reports/tests/test the automaticaly gradle generated HTML report (index.html) go to build/test-reults/test for the autmaticaly gradle generated XML test report adding test task in build.gradle:\ntest { useJUnitPlatform() // Java plugin expect Java 4 by default. Add useJUnitPlatform to indicates to use JUnit5 instead testLogging { events 'started', 'skipped', 'failed' // will display specified events on run time exceptionFormat 'full' // show stack trace on failures } }",
    "description": "Project structure for gradle project in Java In build.gradle JAVA plugin APPLICATION plugin Commands Maven Dependencies Testing with Gradle JUnit 5 dependencies Project structure for gradle project in Java single Java project with Gradle\nexample: root + src + main + java + resources + test + java + resources + build + classes -\u003e compiled class files + libs -\u003e generated JAR files A multi-modules project file structure:",
    "tags": [],
    "title": "Gradle for Java-Based applications and libraries",
    "uri": "/notes/gradle-for-java-based-applications-and-libraries/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "How to promote the Quality Best Practices Influence or evangelize Data-Driven decisions Enforce How to promote the Quality Best Practices Two ways to promote the quality best practices are: Influencing the teams or to enforce the best practices. A periodical team reality check about the current practices is the beginning of the process. A team can easily live in their own Ivory Tower or to be biased by this statement: “We always did it that way so why changing it?”.\nInfluence or evangelize Discussions: gather information about current pain points (retrospectives and lessons learned) Involve QA Specialists in all SDLC (or Software Development Life Cycle) 1:1 meetings to get information and expose point of views, share ideas and explain the best practices: it may be easier to convince on person than a group Talk to the teams Provide existing articles and blog posts Tech talk to expose the vision Create training material: guidelines, processes, videos (e.g.: Pluralsight, etc.) Involve people in quality solutions: Organize Workshops (on short term) so that the targeted public becomes active participants Organize Work Groups (on long term) so that the concerned people become active participants To use data as leverage; more information is available in the data-driven decision section Data-Driven decisions To show metrics of the current situation (or challenge the “no metrics” or lack of metrics). It is the also called “Shift-Right” approach in the Software Quality world.\nEnd-users or Customers survey can be used (e.g.: use a post-transaction experience survey on the mobile app or web site) White-Label partner surveys Use production dashboards (examples: Datadog or SignalFX metrics) and performance tools (e.g.: WebVitals, etc.) to monitor APIs, SQLs, etc. Daily performance emails can be sent to the engineering teams NPS and CSAT surveys: NPS vs CSAT definitions Real Customers Journey (traces of the end-users through the applications) Use support CIM dashboards: customer tickets, production tickets, ticket aging/bug fix time, tickets by services or teams, RCA (find the issues caused by quality process) CIM stands for Customer Interaction Management A way to generate CIM data is to use a tool like eazy bi Correlation between test coverage (static code) and production issues It is also possible to get test coverage from a live service using a tool like Sealights Compile the functional testers (manual tests) created Tickets/JIRA vs current team or service test coverage Create Success Stories: start with one team and use the before/after statistics GIT statistics (e.g. with a tool like GitPrime (now Pluralsight Flow)) in order to correlate bugs/tickets and developers stats (e.g.: number of commits, active days, etc.) I.e.: low stats may be caused by developers not working on features or test automations because they are working on manual tests or production support Correlate the release cycle (how often the team releases or time to market) and test coverage; better/reliable test coverage and test regression suite will shorten the release cycle. Gather statistic about how much time it takes to refactor an existing feature? Better test coverage will result in faster refactoring Enforce Contact the best practices stakeholders and ask for support (I.e.: to push the best practices downward) To fix deadlines (must be adopted by specific dates) Enforce with Gates, checklists, DoD, etc. Set Quality specific OKRs Assign Quality trainings",
    "description": "How to promote the Quality Best Practices Influence or evangelize Data-Driven decisions Enforce How to promote the Quality Best Practices Two ways to promote the quality best practices are: Influencing the teams or to enforce the best practices. A periodical team reality check about the current practices is the beginning of the process. A team can easily live in their own Ivory Tower or to be biased by this statement: “We always did it that way so why changing it?”.",
    "tags": [],
    "title": "How to promote the Quality Best Practices",
    "uri": "/blogs/how-to-promote-the-quality-best-practices/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac From Windows Create GitHub Page Create or Clone exiting Hugo repository Clone the GitHub repo to publish on the hugo site Create a new Hugo site Clone the Theme directory Update the Hugo config.toml file content with team Build Verify if the configuration is good Update the logo Use github documentation repo as content Try locally Build for GitHub Page Links and references Training References Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac brew install hugo\nFrom Windows choco install hugo -confirm\nReference: [https://gohugo.io/getting-started/installing#chocolatey-windows]\nCreate GitHub Page From GitHub.com -\u003e Create a new Repository, e.g., AlainBouchard.github.io\nCreate or Clone exiting Hugo repository In this example, we’ll use project engineering-notes-site\nClone the GitHub repo to publish on the hugo site engineering-notes-site\u003e git clone git@github.com:AlainBouchard/engineering-notes-site.git Create a new Hugo site engineering-notes-site\u003e hugo new site --force engineering-notes-site\u003e ls archetypes config.toml content data layouts public static themes Clone the Theme directory Example of theme, many can be found on Hugo site.\nHugo Theme Source: [https://themes.gohugo.io/themes/hugo-theme-relearn/]\ncd themes themes\u003e git clone git@github.com:McShelby/hugo-theme-relearn.git Update the Hugo config.toml file content with team \u003e vi config.toml baseURL = \"https://AlainBouchard.github.io/\" languageCode = \"en-us\" title = \"Alain Bouchard's Engineering Notes\" theme = \"hugo-theme-relearn\" [outputs] home = [ \"HTML\", \"RSS\", \"JSON\"] [module] [[module.imports]] path = \"github.com/alain-bouchard-quality/engineering-notes\" [[module.imports.mounts]] source = \"content\" target = \"content\" Build \u003e hugo -t hugo-theme-relearn Verify if the configuration is good \u003e hugo server Update the logo Create statics/logo.png\nCreate layouts/partials/logo.html\n\u003cimg src=\"logo.png\"\u003e Link (git submodule) the documents into the GitHub Page\n\u003e rm -Rf public \u003e git submodule add -b master git@github.com:AlainBouchard/AlainBouchard.github.io.git public \u003e git remote -v Expect the public directory to get created with the repo content\nUse github documentation repo as content In document source github repo Source repo example: github.com/AlainBouchard/engineering-notes\n\u003e hugo mod init github.com/AlainBouchard/engineering-notes In hugo project github repo Hugo repo example: github.com/AlainBouchard/engineering-notes-site\n\u003e hugo mod init github.com/AlainBouchard/engineering-notes-site Verify if the module work:\n\u003e hugo mod get github.com/AlainBouchard/engineering-notes Expect go.sum to get created:\ngithub.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6 h1:0s4tNjEN0+jGCkNEObTbnW+akRJD5RdjJ8pPsUU5ROU= github.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6/go.mod h1:Z6BTmpjCul+gI1Za7PY2E0LgyfIJpyeII/zcRE3e654= Expect go.mod to get updated:\nmodule engineering-notes-site go 1.18 require github.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6 // indirect Try locally Build\n\u003e hugo \u003e hugo server --disableFastRender --ignoreCache Expect Hugo to run on [http://localhost:1313/]\nBuild for GitHub Page build for theme\n\u003e hugo -t hugo-theme-relearn \u003e go the `/public` \u003e git status \u003e git add . \u003e git commit -m \"xyz\" \u003e git push expect the GitHub Page repo to be updated.\nLinks and references Training (10 min)\nReferences Master Hugo Modules: Handle Content Or Assets As Modules/ Working with Hugo Module Locally Hugo Relearn Theme \u003e Content \u003e Pages organization Learn Theme for Hugo \u003e Shortcodes \u003e Children How to Add Table Of Contents to a Hugo Blog",
    "description": "Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac From Windows Create GitHub Page Create or Clone exiting Hugo repository Clone the GitHub repo to publish on the hugo site Create a new Hugo site Clone the Theme directory Update the Hugo config.toml file content with team Build Verify if the configuration is good Update the logo Use github documentation repo as content Try locally Build for GitHub Page Links and references Training References Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac brew install hugo",
    "tags": [],
    "title": "Hugo",
    "uri": "/notes/hugo/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "IntelliJ cheat sheet IntelliJ cheat sheet Create Main Method Create Main Method Create the main method: write psvm and select the method to create:\npublic static void main(String[] args) { }",
    "description": "IntelliJ cheat sheet IntelliJ cheat sheet Create Main Method Create Main Method Create the main method: write psvm and select the method to create:\npublic static void main(String[] args) { }",
    "tags": [],
    "title": "IntelliJ Cheat Sheet",
    "uri": "/notes/intellij-ide/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Java Object-Oriented Programming Class blueprint Static vs Non-Static members Enum blueprint Main class blueprint Princibles Encapsulation Inheritance Polymorphism Abstraction Usefull build-in JAVA commands and other information Method Reference Operator (or ::) The Arbitrary Number of Arguments (or ... as a function argument) Predicate The final keyword Generics Java Object-Oriented Programming Class blueprint // Class blueprint public class MyClass { // Can have attributes... private String attribute1; // private attribute member, getter/setter are needed MyEnum myEnum; static string staticAttribute1 = \"Static Attribute\"; // belongs to the class private final Integer myFinalInteger; // can't be modified or be overridden by any subclasses // Constructor MyClass() { // Initiate the instance of MyClass class. this(\"'this' is used to call alternate constructors from within a constructor\"); // optional code lines... } // Constructor MyClass(String message) { // Initiate the instance of MyClass class. } // Setter for private attribute public void setAttribute1(attribute1) { this.attribute1 = attribute1; // note: \"this\" keyword is needed to disambuguate the variable reference. } // Getter for private attribute public String getAttribute1() { return attribute1; } // Methods or behaviours: void myMethod() { // no modifier so the method is accessible from its package only. this.attribute1 = 'xyz'; } private void myPrivateMethod() { // private so only accessible from this classs } protected void myProtectedMethod() { // protected method so accessible from this class and sub-classes } public void myPublicMethod() { // public method so accessible from anywhere; } // Static method static void myStaticMethod() { // A static method does not rely on any non-static attribute or member. } } Static vs Non-Static members Non-Static Member: is accessible from an instance and belongs to that instance Static Member: is accessible through the class and belongs to that class Static members can be accessed using the class name, example: Enum blueprint // Enum blueprint public enum MyEnum { CONSTANT1, CONSTANT2, ..., CONSTANTN } Main class blueprint // Main class public class Main { public static void main(String[] arg) { MyClass myClass = new MyClass('abc'); // Create an object of type MyClass myClass.myMethod('xyz'); // Call a method from my object } } Princibles Encapsulation allow us to bind together data and related functionality prevent classes from becoming tightly coupled easily modify the inner workings of one class without affecting the rest of the program restrictions we need a clear interface between a given class and the rest of the program everything can’t have direct access make the class attributes hideen from other classes using encapsulation provide a clear interface through public methods benefits clear pathways for classes to communicate less code changes required for a refactoring change less likely for an attribute to be overwritten with an invalid or null value unexpectedly Access Modifiers in Java: private: only visible in class that the member lives in no modifier: only visible in package the member lives in protected: visible to the package and all subclasses public: accessible everywhere within the program Inheritance allow us to create class hierarchies Subclass (child class) inherits properties referred to as the child class Superclass (parent class) is inherited from referred to as the parent class promotes code reusability and scalability leveraging inheritance: a class can only have one superclass but multiple subclasses if multiple super classes is needed then multilevel inheritance is required public class MySuperClass { protected String a1; private String a2; MySuperClass(String a1, String a2) { // Super Class constructor this.a1 = a1; this.a2 = a2; } public void myMethod() { // Method of the Super Class } } public class MySubClass extends MySuperClass { MySubClass(String arg1, String arg2) { super(arg1, arg2); // call superclass with arguments // Sub Class constructor } public String getA1() { return super.a1; // this.a1 would also work for this Superclass protected variable; } @Override public void myMethod() { // Override the Super Class Method within the Sub Class } } Polymorphism the ability for an object or function to take amny different forms Java supports two types of polymorphism: run time and compile-time polymorphism it helps to reduce complexity and write reusable code Abstraction helps us with hide implementation complexity\nJava supports abstact classes and interfaces\nhelps by fixing inputs and outputs and giving general idea of what the system does\nan abstract class:\nalmost like a template can’t be instencied other classes can extend the abstract calss and implement the appropriate functionality an example of an abstract class:\npublic abstract class myAbstractClass { // The class requires the `abstract` keyword since it contains an abstract method. private final String myString; // final String (aka constant) protected abstract void myAbstractMethod(); // an abstract method is not implemented! } public class myOtherClass extends MyAbstractClass { // variables, constructors... etc. @Override protected abstract void myAbstractMethod() { // The abstract method from the abstract class must be implemented by the sub-class. } } an interface is:\na set of method signatures for to-be-implemented functionality a specification for a set of behaviors without implementation can’t be instencied public interface MyInterface { Long myMethod1(); // No method implementation void myMethod2(); } public class myClassImplementingMyInterface implements MyInterface { @Override public Long myMethod1() { // the implementation for myMethod1 method } @Override public void myMethod2() { // the implementation for myMethod2 method } } Consider using abstract classes if any of these statements apply to your situation:\nIn the java application, there are some related classes that need to share some lines of code then you can put these lines of code within the abstract class and this abstract class should be extended by all these related classes. You can define the non-static or non-final field(s) in the abstract class so that via a method you can access and modify the state of the Object to which they belong. You can expect that the classes that extend an abstract class have many common methods or fields, or require access modifiers other than public (such as protected and private). Consider using interfaces if any of these statements apply to your situation:\nIt is a total abstraction, All methods declared within an interface must be implemented by the class(es) that implements this interface. A class can implement more than one interface. It is called multiple inheritances. You want to specify the behaviour of a particular data type, but not concerned about who implements its behaviour. Usefull build-in JAVA commands and other information System.out.println(“string…”); Method Reference Operator (or ::) a method reference operator (or ::) is used to call a method by referring to it with the help of its class directly\nlike using a lambda expression, example: // Get the stream Stream\u003cString\u003e stream = Stream.of(\"Geeks\", \"For\", \"Geeks\", \"A\", \"Computer\", \"Portal\"); // Print the stream using lambda method: stream.forEach(s -\u003e System.out.println(s)); // Print the stream using double colon operator stream.forEach(System.out::println); // Both lambda and :: will do the same thing. The Arbitrary Number of Arguments (or ... as a function argument) it means that zero or more String objects (or a single array of them) may be passed as the argument(s) for that method Reference : [http://java.sun.com/docs/books/tutorial/java/javaOO/arguments.html#varargs] important note: the argument(s) passed in this way is always an array - even if there’s just one. Make sure you treat it that way in the method body the argument that gets the ... must be the last in the method signature. So, myMethod(int i, String… strings) is okay, but myMethod(String… strings, int i) is not okay example: public static int myFunction (int ... a) { int sum = 0; for (int i : a) sum += i; return sum; } public static void main( String args[] ) { int ans = myFunction(1,1,1); // could have any number of arguments System.out.println( \"Result is \"+ ans ); } Predicate a Predicate in general meaning is a statement about something that is either true or false. In programming, predicates represent single argument functions that return a boolean value\nexample:\n@FunctionalInterface public interface Predicate\u003cT\u003e { boolean test(T t); } An example with filter() since it does accept a Predicate as parameter:\n// With lambda function: List\u003cInteger\u003e list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); List\u003cInteger\u003e collect = list.stream().filter(x -\u003e x \u003e 5).collect(Collectors.toList()); System.out.println(collect); // [6, 7, 8, 9, 10] // With predicate: Predicate\u003cInteger\u003e noGreaterThan5 = x -\u003e x \u003e 5; List\u003cInteger\u003e list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); List\u003cInteger\u003e collect = list.stream().filter(noGreaterThan5).collect(Collectors.toList()); System.out.println(collect); // [6, 7, 8, 9, 10] More examples with Java 8 Predicate Examples\nThe final keyword Java final keyword is a non-access specifier that is used to restrict a class, variable, and method. If we initialize a variable with the final keyword, then we cannot modify its value if we declare a method as final, then it cannot be overridden by any subclasses if we declare a class as final, we restrict the other classes to inherit or extend it example: final variables: to create constants final classes: to prevent inheritance final methods: to prevent method overriding Generics using generics enable types (classes and interfaces) to be parameters when defining classes, interfaces and methods.\nusing generics give many benefits over using non-generic code\nstronger type checks at compile time:\nJava compiler applies strong type checking to generic code and issues errors if the code violates type safety. Fixing compile-time errors is easier than fixing runtime errors, which can be difficult to find example: // Without Generics List list = new ArrayList(); list.add(\"hello\"); // With Generics List\u003cInteger\u003e list = new ArrayList\u003cInteger\u003e(); list.add(\"hello\"); // will not compile enabling programmers to implement generic algorithms\nby using generics, programmers can implement generic algorithms that work on collections of different types, can be customized, and are type safe and easier to read. elimination of casts\nexample: // Without Generics: List list = new ArrayList(); list.add(\"hello\"); String s = (String) list.get(0); // Need to cast return value to String // With Generics: List\u003cString\u003e list = new ArrayList\u003cString\u003e(); list.add(\"hello\"); String s = list.get(0); // no cast needed Generics type parameters:\nT: Type E: Element K: Key (used in Map) N: Number V: Value (used in Map) Reference: [https://www.journaldev.com/1663/java-generics-example-method-class-interface]",
    "description": "Java Object-Oriented Programming Class blueprint Static vs Non-Static members Enum blueprint Main class blueprint Princibles Encapsulation Inheritance Polymorphism Abstraction Usefull build-in JAVA commands and other information Method Reference Operator (or ::) The Arbitrary Number of Arguments (or ... as a function argument) Predicate The final keyword Generics Java Object-Oriented Programming Class blueprint // Class blueprint public class MyClass { // Can have attributes... private String attribute1; // private attribute member, getter/setter are needed MyEnum myEnum; static string staticAttribute1 = \"Static Attribute\"; // belongs to the class private final Integer myFinalInteger; // can't be modified or be overridden by any subclasses // Constructor MyClass() { // Initiate the instance of MyClass class. this(\"'this' is used to call alternate constructors from within a constructor\"); // optional code lines... } // Constructor MyClass(String message) { // Initiate the instance of MyClass class. } // Setter for private attribute public void setAttribute1(attribute1) { this.attribute1 = attribute1; // note: \"this\" keyword is needed to disambuguate the variable reference. } // Getter for private attribute public String getAttribute1() { return attribute1; } // Methods or behaviours: void myMethod() { // no modifier so the method is accessible from its package only. this.attribute1 = 'xyz'; } private void myPrivateMethod() { // private so only accessible from this classs } protected void myProtectedMethod() { // protected method so accessible from this class and sub-classes } public void myPublicMethod() { // public method so accessible from anywhere; } // Static method static void myStaticMethod() { // A static method does not rely on any non-static attribute or member. } } Static vs Non-Static members Non-Static Member: is accessible from an instance and belongs to that instance Static Member: is accessible through the class and belongs to that class Static members can be accessed using the class name, example: Enum blueprint // Enum blueprint public enum MyEnum { CONSTANT1, CONSTANT2, ..., CONSTANTN } Main class blueprint // Main class public class Main { public static void main(String[] arg) { MyClass myClass = new MyClass('abc'); // Create an object of type MyClass myClass.myMethod('xyz'); // Call a method from my object } } Princibles Encapsulation allow us to bind together data and related functionality prevent classes from becoming tightly coupled easily modify the inner workings of one class without affecting the rest of the program restrictions we need a clear interface between a given class and the rest of the program everything can’t have direct access make the class attributes hideen from other classes using encapsulation provide a clear interface through public methods benefits clear pathways for classes to communicate less code changes required for a refactoring change less likely for an attribute to be overwritten with an invalid or null value unexpectedly Access Modifiers in Java: private: only visible in class that the member lives in no modifier: only visible in package the member lives in protected: visible to the package and all subclasses public: accessible everywhere within the program Inheritance allow us to create class hierarchies Subclass (child class) inherits properties referred to as the child class Superclass (parent class) is inherited from referred to as the parent class promotes code reusability and scalability leveraging inheritance: a class can only have one superclass but multiple subclasses if multiple super classes is needed then multilevel inheritance is required public class MySuperClass { protected String a1; private String a2; MySuperClass(String a1, String a2) { // Super Class constructor this.a1 = a1; this.a2 = a2; } public void myMethod() { // Method of the Super Class } } public class MySubClass extends MySuperClass { MySubClass(String arg1, String arg2) { super(arg1, arg2); // call superclass with arguments // Sub Class constructor } public String getA1() { return super.a1; // this.a1 would also work for this Superclass protected variable; } @Override public void myMethod() { // Override the Super Class Method within the Sub Class } } Polymorphism the ability for an object or function to take amny different forms Java supports two types of polymorphism: run time and compile-time polymorphism it helps to reduce complexity and write reusable code Abstraction helps us with hide implementation complexity",
    "tags": [],
    "title": "Java Object-Oriented Programming",
    "uri": "/notes/java-object-oriented-programming/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Java Spring Boot 2 Terminology Getting Started Why Spring Boot? Spring Initializr Inversion of Control Proxies Data Access in Spring Spring Data Embeedded DB with Spring Boot Repository with Spring Data Using remote database Service Tier Utilizing IoC Service abstraction Spring Service Object Web pages with Spring Controller Java Spring Boot 2 Terminology POJO : Plain Old Java Object (may have more that setters/getters in Spring world) Java Beans : Simple objects with only setters/getters Spring Beans : POJOs confiugered in the application context DTO : Data Transfer Objects are Java Beans used to move state between layers IOC : Inversion Of Control IoC provides mechanism of dependency injection Application Context wraps the Bean Factory which serves the beans at the runtime of the application Spring Boot provides auto-configuration of the Application Context Getting Started Why Spring Boot? Support rapid development Remove boilerplate of application setup Many uses Cloud Native support but also traditional Key Aspects Embedded tomacat (or others) Auto-configuration of Application Context Automatic Servlet Mappings Database support and Hibermate/JPA dialect Automatic Controller Mappings Auto Config Default opiniated configuration Very to override defaults Configuration on presence Spring Initializr start.spring.io Spring Boot: pick latest released version (ie. 2.5.6) Packaging: Jar Add Dependencies: Pring Web TBD… Generate Now it can build and run as is:\njava -jar target/xyz-0.0.1-SNAPSHOT.jar\nUse Chrome: localhost:8080\nInversion of Control Container mainains your class dependencies Objects injected at runtime or startup time An object accepts the dependencies for construction instead of constructing them Spring IoC Bean Factory Application Context References Analysis of construction order Proxies Beans in Bean Faactory are proxied Annitations drive proxies Annitations are easy extension points, for your own abstracts too Method calling order matters Data Access in Spring Spring Data Provides a common set of interfaces Provides a common naming convention Provides aspected behavior Provides Repository and Data Mapping convention Benefits of Spring Data Remove boilerplate code Allows for swapping datasources easier Allows to focus on buisiness logic Key Components Repository Interface Entity Object DataSource no accessed directly Embeedded DB with Spring Boot Needed dependencies:\norg.springframework.boot:spring-boot-starter-data-jpa com.h2database:h2 Set application.properties:\nlogging.level.org.springframework.jdbc.datasource.init.ScriptUtils=debug : by default is set to info spring.jpa.hibernate.ddl-auto=none : don’t create schema, and just connect to DB Repository with Spring Data Java Persistence API (or JPA)\nMapping Java objects to DB tables and vice versa is called Object-relational Mapping (ORM) JPA permits the developer to works directly with objects rather tahn with SQL statements Based on Annotations Entity\nA class which should be persisted in a database it must be annotated with javax.persistence.Entity JPA uses a database table for every entity Persisted instances of the class will be represented as one row in the table JPA allows to auto-generate the primary key in the database via the @GeneratedValue annotation By default, the table name corresponds to the class name. You can change this with the addition to the annotation @Table(name=\"NEWTABLENAME\") Code Example @Entity @Table(name=\"ROOM\") public class Room { @Id @GeneratedValue(strategy = GenerationType.AUTO) @Column(name=\"ROOM_ID\") private long id; @Column(name=\"NAME\") private String name; @Column(name=\"ROOM_NUMBER\") private String roomNumber; @Column(name=\"BED_INFO\") private String bedInfo; public long getId() { return id; } public void setId(long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getRoomNumber() { return roomNumber; } public void setRoomNumber(String roomNumber) { this.roomNumber = roomNumber; } public String getBedInfo() { return bedInfo; } public void setBedInfo(String bedInfo) { this.bedInfo = bedInfo; } @Override public String toString() { return \"Room{\" + \"id=\" + id + \", name='\" + name + '\\'' + \", roomNumber='\" + roomNumber + '\\'' + \", bedInfo='\" + bedInfo + '\\'' + '}'; } } Create a CrudRepository interface for the created Entity: Code Example @Repository public interface RoomRepository extends CrudRepository\u003cRoom, Long\u003e { // Room is the Entity class // Long is the ID type } Create a Component Event:\nA @Component Annotation is automatically picked by Spring Code Example @Component public class AppStartupEvent implements ApplicationListener\u003cApplicationReadyEvent\u003e { private final RoomRepository roomRepository; public AppStartupEvent(RoomRepository roomRepository) { this.roomRepository = roomRepository; } @Override public void onApplicationEvent(ApplicationReadyEvent event) { Iterable\u003cRoom\u003e rooms = this.roomRepository.findAll(); rooms.forEach(System.out::println); } } Using remote database Replace the H2 database for an other database, example a PostgreSQL:\ndependencies: org.postgresql:postgresql In application.properties:\nspring.jpa.database=postgresql spring.datasource.url=jdbc:postgresql://localhost:5432/dev spring.datasource.username=postgres spring.datasource.password=postgres Service Tier Utilizing IoC Why use IoC?\nAllows you to focus on contracts Develoip business code only, leave constuction to the container Build intermediate abstractions Produce clean code Srping and IoC:\nIoC container is configured by developer Spring maintains handles to objects constucted at startup Spring serves singletons to classes during construction Spring maintains lifecycle of beans Developer only accesses the application context Service abstraction Why building Service Abstractions:\nEncapsulate layers? Abstract 3rd partys APIs Simplify implementations Swap out implementations as runtime (ie. factory pattern) How to build one?\nDefine our interface (or class) Create the API Inject the dependencies Annotate or configure (classes) Code the implemantation Spring Service Object We mark beans with @Service to indicate that they’re holding the business logic. Besides being used in the service layer, there isn’t any other special use for this annotation. Starting with Spring 2.5, the framework introduced annotations-driven Dependency Injection. The main annotation of this feature is @Autowired. It allows Spring to resolve and inject collaborating beans into our bean. Using @Autowired or either properties or setters/getters isn’t a good practice or easy to test; Use final properties with constructors to have immutable object. If more than one constructor is defined then using @Autowired on the default one will make Spring to use it. Code Example @Service public class ReservationService { private final RoomRepository roomRepository; private final GuestRepository guestRepository; private final ReservationRepository reservationRepository; @Autowired // optional if only one constructor public ReservationService(RoomRepository roomRepository, GuestRepository guestRepository, ReservationRepository reservationRepository) { this.roomRepository = roomRepository; this.guestRepository = guestRepository; this.reservationRepository = reservationRepository; } // Business logic here... } Web pages with Spring Controller Model View Controller (or MVC)\nFundamental pattern for Web application development The Model is the data The View is the visual display that is populated The Controller wires the view with the model Spring Controller\nSpring bean Annotated for the servlet mapping Responds to incoming web requests Output a view or raw data Template Engines\nSpring supports several Thymeleaf most popular Provides a DSL for HTML leaving raw html documents Placeholders for dynamic data Rendiring engin allows for final products Code Example @Controller @RequestMapping(\"/reservations\") public class RoomReservationController { private final DateUtils dateUtils; private final ReservationService reservationService; public RoomReservationController(DateUtils dateUtils, ReservationService reservationService) { this.dateUtils = dateUtils; this.reservationService = reservationService; } @RequestMapping(method = RequestMethod.GET) public String getReservations(@RequestParam(value=\"date\", required=false) String dateString, Model model){ Date date = this.dateUtils.createDateFromDateString(dateString); List\u003cRoomReservation\u003e roomReservations = this.reservationService.getRoomReservationsForDate(date); model.addAttribute(\"roomReservations\", roomReservations); return \"roomres\"; } } Can use Thymeleaf to create HTML pages.\nAdd the web page to src/main/resources/templates",
    "description": "Java Spring Boot 2 Terminology Getting Started Why Spring Boot? Spring Initializr Inversion of Control Proxies Data Access in Spring Spring Data Embeedded DB with Spring Boot Repository with Spring Data Using remote database Service Tier Utilizing IoC Service abstraction Spring Service Object Web pages with Spring Controller Java Spring Boot 2 Terminology POJO : Plain Old Java Object (may have more that setters/getters in Spring world) Java Beans : Simple objects with only setters/getters Spring Beans : POJOs confiugered in the application context DTO : Data Transfer Objects are Java Beans used to move state between layers IOC : Inversion Of Control IoC provides mechanism of dependency injection Application Context wraps the Bean Factory which serves the beans at the runtime of the application Spring Boot provides auto-configuration of the Application Context Getting Started Why Spring Boot? Support rapid development Remove boilerplate of application setup Many uses Cloud Native support but also traditional Key Aspects Embedded tomacat (or others) Auto-configuration of Application Context Automatic Servlet Mappings Database support and Hibermate/JPA dialect Automatic Controller Mappings Auto Config Default opiniated configuration Very to override defaults Configuration on presence Spring Initializr start.spring.io Spring Boot: pick latest released version (ie. 2.5.6) Packaging: Jar Add Dependencies: Pring Web TBD… Generate Now it can build and run as is:",
    "tags": [],
    "title": "Java Spring Boot 2",
    "uri": "/notes/java-spring-boot2/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Java with Rest-Assured Pattern API response Deserialize a response Tools Java with Rest-Assured Rest-Assured link: [https://rest-assured.io] can get latest version: [https://mvnrepository.com] jackson databind package can be used for data-binding hamcrest package can be used for matchers org.hamcrest.Matchers.* Pattern using the Given, When and Then pattern\nthe Given specify prerequisites the When describe the action to take the Then describe the expected result using JUnit 5: @Test public void getTest() { String endpoint = \"http://localhost:8888/a/b/c\"; var response = given(). // using easy to read format for doc only queryParam(\"id\", \"2\"). when(). get(endpoint). then(); } @Test public void postTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().post(endpoint).then(); } @Test public void putTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().put(endpoint).then(); } @Test public void deleteTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\" } \"\"\" var response = given().body(body).when().delete(endpoint).then(); } API response validate the status code\nassertThat example: @Test public void getTest() { String endpoint = \"http://localhost:8888/a/b/c\"; given().queryParam(\"key\", \"value\") .when().get(endpoint) .then().assertThat() .statusCode(200) // check status code is OK/200 .body(\"key\", equalTo(\"value\")) // check for response body key = value .body(\"records.size()\", greaterThan(0)) // check for response body record array to have 1+ items .body(\"records.id\", everyItem(notNullValue())) // make sure each records.id item from the array is not null .body(\"records.id[0]\", equalTo(8)) // make sure first records.id item = 0 .header(\"Content-Type\", equalTo(\"application/json\")); // verify the headers content-type field } Deserialize a response a class can be used to deserialize a response\nexample: @Test public void deserializeTest() { String endpoint = \"http://localhost:8888/a/b/c\"; MyResponseClass request = new MyResponseClass(1, 2, 3, \"value\"); MyResponseClass response = given() .queryParam(\"key\",\"value\") .when() .get(endpoint) .as(MyResponseClass.class); assertThat(response, samePropertyValuesAs(request)); // comparing every property of the classes } Tools response.log().body() -\u003e print the response to console",
    "description": "Java with Rest-Assured Pattern API response Deserialize a response Tools Java with Rest-Assured Rest-Assured link: [https://rest-assured.io] can get latest version: [https://mvnrepository.com] jackson databind package can be used for data-binding hamcrest package can be used for matchers org.hamcrest.Matchers.* Pattern using the Given, When and Then pattern\nthe Given specify prerequisites the When describe the action to take the Then describe the expected result using JUnit 5: @Test public void getTest() { String endpoint = \"http://localhost:8888/a/b/c\"; var response = given(). // using easy to read format for doc only queryParam(\"id\", \"2\"). when(). get(endpoint). then(); } @Test public void postTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().post(endpoint).then(); } @Test public void putTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().put(endpoint).then(); } @Test public void deleteTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\" } \"\"\" var response = given().body(body).when().delete(endpoint).then(); } API response validate the status code",
    "tags": [],
    "title": "Java with Rest-Assured",
    "uri": "/notes/java-rest-assured/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "@ToString The Class @ToString tag will replace the Overriding ToString method automatically.\n@AllArgsConstructor and @NoArgsConstructor The Class @AllArgsConstructor and @NoArgsConstructor tags will automatically replace the constructors.\n@EqualsAndHashCode The Class @EqualsAndHashCode tag will allow us to compare 2 objects.\n@Log4J The Class @Log4J tag can replace the logger initiator in the class. The LOGGER can be replaced by log.\n@Data The Class @Data tag will replace the @ToString, @RequiredArgsConstructor, @EqualsAndHashCode, @Setter (for non-final attributes) and the @Getter.",
    "description": "@ToString The Class @ToString tag will replace the Overriding ToString method automatically.\n@AllArgsConstructor and @NoArgsConstructor The Class @AllArgsConstructor and @NoArgsConstructor tags will automatically replace the constructors.\n@EqualsAndHashCode The Class @EqualsAndHashCode tag will allow us to compare 2 objects.\n@Log4J The Class @Log4J tag can replace the logger initiator in the class. The LOGGER can be replaced by log.\n@Data The Class @Data tag will replace the @ToString, @RequiredArgsConstructor, @EqualsAndHashCode, @Setter (for non-final attributes) and the @Getter.",
    "tags": [],
    "title": "Lombok Java Library",
    "uri": "/notes/java-lombok/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "MongoDB Create a doc Insert to a collection Show documents Searching using regex Sort, limit and skip Operators and arrays Updating a document Update one field Add one field Remove one field Increment a counter by N updating an array of a document Delete a document MongoDB show dbs show collections\njavascript shell\nuse \u003cdb name\u003e db.getName() -\u003e will return current DB Create a doc doc = {\"key\":value, ...} Insert to a collection db.\u003ccollection\u003e.insertOne(doc) Show documents db.\u003ccollection\u003e.find() db.\u003ccollection\u003e.find().pretty() -\u003e json file formated db.\u003ccollection\u003e.find({}, {\"title\": 1}) -\u003e will return only the title for all documents db.\u003ccollection\u003e.find({\"title\": \"tacos\"}) -\u003e will return all documents that match the condiction, ex. “title”:“tacos” Searching using regex db.\u003ccollection\u003e.find(\"title\": {$regex: /taco/i}}, {title: 1}) -\u003e returns the title for all documents that title march regex /taco/i\nSort, limit and skip db.\u003ccollection\u003e.find().count() db.\u003ccollection\u003e.find({}, {\"title\": 1}).sort({\"title\": 1}) where 1 for asc and -1 for desc db.\u003ccollection\u003e.find().limit(2) -\u003e returns 2 results only db.\u003ccollection\u003e.find({}, {\"title\": 1}).skip(1) -\u003e will return the items skipping first one Operators and arrays greater than: $gt less than: $lt less than or equal to: $lte db.\u003ccollection\u003e.find({ \"cook_time\": { $lte: 30 }}, {\"title\":1}) -\u003e find items with cook_time \u003c= 30 db.\u003ccollection\u003e.find({ \"cook_time\": { $lte: 30 }, \"prep_time\": { $lte: 10 } }, {\"title\":1}) -\u003e find items with cook_time \u003c= 30 AND prep_time \u003c= 10 db.\u003ccollection\u003e.find( { $or: [{ \"cook_time\": { $lte: 30 }, \"prep_time\": { $lte: 10 } }]}, {\"title\":1}) -\u003e find items with cook_time \u003c= 30 OR prep_time \u003c= 10 $all operator: require all items in a doc arrays $in operator: require one from the arrays db.\u003ccollection\u003e.find( { \"tags\" : {$all: [\"easy\", \"quick\"]}}, {\"title\":1, \"tag\":1}) -\u003e returns item with both tags “easy” or “quick” db.\u003ccollection\u003e.find( { \"tags\" : {$in: [\"easy\", \"quick\"]}}, {\"title\":1, \"tag\":1}) -\u003e returns item with either tags “easy” or “quick” Updating a document $set $unset $inc -\u003e increment Update one field db.\u003ccollection\u003e.updateOne(find, modification) db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $set: {\"title\":\"thin crust pizza\"}}) Add one field db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $set: {\"vegan\":true}}) Remove one field db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $unset: {\"vegan\":1}}) -\u003e one for true?!?! Increment a counter by N db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $inc: {\"likes_count\":1}}) -\u003e increment likes_count field by 1 updating an array of a document db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $push: {\"likes\":60}}); -\u003e will add value 60 to the likes array db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $pull: {\"likes\":60}}); -\u003e will remove value 60 from the likes array Delete a document db.\u003ccollection\u003e.deleteOne({\"_id\":\"....\"})",
    "description": "MongoDB Create a doc Insert to a collection Show documents Searching using regex Sort, limit and skip Operators and arrays Updating a document Update one field Add one field Remove one field Increment a counter by N updating an array of a document Delete a document MongoDB show dbs show collections\njavascript shell\nuse \u003cdb name\u003e db.getName() -\u003e will return current DB Create a doc doc = {\"key\":value, ...} Insert to a collection db.\u003ccollection\u003e.insertOne(doc) Show documents db.\u003ccollection\u003e.find() db.\u003ccollection\u003e.find().pretty() -\u003e json file formated db.\u003ccollection\u003e.find({}, {\"title\": 1}) -\u003e will return only the title for all documents db.\u003ccollection\u003e.find({\"title\": \"tacos\"}) -\u003e will return all documents that match the condiction, ex. “title”:“tacos” Searching using regex db.\u003ccollection\u003e.find(\"title\": {$regex: /taco/i}}, {title: 1}) -\u003e returns the title for all documents that title march regex /taco/i",
    "tags": [],
    "title": "MongoDB",
    "uri": "/notes/mongodb/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Python and PyTest PyTest Why PyTest? requirements.txt content for pytest Run a test pytest.ini configuration file example Tox tox.ini Test file skeleton Test class skeleton Fixtures Python Class file skeleton Commonly used functions and examples Python and PyTest Working with multiple python versions may cause issues and confusion. It is recommended to specify the python version to use.\nWorking with python 3.9:\npy -3.9 -m \u003ccommand\u003e It is recommended to upgrade the pip version:\npy -3.9 -m pip install --upgrade pip It is recommended to install tox, which is a virtual environment (venv) manager for Python.\npy -3.9 -m pip install tox To run tests with tox, the following example assumes that pytest has been configured by the tox.ini commands parameter:\npy -3.9 -m tox --recursive -- \u003cpytest parameters\u003e PyTest Why PyTest? allow to run a standalone test function as its own case easy to read syntax, allowing you to use the standard assert method powerful CLI automates test setup, teardown, and common test scenarios (uses fixtures) Great to use with CI tools like Jenkins, Travis, Circle CI, etc. actively maintained with participatory open-source community requirements.txt content for pytest coverage===xxx pytest===xxx pytest-cov===xxx pytest-flakes===xxx pytest-pep8===xxx pytest-pythonpath==xxx docker pytest-flakes will make pytest use PyFlake and Flake8. It will make pytest and python use a Linter and code style checker. Run a test Get pytest help:\npytest -h Run all tests:\npytest Run test using keyword for filename\npytest -k \u003ctest_name_keyword\u003e Explore test coverage of a script - requires the pytest-cov package to be installed:\npytest --cov scripts pytest.ini configuration file example [pytest] # Configure the logging within PyTest (the configuration won't work if configured in test files) log_cli = 1 log_cli_level = WARNING log_cli_format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s) log_cli_date_format=%Y-%m-%d %H:%M:%S # Filter Warnings filterwarnings = ignore::FutureWarning # Uses classes with prefix Test as test files python_files = Test*.py Tox Tox aims to automate and standardize testing in Python. It is part of a larger vision of easing the packaging, testing and release process of Python software.\ntox.ini Tox may be used along with a tox.ini configuration file.\n[framework] files = tests coverages = --cov=src [tox] envlist = py3.9 skip_missing_interpreters = false skipsdist = true toxworkdir = tmp [flake8] max-line-length = 159 # E501: Ignore max line length # ignore = E501 [pytest] norecursedirs = .cache tmp # pytest-spec configuration spec_header_format = {module_path}: spec_test_format = {result} {name} [testenv] deps = -rrequirements.txt # Only forward the environment variables with the following prefix. passenv = PYTHON_SANDBOX_* commands = flake8 src tests --exclude=__init__.py pytest -p no:cacheprovider --spec --durations=5 --cov-config .coveragerc --cov-report term-missing {posargs} {[framework]coverages} {[framework]files} Test file skeleton from path.to.class.to.test import ClassName from pytest # to use pytest Context Manager def test_name(): obj = ClassName(\"value1\", \"value2\") assert obj.value1 == \"value1\" assert obj.value2 == \"value2\" def test_with_exception_context_manager(): with pytest.raises(ValueError) as ex: # Code that will raise an exception. obj = ClassName(\"value1\", \"value2\") obj.raises_exception() assert str(ex.value) == \"this is an exception!\" Test class skeleton The Test Class will work like the fixtures from the test file.\n# conftest.py import pytest import logging @pytest.fixture(scope=\"session\", autouse=True) def set_logging() -\u003e None: logging.info(\"set_logging on conftest.py\") # TestExample.py import logging class TestExample: @classmethod def setup_class(cls): logging.info(\"starting class: {} execution\".format(cls.__name__)) @classmethod def teardown_class(cls): logging.info(\"starting class: {} execution\".format(cls.__name__)) def setup_method(self, method): logging.info(\"starting execution of tc: {}\".format(method.__name__)) def teardown_method(self, method): logging.info(\"starting execution of tc: {}\".format(method.__name__)) def test_tc1(self): logging.info(\"running tc1\") assert True def test_tc2(self): logging.info(\"running tc2\") assert True The output of this example Test Class will be:\n============================= test session starts ============================= collecting ... collected 2 items TestExample.py::TestExample::test_tc1 ------------------------------- live log setup -------------------------------- 2022-07-18 12:57:45 [ INFO] set_logging on conftest.py (conftest.py:7) 2022-07-18 12:57:45 [ INFO] starting class: TestExample execution (TestExample.py:7) 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc1 (TestExample.py:14) -------------------------------- live log call -------------------------------- 2022-07-18 12:57:45 [ INFO] running tc1 (TestExample.py:20) PASSED [ 50%] ------------------------------ live log teardown ------------------------------ 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc1 (TestExample.py:17) TestExample.py::TestExample::test_tc2 ------------------------------- live log setup -------------------------------- 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc2 (TestExample.py:14) -------------------------------- live log call -------------------------------- 2022-07-18 12:57:45 [ INFO] running tc2 (TestExample.py:24) PASSED [100%] ------------------------------ live log teardown ------------------------------ 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc2 (TestExample.py:17) 2022-07-18 12:57:45 [ INFO] starting class: TestExample execution (TestExample.py:11) ============================== 2 passed in 0.02s ============================== Process finished with exit code 0 A session configuration can be done in the conftest.py as shown in the example. It will be run once only in the session setup.\nFixtures A test fixture is a concept used in both electronics and software. It’s a piece of software or device that sets up a system to satisfy certain preconditions of the process. Its biggest advantage is that it provides consistent results so that the test results can be repeatable. Examples of fixtures could be loading a test set to the database, reading a configuration file, setting up environment variables, etc.\nA pytest fixture has a specific scope. By default, the scope is a function. Pytest fixtures have five different scopes: function, class, module, package, and session. The scope basically controls how often each fixture will be executed.\nOrder of priority:\nsession (higher priority) package module class function (lower priority) Function THe default scope is function: `scope=“function” and therefore may be omitted.\nimport pytest from datetime import datetime @pytest.fixture() def only_used_once(): with open(\"app.json\") as f: config = json.load(f) return config @pytest.fixture() def light_operation(): return \"I'm a constant\" @pytest.fixture() def need_different_value_each_time(): return datetime.now() Class The scope=\"class\" run before any function or test of the Test Class.\n@pytest.fixture(scope=\"class\") def dummy_data(request): request.cls.num1 = 10 request.cls.num2 = 20 logging.info(\"Execute fixture\") @pytest.mark.usefixtures(\"dummy_data\") class TestCalculatorClass: def test_distance(self): logging.info(\"Test distance function\") assert distance(self.num1, self.num2) == 10 def test_sum_of_square(self): logging.info(\"Test sum of square function\") assert sum_of_square(self.num1, self.num2) == 500 Special usage of the keyword yield in a fixture: the code before the yield keyword will be executed before the test functions of the Test Class while the code after the yield keyword will be executed after the test functions of the Test Class.\n@pytest.fixture(scope=\"class\") def prepare_db(request): # pseudo code connection = db.create_connection() request.cls.connection = connection yield connection = db.close() @pytest.mark.usefixtures(\"prepare_db\") class TestDBClass: def test_query1(self): assert self.connection.execute(\"..\") == \"...\" def test_query2(self): assert self.connection.execute(\"..\") == \"...\" Module and package The scope=\"module\" runs the fixture per module while the scope=\"package\" runs by package. The scope module is usually used more often than the scope package. The difference between scope function and scope module is that the scope module will only be run once, even if used in many functions in the module.\n@pytest.fixture(scope=\"module\") def read_config(): with open(\"app.json\") as f: config = json.load(f) logging.info(\"Read config\") return config def test1(read_config): logging.info(\"Test function 1\") assert read_config == {} def test2(read_config): logging.info(\"Test function 2\") assert read_config == {} Session The scope=\"session\" is only run once every time pytest is run. A per-directory conftest.py file will be executed once per pytest execution.\n# test/conftest.py @pytest.fixture(scope=\"session\") def read_config(): with open(\"app.json\") as f: config = json.load(f) logging.info(\"Read config\") return config # test/test_code1.py def test1(read_config): logging.info(\"Test function 1\") assert read_config == {} def test2(read_config): logging.info(\"Test function 2\") assert read_config == {} # test/test_code2.py def test3(read_config): logging.info(\"Test function 3\") assert read_config == {} def test4(read_config): logging.info(\"Test function 4\") assert read_config == {} Using conftest.py for common functions:\nstores common utility test fixtures and extension code often referred to as hooks pytest collects the fixtures in this file so they are globally accessible within the testing directory it must be placed under your /tests directory good practice to cross-reference this file when reading a testing suite conftest.py modularization It is possible to modularize the conftest.py file when it is getting too big.\n# referring to modules: # tests/utils/db.py # tests/utils/network.py # in conftest.py pytest_plugins = [ \"tests.utils.db\", \"tests.utils.network\" ] The autouse=True fixtures must stay in the conftest.py file.\nAutouse The fixture parameter autouse=True will make the fixture used automatically even if the fixture isn’t called by the test function.\n@pytest.fixture(autouse=True) def function_autouse(): logging.info(\"scope function with autouse\") def test_autouse(): assert True Parametrize The [pytest.mark.parametrize] fixture allows the user to run the same test, multiple times, by modifying the input parameters.\n@pytest.mark.parametrize(\"num, output\",[(1,11),(2,22),(3,35),(4,44)]) def test_multiplication_11(num, output): assert 11*num == output Python Class file skeleton class ClassName(): \"\"\" This is a multi-line comment (used for header in this example) This is a second line of comment.. \"\"\" def __init__(self, var1: str, var2: str): # the \"var1: str\" format will require the var1 to be a string type self._var1 = var1 # the _ is to make the variable \"protected\" self._var2 = var2 @property # Using property decorator as a getter function def var1(self) -\u003e str: # the -\u003e specifies the return type return self._var1 @var1.setter # using setter decorator for setter function def var1(self, value: str): self.var1 = value def raises_exception(): raise ValueError(\"this is an exception!\") It is possible to use decorators for getters and setters.\nCommonly used functions and examples Verify variable type if not isinstance(variable, str): raise ValueError(\"wrong type!\") Open file with context manager with open(\"test.txt\", 'w', encoding='utf-8') as f: f.write(\"my first file\\n\") f.write(\"This file\\n\\n\") f.write(\"contains three lines\\n\") Iterate a list data = [\"abc\", \"def\", \"ghi\"] for each_data in data: assert each_data in data try except try: # Some Code except: # Executed if error in the # try block else: # execute if no exception finally: # Some code .....(always executed)",
    "description": "Python and PyTest PyTest Why PyTest? requirements.txt content for pytest Run a test pytest.ini configuration file example Tox tox.ini Test file skeleton Test class skeleton Fixtures Python Class file skeleton Commonly used functions and examples Python and PyTest Working with multiple python versions may cause issues and confusion. It is recommended to specify the python version to use.\nWorking with python 3.9:\npy -3.9 -m \u003ccommand\u003e It is recommended to upgrade the pip version:",
    "tags": [],
    "title": "Python and PyTest",
    "uri": "/notes/python-and-pytest/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "Role Strategic Tactics Role The architect is responsible for two main types of high-level activities:\nStrategic (foundations): take care of the technical vision in the medium to long term (one, two years or even more) Tactics (operations): technical support for teams, problem solving, proof of concept, etc. Strategic Define the “Quality Vision”, guidelines, technical documents (ADR, technical designs, terminology/technical glossary, etc.) while taking into account security rules, PII, etc Evaluate technologies: research, comparisons, proof of concepts, publications, etc Promote best practices for the chosen technologies Mentoring and coaching Promote quality during the six stages of software development, SDLC or “Software Development Life Cycle”: “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” - \u003e “Deployment” Assist or lead working groups for specific topics, define the purpose and deliverables, periodic and monthly events Attend or lead workshops for specific topics, define the goal and deliverables, event over a few hours or a few days Audit of the services for test coverage and correlate the data with the problematic services in production (Data-Driven decisions) Analyze the current testing strategy, test characterization, define a plan, promote good testing practices, follow the test pyramid: Unit (solitary and sociable) testing Component (in-process and out-of-process) testing Contract testing Integration (narrow and broad) testing UI End-to-end (E2E) and API E2E (Subcutaneous) User journey tests (and Synthetic Traffic tests) Non-functional tests: load, scalability, webvital metrics LCP, chaos/robustness, etc Alignment on a test strategy with the deployment procedure (CI/CD) and a branching procedure (gitflow, githubflow, etc), blue/green deployment, etc Participates in the recruitment process for the “quality” aspects Tactics Apply the test strategy: develop test projects, add the projects to the process (build, CI/CD, cluster/kubernetes. etc), document, etc Support test development: support QAs and developers who add tests, PR reviews, etc Add test coverage: JavaScript (e.g., cypress.io, etc), python/pytest, java/rest-assured, etc Addition of Synthetic Traffic tests and verification of customer flows and scenarios (i.e., customer flows) Analyze system performance (e.g., load tests) and according to defined standards (i.e., “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (i.e., Largest Contentful Paint) (e.g., “a page should be considered usable in less than 2.5 seconds”) Using observation tools to understand issues: datadoghq, splunk/signalfx, prometheus, ​​grafana, etc Collaborates horizontally with the different “domaines” of the company and suppliers to improve the overall quality of the company",
    "description": "Role Strategic Tactics Role The architect is responsible for two main types of high-level activities:\nStrategic (foundations): take care of the technical vision in the medium to long term (one, two years or even more) Tactics (operations): technical support for teams, problem solving, proof of concept, etc. Strategic Define the “Quality Vision”, guidelines, technical documents (ADR, technical designs, terminology/technical glossary, etc.) while taking into account security rules, PII, etc Evaluate technologies: research, comparisons, proof of concepts, publications, etc Promote best practices for the chosen technologies Mentoring and coaching Promote quality during the six stages of software development, SDLC or “Software Development Life Cycle”: “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” - \u003e “Deployment” Assist or lead working groups for specific topics, define the purpose and deliverables, periodic and monthly events Attend or lead workshops for specific topics, define the goal and deliverables, event over a few hours or a few days Audit of the services for test coverage and correlate the data with the problematic services in production (Data-Driven decisions) Analyze the current testing strategy, test characterization, define a plan, promote good testing practices, follow the test pyramid: Unit (solitary and sociable) testing Component (in-process and out-of-process) testing Contract testing Integration (narrow and broad) testing UI End-to-end (E2E) and API E2E (Subcutaneous) User journey tests (and Synthetic Traffic tests) Non-functional tests: load, scalability, webvital metrics LCP, chaos/robustness, etc Alignment on a test strategy with the deployment procedure (CI/CD) and a branching procedure (gitflow, githubflow, etc), blue/green deployment, etc Participates in the recruitment process for the “quality” aspects Tactics Apply the test strategy: develop test projects, add the projects to the process (build, CI/CD, cluster/kubernetes. etc), document, etc Support test development: support QAs and developers who add tests, PR reviews, etc Add test coverage: JavaScript (e.g., cypress.io, etc), python/pytest, java/rest-assured, etc Addition of Synthetic Traffic tests and verification of customer flows and scenarios (i.e., customer flows) Analyze system performance (e.g., load tests) and according to defined standards (i.e., “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (i.e., Largest Contentful Paint) (e.g., “a page should be considered usable in less than 2.5 seconds”) Using observation tools to understand issues: datadoghq, splunk/signalfx, prometheus, ​​grafana, etc Collaborates horizontally with the different “domaines” of the company and suppliers to improve the overall quality of the company",
    "tags": [],
    "title": "Quality Architect Role",
    "uri": "/blogs/quality-architect-role/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "Role Stratégique Tactique Role L’architecte est responsable de deux grands types d’activités à haut niveau:\nStratégique (fondations): s’occuper de la vision technique à moyen à long terme (un, deux ans ou même plus) ; Tactique (opérations): support technique des équipes, résolution de problèmes, preuve de concept, etc ; Stratégique Définir la “Vision Qualité”, les guides (ou “guidelines”), documents techniques (ADR, Technical Design, Terminologie/glossaire technique, etc.) tout en tenant compte des règles de sécuritées, PII, etc. ; Évaluer les technologies : Recherches, comparatifs, preuves de concepts, publications, etc.; Promouvoir les bonnes pratiques pour les technologies choisies; Mentorat et “Coaching”; Promouvoir la qualité lors des six étapes du développement logiciel, SDLC ou “Software Development Life Cycle” : “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” -\u003e “Deployment” ; Assister ou diriger des groupes de travail (ou “Work Groups”) pour des sujets précis, définir le but et les livrables, événements périodiques et sur plusieurs mois ; Assister ou diriger des ateliers de travail (ou “Workshops”) pour des sujets précis, définir le but et les livrables, événement sur quelques heures ou quelques jours ; Audit de la couverture de tests et corrélation des données avec les services problématiques en production (décision basée sur les données ou “Data-Driven”); Analyser la stratégie de tests actuelle, caractérisation des tests, définir un plan, promouvoir les bonnes pratiques de tests, la pyramide de tests “pratique” (en anglais pour faciliter) : Unit (“solitary” et “sociable”) testing ; Component (“in-process” et “out-of-process”) testing ; Contract testing ; Integration (“narrow” et “broad”) testing ; UI End-to-end (E2E) et API E2E (ou Subcutaneous) ; User journey tests (and Synthetic Traffic tests) ; Non functional tests : load, scalability, webvital metrics LCP, chaos/robustness, etc. ; Création de projets archétypes et création de matériel de formation (on-demand training) suivi de projets pilotes ou réels, écriture de tests, documentation, etc. ; Alignment sur une stratégie de tests avec la procédure de déploiement (CI/CD) avec une procédure de “branching” (examples : gitflow ou trunk-based workflows), blue/green deployment, smoke tests, etc. ; Participe au processus de recrutement pour le volet “qualité” ; Tactique Appliquer la stratégie de tests : développer des projets tests, ajouter les projets au processus (build, CI/CD, cluster/kubernetes. etc), documenter, etc. ; Supporter le développement de tests: supporter les QAs et développeurs qui ajoutent des tests, PR reviews, etc. ; Ajouter une couverture de tests : javascript (exemple : cypress.io), python/pytest, java/rest-assured, etc. ; Ajout de “tests de trafic synthétique” et vérification des flux et scénarios clients (ou “customer flows”) ; Analyser les performances systèmes (example: load tests) et selon les standards définis (example: “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (ou “Largest Contentful Paint”, example : “une page devrait être considérée utilisable en moins de 2.5 secondes”) ; Utilisation d’outils d’observation pour comprendre les problèmes : datadoghq, splunk/signalfx, prometheus, grafana, etc. ; Collabore horizontalement avec les différents “domaines” de l’entreprises et des fournisseurs pour améliorer la qualité globale de l’entreprise ;",
    "description": "Role Stratégique Tactique Role L’architecte est responsable de deux grands types d’activités à haut niveau:\nStratégique (fondations): s’occuper de la vision technique à moyen à long terme (un, deux ans ou même plus) ; Tactique (opérations): support technique des équipes, résolution de problèmes, preuve de concept, etc ; Stratégique Définir la “Vision Qualité”, les guides (ou “guidelines”), documents techniques (ADR, Technical Design, Terminologie/glossaire technique, etc.) tout en tenant compte des règles de sécuritées, PII, etc. ; Évaluer les technologies : Recherches, comparatifs, preuves de concepts, publications, etc.; Promouvoir les bonnes pratiques pour les technologies choisies; Mentorat et “Coaching”; Promouvoir la qualité lors des six étapes du développement logiciel, SDLC ou “Software Development Life Cycle” : “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” -\u003e “Deployment” ; Assister ou diriger des groupes de travail (ou “Work Groups”) pour des sujets précis, définir le but et les livrables, événements périodiques et sur plusieurs mois ; Assister ou diriger des ateliers de travail (ou “Workshops”) pour des sujets précis, définir le but et les livrables, événement sur quelques heures ou quelques jours ; Audit de la couverture de tests et corrélation des données avec les services problématiques en production (décision basée sur les données ou “Data-Driven”); Analyser la stratégie de tests actuelle, caractérisation des tests, définir un plan, promouvoir les bonnes pratiques de tests, la pyramide de tests “pratique” (en anglais pour faciliter) : Unit (“solitary” et “sociable”) testing ; Component (“in-process” et “out-of-process”) testing ; Contract testing ; Integration (“narrow” et “broad”) testing ; UI End-to-end (E2E) et API E2E (ou Subcutaneous) ; User journey tests (and Synthetic Traffic tests) ; Non functional tests : load, scalability, webvital metrics LCP, chaos/robustness, etc. ; Création de projets archétypes et création de matériel de formation (on-demand training) suivi de projets pilotes ou réels, écriture de tests, documentation, etc. ; Alignment sur une stratégie de tests avec la procédure de déploiement (CI/CD) avec une procédure de “branching” (examples : gitflow ou trunk-based workflows), blue/green deployment, smoke tests, etc. ; Participe au processus de recrutement pour le volet “qualité” ; Tactique Appliquer la stratégie de tests : développer des projets tests, ajouter les projets au processus (build, CI/CD, cluster/kubernetes. etc), documenter, etc. ; Supporter le développement de tests: supporter les QAs et développeurs qui ajoutent des tests, PR reviews, etc. ; Ajouter une couverture de tests : javascript (exemple : cypress.io), python/pytest, java/rest-assured, etc. ; Ajout de “tests de trafic synthétique” et vérification des flux et scénarios clients (ou “customer flows”) ; Analyser les performances systèmes (example: load tests) et selon les standards définis (example: “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (ou “Largest Contentful Paint”, example : “une page devrait être considérée utilisable en moins de 2.5 secondes”) ; Utilisation d’outils d’observation pour comprendre les problèmes : datadoghq, splunk/signalfx, prometheus, grafana, etc. ; Collabore horizontalement avec les différents “domaines” de l’entreprises et des fournisseurs pour améliorer la qualité globale de l’entreprise ;",
    "tags": [],
    "title": "Rôle de l'Architecte Qualité (FR)",
    "uri": "/blogs/role-architecte-qualite/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Node.JS Installation Quick Overview Initializing TypeScript Project Setting TypeScript Configuration File Searching and Importing Open Source Defined Types Types Primitive and Built-in Types Creating Custom Types Using Interface Creating Custom Types Using Aliases Enumerable Types Typing a Function Using Specific Types in a Function Defining a Megatype using Generics More Complex Types Using Aliases for Complex Types Definition Working with Record Type Resource Management with using Keyword Extending and modifying existing types The Partial helper type The Omit helper type The Pick helper type The Required helper type Decorators Decorator with no arguments Decorator with no arguments Using modules in typescript Using the globals.d.ts file The declare global {} statement Using or not using the declare global {} statement? TypeScript JavaScript and TypeScript are two popular programming languages for developing web applications. JavaScript is a simple and versatile language that supports dynamic typing. TypeScript extends JavaScript by adding static typing and features like interfaces, enums, and advanced type-checking.\nNode.JS Installation Quick Overview Get current node version:\n\u003e node -v v22.9.0 Installing Node.js using brew:\n\u003e brew install node Initializing TypeScript Project Installing typescript using npm at the project level:\n\u003e npm install typescript --save-dev \u003e ls -1 node_modules package-lock.json package.json \u003e npx tsc -v Version 5.6.3 \u003e cat package.json { \"devDependencies\": { \"typescript\": \"^5.6.3\" } } Setting TypeScript Configuration File A configuration file tsconfig.json is required at the Project root directory:\ntsconfig.json:\n{ \"compilerOptions\": { \"outDir\": \"build\", \"target\": \"ES6\" , \"noEmit\": false, \"experimentalDecorators\": true, \"emitDecoratorMetadata\": true }, \"include\": [\"src/**/*\"] } Details:\ninclude: the directories to search the TypeScript files from compilerOptions.target: the ECMAScript version compatibility compilerOptions.outDir: the built files directory, or where tsc will drop the generated js files compilerOptions.noEmit: true or false, if the js files should be generated. The true value will make tsc to only check the files without generating any js file. compilerOptions.experimentalDecorators: true or false, if the then the decorators (experimental) feature will be enabled. compilerOptions.emitDecoratorMetadata: true or false, This library implements polyfills for another set of proposed ECMAScript features (experimental), requires the reflect-metadata library to be installed : npm i reflect-metadata --save. Searching and Importing Open Source Defined Types All defined types can be found in [DefinitelyTypes] github repository, or more easily on [npnjs.com] package locator:\nFor example, search for @types jquery within npmjs.com search field.\nThe installation command will be:\nnpm install --save @types/jquery The imported library will be located in node_modules/@type directory.\nTypes Primitive and Built-in Types By default, Javascript will infer the variable type. For example, this will create a number type by default:\nlet x = 5 However, TypeScript won’t infer anymore since variable type must be defined:\nlet x: number let y: string let z: boolean let a: Date let b: string[] let c: any The any type will be automatically inferred.\nIt is possible to use casting.\nlet b: string[] b = \"Hello World!\" as any Using as any is however avoiding the whole TypeScript purpose.\nCreating Custom Types Using Interface It is possible to create custom types in TypeScript by using interface, for example:\ninterface Contact { id: number; name: string; birthDate?: Date; // Optional since ? was added } let contact1: Contact; let contact2: Contract = { birthDate: new Date(\"01-01-1999\"), id: 1234, name: \"John Snow\" } let contact3: Contract = { id: 1234, name: \"John Snow\" } When a ? is added to the interface field then it will make this field optional.\nIt is possible to extend an actual interface, for example:\ninterface Contact extend Address { id: number; name: string; birthDate?: Date; // Optional since ? was added } interface Address { line1: string; line2: string; province: string; region: string; postalCode: string; } let contact1: Contract = { birthDate: new Date(\"01-01-1999\"), id: 1234, name: \"John Snow\" postalCode: \"H0H0H0\" ... } Creating Custom Types Using Aliases An Alias is only a replacement name for an existing type, for example:\ntype ContactName = string So ContactName is in reality a string, and both are now interchangeable.\nEnumerable Types Enums allow a developer to define a set of named constants, for example:\nenum ContactStatus { Active = \"active\", // assigned value \"active\" is optional Inactive = \"inactive\", New = \"new\" } interface Contact { id: number; name: string; birthDate?: Date; status: ContactStatus; } let contact1: Contract = { birthDate: new Date(\"01-01-1999\"), id: 1234, name: \"John Snow\", status: ContactStatus.Active } Typing a Function This is a JavaScript function:\nfunction clone(source) { return Object.apply({}, source) // Apply will return an \"any\" type } const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone(contact1); // contact2's type will be \"any\" Using Specific Types in a Function This is the typed version in TypeScript:\nfunction clone(source: Contact): Contact { return Object.apply({}, source) // Apply will return an \"any\" type, but the function's returned type will be Contact } const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone(contact1); // contact2's type will be \"Contact\" Defining a Megatype using Generics function clone\u003cT\u003e(source: T): T { return Object.apply({}, source) } const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone(contact1); // clone() will work for any type, e.g. const dateRange = { startDate: Date.now(), endDate: Date.now() }; const dateRageCopy = clone(dateRange); It is possible to define multiple types using Generics, for example:\nfunction clone\u003cT1, T2\u003e(source: T1): T2 { return Object.apply({}, source) } // T1 and T2 must be specified at the function's call const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone\u003cContact, Date\u003e(contact1); It is possible to use generics with an interface:\ninterface ExternalContact\u003cTExternalId\u003e { id: number; name: string; birthDate?: Date; externalId: TExternalId; loadExternalId(): Task\u003cTExternalId\u003e; } More Complex Types Using Aliases for Complex Types Definition It is possible to allow multiple types for one field, for example:\ninterface Contact { id: number; name: string; birthDate: Date | number | string; // all 3 types can be used } Now it is possible to use an alias, for example:\ntype ContactBirthDate = Date | number | string; interface Contact { id: number; name: string; birthDate: ; ContactBirthDate; // all 3 types can be used } Creating a new interface or custom type using an alias:\ninterface Contact { id: number; name: string; birthDate: Date; } interface Address { line1: string; line2: string; province: string; region: string; postalCode: string; } type AddressableContact = Contact \u0026 Address; Replacing an Enum type with an alias, for example:\nenum ContactStatus { Active = \"active\", Inactive = \"inactive\", New = \"new\" } let a: ContactStatus = ContactStatus.Active; // Can be replaced by type ContactStatus = \"active\" | \"inactive\" | \"new\"; let a: ContactStatus = \"active\"; Using keyof to get the available fields for a given type:\ninterface Contact { id: number; name: string; birthDate: Date; } type ContactFields = keyof Contact; function getValue(source, propertyName: ContactFields) { return source[propertyName] } // With a generic function getValue\u003cT\u003e(source, propertyName: keyof T) { return source[propertyName] } Working with Record Type In TypeScript, a Record is a utility type that allows you to map keys of a specific type to values of another type. It’s a concise way to define objects with a uniform key-value structure. Here’s a simple example:\n// Syntax: Record\u003cKeyType, ValueType\u003e // Example 1: Record with string keys and number values const scores: Record\u003cstring, number\u003e = { Alice: 85, Bob: 92, Charlie: 88, }; // Example 2: Record with string keys and boolean values const isActive: Record\u003cstring, boolean\u003e = { Alice: true, Bob: false, Charlie: true, }; Explanation:\nRecord\u003cstring, number\u003e: The keys are strings, and the values are numbers. So, each key (like “Alice”) must map to a number (like 85). Record\u003cstring, boolean\u003e: Here, the keys are also strings, but the values are booleans (true or false). It is possible to define a list of types, e.g.:\nconst scores: Record\u003cstring, number | string\u003e = { Alice: 85, Bob: 92, Charlie: \"88\", }; It is possible to limit the keys to a custom type using keyof, e.g.:\nconst contact1: Record\u003ckeyof Contact, number | string\u003e = { id: 85, name: \"Jon Snow\", }; Resource Management with using Keyword As of TypeScript 5.2, a new using keyword has been introduced to work similarly to C#’s using statement. This feature allows you to automatically dispose of resources when they are no longer needed, provided the resource implements a Disposable pattern.\nHere’s a small example of using this feature in TypeScript 5.2+\n// can safely omit the `implements Disposable` statement class Resource { constructor(private name: string) { console.log(`${this.name} is created`); } [Symbol.dispose]() { console.log(`${this.name} is disposed`); } use() { console.log(`Using ${this.name}`); } } function main() { using resource = new Resource(\"MyResource\"); resource.use(); } main(); Explanations:\nResource class: Implements [Symbol.dispose] to clean up resources. using keyword: Automatically calls Symbol.dispose when the block ends. In main(): Creates a resource, uses it, and automatically disposes it when done. The implements Disposable part is optional; the critical part is implementing the Symbol.dispose method for the using keyword to automatically dispose of the resource. Extending and modifying existing types The Partial helper type Partial is a TypeScript utility type that makes all properties of a given type T optional. This allows you to create objects where only some properties of the original type are required. This is an example:\ninterface User { name: string; age: number; email: string; } // Using Partial to make all fields optional const updateUser = (user: Partial\u003cUser\u003e) =\u003e { console.log(user); }; // Example usage updateUser({ name: \"Alice\" }); updateUser({ age: 30, email: \"alice@example.com\" }); Explanation:\nPartial\u003cUser\u003e: Makes all properties in the User interface optional. updateUser: Can accept an object with any subset of User properties (e.g., just name, age, or email). The Omit helper type Omit\u003cT, K\u003e is a TypeScript utility type that constructs a new type by removing the specified keys K from type T. This allows you to create a type without certain properties. This is an example:\ninterface User { name: string; age: number; email: string; address: string; } // Using Omit to exclude 'email' and 'address' const createBasicUser = (user: Omit\u003cUser, 'email' | 'address'\u003e) =\u003e { console.log(user); }; // Example usage createBasicUser({ name: \"Alice\", age: 30 }); Explanation:\nOmit\u003cUser, 'email' | 'address'\u003e: Excludes both email and address from the User type. createBasicUser: Accepts an object with only name and age, omitting email and address. The Pick helper type Pick\u003cT, K\u003e is a TypeScript utility type that creates a new type by selecting specific keys K from type T. It allows you to include only the desired properties. There is an example:\ninterface User { name: string; age: number; email: string; address: string; } // Using Pick to select only 'name' and 'email' const getUserContactInfo = (user: Pick\u003cUser, 'name' | 'email'\u003e) =\u003e { console.log(user); }; // Example usage getUserContactInfo({ name: \"Alice\", email: \"alice@example.com\" }); Explanation:\nPick\u003cUser, 'name' | 'email'\u003e: Selects only name and email from the User type. getUserContactInfo: Accepts an object with just name and email, ignoring other properties like age and address. The Required helper type Required\u003cT\u003e is a TypeScript utility type that makes all properties of a given type T required, removing any optional modifiers. This is an example:\ninterface User { name?: string; age?: number; } // Using Required to make all fields mandatory const createUser = (user: Required\u003cUser\u003e) =\u003e { console.log(user); }; // Example usage (now both 'name' and 'age' are required) createUser({ name: \"Alice\", age: 30 }); Explanation:\nRequired\u003cUser\u003e: Converts all optional properties in User to required. createUser: Now expects both name and age to be provided. Decorators A TypeScript decorator is a special kind of declaration that can be attached to classes, methods, properties, or parameters to modify their behavior. Decorators are essentially functions that take the target (like a class or method) as an argument and allow you to apply additional logic to it. They are commonly used for things like logging, validation, or adding metadata.\nDecorators are written with the @ symbol followed by the decorator function name and can be applied to:\nClasses Methods Properties Accessors Parameters To enable decorators in TypeScript, you need to add the experimentalDecorators option in your tsconfig.json.\nMore details may be found in the Decorators official documentation.\nDecorator with no arguments This is an example:\nfunction log(target: any, propertyKey: string, descriptor: PropertyDescriptor) { const originalMethod = descriptor.value; descriptor.value = function (...args: any[]) { console.log(`Method ${propertyKey} was called with args: ${args}`); return originalMethod.apply(this, args); }; } class Example { @log greet(name: string) { console.log(`Hello, ${name}!`); } } // Example usage const example = new Example(); example.greet(\"Alice\"); In this example, the @log decorator adds logging behavior to the greet method, printing when the method is called and the arguments passed to it.\nDecorator with no arguments This is an example:\nfunction logWithMessage(message: string) { return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) { const originalMethod = descriptor.value; descriptor.value = function (...args: any[]) { console.log(`${message} - Method ${propertyKey} was called with args: ${args}`); return originalMethod.apply(this, args); }; }; } class Example { @logWithMessage(\"Custom log\") greet(name: string) { console.log(`Hello, ${name}!`); } } // Example usage const example = new Example(); example.greet(\"Alice\"); Explanation:\nlogWithMessage(message: string): A decorator factory that takes a message argument and returns a decorator function. @logWithMessage(\"Custom log\"): Applies the decorator with the custom log message to the greet method, enhancing it with additional logging behavior. Using modules in typescript It is recommended to work with modules. Basically, the exported functions and properties require the export keyword in front. The importing module require to define the imported functions and properties.\nThis is an example:\nfile: math.ts (exporting module)\nexport function add(a: number, b: number): number { return a + b; } export const PI = 3.14; file: app.ts (importing module)\nimport { add, PI } from './math'; console.log(add(2, 3)); // Output: 5 console.log(PI); // Output: 3.14 Explanation:\nmath.ts: Exports a function (add) and a constant (PI). app.ts: Imports the add function and PI constant from math.ts and uses them in the code. Using the globals.d.ts file The globals.d.ts file is a TypeScript declaration file used to define global types, interfaces, or variables that can be accessed throughout your project without imports. It’s useful for:\nDeclaring global variables. Extending global objects (e.g., Window). Sharing global types or interfaces. This is an example of globals.d.ts:\ndeclare const API_URL: string; interface Window { myCustomProperty: string; } type UserRole = 'admin' | 'user' | 'guest'; This file allows these types and variables to be used globally across your project and is typically placed in the src or types directories.\nThe declare global {} statement The declare global {} statement in TypeScript is used to extend or modify the global scope by adding new types, interfaces, or variables that will be accessible globally throughout the project. This is useful when you need to add custom properties to global objects (like Window, Document, etc.) or declare new global variables and types.\nIt is typically used in .d.ts files to make these global declarations available project-wide.\nThis is an example:\n// globals.d.ts export {}; declare global { interface Window { myCustomProperty: string; } declare const API_URL: string; } Meaning:\ndeclare global {}: Everything inside this block is added to the global scope, meaning you can use it without importing in other files. Extending Window: Adds a custom property myCustomProperty to the global Window object. Declaring global API_URL: Makes the API_URL constant available globally, with its type specified. This approach is useful when working with global objects or variables that are shared across different parts of your application.\nUsing or not using the declare global {} statement? The difference between using declare global {} and not using it lies in where the declarations are scoped and how they affect the global namespace. Here’s a concise comparison:\n1. Without declare global When you use declare directly without declare global {}, the declared entities are treated as global only within the specific file where the declaration exists.\nExample (without declare global {}):\n// globals.d.ts declare const API_URL: string; interface Window { myCustomProperty: string; } These declarations are global, but only if the .d.ts file is included by TypeScript. You don’t explicitly tell TypeScript that this is meant to modify the global namespace; TypeScript assumes the declarations are part of the global scope.\n2. With declare global By wrapping the declarations inside declare global {}, you are explicitly modifying the global namespace and ensuring that these changes apply project-wide.\nExample (with declare global {}):\n// globals.d.ts export {}; // Ensures the file is treated as a module declare global { interface Window { myCustomProperty: string; } declare const API_URL: string; } declare global {} is used when the file is treated as a module (via export {} or similar) and you still want to modify the global scope. This makes it explicit that you’re extending the global namespace from within a module, ensuring the declarations are globally available, even if the .d.ts file contains export statements. Key Differences Without declare global {}: The file is implicitly considered part of the global namespace. These declarations work fine in traditional .d.ts files with no imports or exports. With declare global {}: Explicitly extends the global namespace when the file is a module (i.e., it contains import or export statements). This is necessary if you want to mix global declarations with modular code. You would typically use declare global {} in module-based projects where you are exporting/importing other things but still want to add or modify the global scope.",
    "description": "Node.JS Installation Quick Overview Initializing TypeScript Project Setting TypeScript Configuration File Searching and Importing Open Source Defined Types Types Primitive and Built-in Types Creating Custom Types Using Interface Creating Custom Types Using Aliases Enumerable Types Typing a Function Using Specific Types in a Function Defining a Megatype using Generics More Complex Types Using Aliases for Complex Types Definition Working with Record Type Resource Management with using Keyword Extending and modifying existing types The Partial helper type The Omit helper type The Pick helper type The Required helper type Decorators Decorator with no arguments Decorator with no arguments Using modules in typescript Using the globals.d.ts file The declare global {} statement Using or not using the declare global {} statement? TypeScript JavaScript and TypeScript are two popular programming languages for developing web applications. JavaScript is a simple and versatile language that supports dynamic typing. TypeScript extends JavaScript by adding static typing and features like interfaces, enums, and advanced type-checking.",
    "tags": [],
    "title": "TypeScript",
    "uri": "/notes/typescript/index.html"
  }
]
