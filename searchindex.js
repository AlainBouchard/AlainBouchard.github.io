var relearn_searchindex = [
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "Blogs Building Quality: Aligning Test Strategies with the SDLC Design for Testability How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR)",
    "description": "Blogs Building Quality: Aligning Test Strategies with the SDLC Design for Testability How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR)",
    "tags": [],
    "title": "My Blogs",
    "uri": "/blogs/index.html"
  },
  {
    "breadcrumb": "",
    "content": "My Engineering Notes and Blogs Welcome on my Engineering Notes and Blogs site!\nMy Blogs Building Quality: Aligning Test Strategies with the SDLC Design for Testability How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR) My Notes Apache Kafka (WIP) Cypress with TypeScript Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured JavaScript Best Practices and Principles Lombok Java Library MongoDB Python and PyTest TypeScript",
    "description": "My Engineering Notes and Blogs Welcome on my Engineering Notes and Blogs site!\nMy Blogs Building Quality: Aligning Test Strategies with the SDLC Design for Testability How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR) My Notes Apache Kafka (WIP) Cypress with TypeScript Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured JavaScript Best Practices and Principles Lombok Java Library MongoDB Python and PyTest TypeScript",
    "tags": [],
    "title": "Alain Bouchard's Engineering Notes and Blogs",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "My Engineering Notes Apache Kafka (WIP) Cypress with TypeScript Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured JavaScript Best Practices and Principles Lombok Java Library MongoDB Python and PyTest TypeScript",
    "description": "My Engineering Notes Apache Kafka (WIP) Cypress with TypeScript Data Quality Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured JavaScript Best Practices and Principles Lombok Java Library MongoDB Python and PyTest TypeScript",
    "tags": [],
    "title": "My Notes",
    "uri": "/notes/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Warning 2022-07-11: This is a WORK IN PROGRESS document (WIP) and need to be reviewed.\nApache Kafka Topics, partitions and offsets Brokers Brokers and topics Topic replication factor Producers Consumers Consumer offsets Kafka broker discovery Zookeeper Kafka guarantees Install Kafka using Docker images Use Kafka Topics CLI Topics CLI Kafka console producer Kafka console consumer Kafka-Client with Java Kafka Connect and Kafka Stream Apache Kafka Why Apache Kafka?\nCreated by LinkedIn, now Open Source Project Maintained by Confluent (Apache Stewardship) Distributedm resilient architecture, fault tolerant scales Horizontaly (100s of borkers, millions of messages per seconds, etc.) Use Cases?\nMessaging system Activity tracking Gather metrics from many different locations Application logs gathering Stream processing (e.g., kafka stream API, Apache Spark, etc.) De-coupliung system dependicies Integration with Spark, Flink, Storm, Hadoop, and other big Data technologies Topics, partitions and offsets Topics: a particular stream of data\nsimilar to a table in a DB (but without the constraints) you can have as many topics as you want a topic is identified by its name Partitions: spliting topics\neach partiction is orderied each message within a partiction gets an incremental id, called offset | Partition 0 -\u003e offset: | 0 | 1 | 2 | 3 | 4 | 5 | ... | Kafka Topic + Partition 1 -\u003e offset: | 0 | 1 | 2 | ... writes | | Partition 2 -\u003e offset: | 0 | 1 | 2 | 3 | 4 | ... offset only have a meaning for a specific partiction (e.g., ofsset 3 in partition 0 != offset 3 in partition 1) order is guaranteed only within a partition (not acresoo partitions) data is kept only for a limited time (default is one week) once the data is written to a partition, it can’t be changed (immutability) data is assigned randomly to a partition unless a key is provided (more on this later) Brokers a kafka cluster in composed of multiple brokers (broker = servers) each broker is identified with its ID (integer) each broker contains certain topic partitions after connecting to any broker (called a bootstrap broker), you will be connected to the intire cluster a good number to get started is 3 brokers, but may go over 100 brokers Brokers and topics Example of Topic-A with 3 partitions Exmaple of Topic-B with 2 partitions Broker 101 Broker 102 Broker 103 Topic A partition 0 Topic A partition 2 Topic A partition 1 Topic A partition 1 Topic A partition 0 Topic replication factor topics should have a replication factor \u003e 1 (usually between 2 and 3) this way if a broker is down, another broker can serve the data example: topic-A with 2 partitions and replications factor of 2 Broker 101 Broker 102 Broker 103 Topic-A Partition 0 Topic-A Partition 1 Topic-A Partition 1 Topic-A Partition 0 example: broker 102 goes down; broker 101 and 103 still up, both partitions still work at any time only one broker can be a leader for a given partition only that leader can receive and serve data for a partition the other brokers will synchronize the data therefore each partion has one leader and multiple ISR (in-sync replica) Producers producers write data to topics (which is made of partitions) producers automatically know to which broker and partition to write to in case of broker failures, producers will automatically recover producers can choose to receive acknowledgement of data writes acks=0: producers won’t wait for acknowledgment (possible data loss) (default) acks=1: producer will wait for leader acknowledgment (limited data loss) acks=all: leaders and replicas acknowledgment (no data loss) Message keys producers can choose to send a key with the message (string, number, etc.) if key=null, data is sent round robin (broker 101, 102 then 103, and 101 again…) if a key is sent, then all messages for that key will always goto the same partition a key is basically sent if you need a message ordering for specific field (e.g., truck_id, etc.) we get this guarantee due to key hashing, which depends on the number of partitions Consumers consumers read data from a tipic (identified by name) consumers know which broker to read from in case of broker failures, consumer know how to recover data is read in order within each partitions Consumers groups consumers read data in consumer groups each consumer within a group reads from exclusinve partitions if youy have more consumer than partitions, some consumers with be icative if you have more consumers than partitions, some consumers will be inactive Consumer offsets kafka stores the offsets at which a consumer group has beed reading the offset committed live in a kafka topic named __consumer_offsets when a consumer in a group has processed data received from kafka, it should be committing the offsets if a consumer dies, it will be able to read back from where it left off (due to the committed consumer offsets) Delivery semantics for consumers consumers choose when to commit offsets there are 3 delivery semantics at most once: offsets are committed as soon as the message is received offsets are commited as soon as the message is received if the processing goes wring, the message will be lost (it won’t be read again) at least once (usually preferred) offsets are committed after the message is processed if the processing goes wring, the message will be read again this can reults in duplicate processing of messages, so make sure the processing is idempotent exactly once can be achieved for kafta-to-kafka workflows using kafka streams API for kafka-to-external-system workflows, it requires the consumer to be idempotent Kafka broker discovery every kafka broker is also called a bootstrap server that means that you only need to connect to one broker and you will be connected to the entire cluster each broker knows about all brokers, topics and partitions (metadata) Zookeeper zookeeper manages brokers (keeps a list of them) zookeeper helps in performing leader election partitions zookeeper sends notifications to kafka in case of changes (e.g., new topic, broker dies, brker comes up, delete topics, etc.) kafka can’t work without zookeeper zookeeper by design operates with an odd number of servers (3, 5, 7, …) zookeeper has a leader (handle writes) and the rest if the servers are followers (handle reads) zookeeper does not store consumer offsets with kafka \u003e v0.10 Kafka guarantees Messages are appended to a topic-partition in othe order they are sent consumers read messages in the order stored in a topic-partition with a replication factor of N, producers and consumers can tolerate up to N-1 brokers being down this is why a replicator factor of 4 is a good idea: allows for one broker to be taken down for maintenance allows for another broker to be taken down unexpectedly as long as the number of partitions remains constant for a topic (no new partitions), the same key will always go to the same partition (i.e., hashed key) Install Kafka using Docker images Create the docker-compose.yaml file:\nversion: '3.5' services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 ports: - 22181:2181 kafka: image: confluentinc/cp-kafka:latest depends_on: - zookeeper ports: - 29092:29092 environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 From the CLI:\n\u003e docker-compose up -d \u003e docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ab09b5c8bc7b confluentinc/cp-kafka:latest \"/etc/confluent/dock…\" 11 minutes ago Up 11 minutes 9092/tcp, 0.0.0.0:29092-\u003e29092/tcp kafka-cluster_kafka_1 8e7725b7874b confluentinc/cp-zookeeper:latest \"/etc/confluent/dock…\" 11 minutes ago Up 11 minutes 2888/tcp, 3888/tcp, 0.0.0.0:22181-\u003e2181/tcp kafka-cluster_zookeeper_1 \u003e $ netstat -aon | grep 22181 TCP 0.0.0.0:22181 0.0.0.0:0 LISTENING 23792 TCP [::]:22181 [::]:0 LISTENING 23792 TCP [::1]:22181 [::]:0 LISTENING 27612 \u003e netstat -aon | grep 29092 TCP 0.0.0.0:29092 0.0.0.0:0 LISTENING 23792 TCP [::]:29092 [::]:0 LISTENING 23792 TCP [::1]:29092 [::]:0 LISTENING 27612 \u003e docker-compose logs kafka | grep -i started kafka_1 | [2022-05-31 14:49:34,438] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 14:49:34,441] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 14:49:34,445] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,450] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,450] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started socket server acceptors and processors (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,459] INFO [KafkaServer id=1] started (kafka.server.KafkaServer) kafka_1 | [2022-05-31 18:51:50,791] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 18:51:50,791] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 19:52:52,922] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 19:52:52,923] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 21:02:51,738] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 21:02:51,739] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 23:24:34,226] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 23:24:34,226] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) Note: nc -z localhost \u003cport\u003e can be used on MacOS and linux.\nFollow the Guide to Setting Up Apache Kafka Using Docker instructions.\nInstall the Kafka Offset Explorer to connect to Kafka Cluster and make sure to configure the bootstrap server: localhost:29092\nUse Kafka Topics CLI Use docker to execute the kafka-topics command:\n\u003e docker-compose ps Name Command State Ports ----------------------------------------------------------------------------------------------------------- kafka-cluster_kafka_1 /etc/confluent/docker/run Up 0.0.0.0:29092-\u003e29092/tcp, 9092/tcp kafka-cluster_zookeeper_1 /etc/confluent/docker/run Up 0.0.0.0:22181-\u003e2181/tcp, 2888/tcp, 3888/tcp \u003e docker exec -t kafka-cluster_kafka_1 kafka-topics Create, delete, describe, or change a topic. Option Description ------ ----------- --alter Alter the number of partitions, replica assignment, and/or configuration for the topic. . . . To use Kafka Container shell:\n\u003e docker exec -it kafka-cluster_kafka_1 sh sh-4.4$ Topics CLI Reference: Apache Kafka CLI commands cheat sheet\nCreating a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --create --topic first_topic --partitions 3 --replication-factor 1 WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic first_topic. sh-4.4$ Note: the --bootstrap-server localhost:9092 replaces the kafka-topics command --zookeeper localhost:9092\nListing current topics sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first_topic sh-4.4$ Describe a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --describe --topic first_topic Topic: first_topic TopicId: vQ6o8fX1Sx-qNQuUQ5vbAg PartitionCount: 3 ReplicationFactor: 1 Configs: Topic: first_topic Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: first_topic Partition: 1 Leader: 1 Replicas: 1 Isr: 1 Topic: first_topic Partition: 2 Leader: 1 Replicas: 1 Isr: 1 sh-4.4$ Delete a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first-topic first_topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --delete --topic first-topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first_topic sh-4.4$ Kafka console producer Create messages Creating 4 messages using Kafka Broker using the kafka-console-producer command. The CTRL-C command will make the kafka-console-producer command to stop.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic \u003emessage1 \u003emessage2 \u003emessage3 \u003emessage4 \u003e^C sh-4.4$ Create messages in a non-existing topic It is possible to create a new topic on-the-fly when adding but it is not recommended since the default values for both PartitionCount and ReplicationFactor are set to 1. Best practices require more partitions and replications.\nDefault replication value can be changed from /etc/kafka/server.properties value num.partitions, default is 1.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic new_topic \u003eThis is a message to a new topic [2022-06-02 18:56:53,185] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 3 : {new_topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient) \u003eanother message sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list first_topic new_topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --topic new_topic --describe Topic: new_topic TopicId: FuC1JLRETNCrgUWaEPZCOg PartitionCount: 1 ReplicationFactor: 1 Configs: Topic: new_topic Partition: 0 Leader: 1 Replicas: 1 Isr: 1 sh-4.4$ Change the producer-property Changing the acks property. Refer to producer above sections for more information about acks=all property.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic --producer-property acks=all \u003eacks message1 \u003e^C sh-4.4$ Kafka console consumer Consuming messages from a topic The kafka-console-consumer won’t consume topic messages from offset=0 by default to avoid consuming millions of existing message. It will consume the upcomming messages only (from now on). To get all messages from offest:0, the --from-beginning option must be specified.\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --from-beginning message2 acks message1 message3 message1 message4 The order isn’t garanteed when the number of topic partitions is greater than 1, as explained in Topics, partitions and offsets section.\nConsuming messages from a topic with groups The --group define a group of kafka consumers that will share the consumption load for one given topic.\nThe kafka-console-producer example:\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic \u003epatate \u003ecarotte \u003epomme \u003eorange \u003e The first kafka-console-consumer example:\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-group carotte orange The second kafka-console-consumer example:\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-group patate pomme Consumer groups command Get all the available consumer groups:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --list my-group sh-4.4$ Get consumer group information:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 3 3 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 1 2 2 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 2 4 4 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer sh-4.4$ Consumer groups: reseting offsets All messages are currently consumed: current-offsets=log-end-offsets:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 3 3 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 1 2 2 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 2 4 4 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer sh-4.4$ Reseting the current-offsets to 0 using --to-earliest option:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --reset-offsets --to-earliest --execute --topic first_topic GROUP TOPIC PARTITION NEW-OFFSET my-group first_topic 0 0 my-group first_topic 1 0 my-group first_topic 2 0 sh-4.4$ Verify the group offsets is not 0:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group Consumer group 'my-group' has no active members. GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 0 3 3 - - - my-group first_topic 1 0 2 2 - - - my-group first_topic 2 0 4 4 - - - sh-4.4$ Shift the offset by 1 using --shift-by option:\nkafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --reset-offsets --shift-by 1 -- execute --topic first_topic GROUP TOPIC PARTITION NEW-OFFSET my-group first_topic 0 1 my-group first_topic 1 1 my-group first_topic 2 1 sh-4.4$ Available --reset-offsets options are :\n--to-datetime --by-period --to-earliest --to-latest --shift-by --from-file --to-current Kafka-Client with Java Use Kafka Documentation as much as possible.\nCreate Java project Using IntelliJ:\ncreate a project search for kafka-client in https://mvnrepository.com/ select the desired version (i.e., the following will use 3.2.0) copy the Gradle implementation information in the dependencies section of the build.gradle file download the dependencies using IntelliJ Gradle window -\u003e Reload... button repeat same steps for slf4j-simple package and set scope to implementation instead of testImplementation create a new package, e.g., com.github.alainbouchard.kafka-demo.demo1 Create a simple producer The following are needed when creating a producer:\nkafka producer properties (or Properties object) kafka producter creation Send data (verification) Close the producer Reference for Kafka Producer Properties.\npublic class KafkaProducerDemo { public static void main(String[] args) { Properties properties = new Properties(); // Set Producer Properties properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\"); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // Create Kafka Producer KafkaProducer\u003cString, String\u003e producer = new KafkaProducer\u003cString, String\u003e(properties); ProducerRecord\u003cString, String\u003e record = new ProducerRecord\u003c\u003e(\"first_topic\", \"hello world from Java!\"); // Send data producer.send(record); // asynchronous, need to flush data producer.flush(); // Tear down Producer producer.close(); } } Create a producer with a callback public class KafkaProducerDemoWithCallback { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(KafkaProducerDemoWithCallback.class); Properties properties = new Properties(); // Set Producer Properties properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\"); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // Create Kafka Producer KafkaProducer\u003cString, String\u003e producer = new KafkaProducer\u003cString, String\u003e(properties); ProducerRecord\u003cString, String\u003e record = new ProducerRecord\u003c\u003e(\"first_topic\", \"hello world from Java!\"); // Send data producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { // execute everytime a message is sent OR an exception is thrown. if (exception == null) { // successfully sent message logger.info(\"Received metadata - Topic: \" + metadata.topic() + \" Partition: \" + metadata.partition() + \" Offset: \" + metadata.offset() + \" Timestamp: \" + metadata.timestamp()); } else { logger.error(\"Error while producer sent a message\", exception); } } }); // asynchronous, need to flush data producer.flush(); // Tear down Producer producer.close(); } } Create a simple consumer The following are needed when creating a consumer:\nkafka consumer properties (or Properties object) kafka consumer creation poll data in a loop Reference for Kafka Consumer Properties.\npublic class KafkaConsumerDemo { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(KafkaConsumerDemo.class.getName()); // Configure the consumer Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\"); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, \"my_group\"); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest // Create consumer KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003cString, String\u003e(properties); // Subscribe the consumer to the Topic or Topics consumer.subscribe(Arrays.asList(\"first_topic\")); // Poll for new data while(true) { // Avoid in production - demo purpose only. ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100)); records.forEach(r -\u003e logger.info(\"Key: \" + r.key() + \" Value: \" + r.value() + \" Partition: \" + r.partition() + \" Offset: \" + r.offset() + \" Timestamp: \" + r.timestamp())); } } } Create a consumer in a thread Note: It is only working if the application does stop gracefully. A Break/Kill signal won’t trigger the shutdown properly. IntelliJ don’t have the exit button on Windows.\npublic class KafkaConsumerWithThreadsDemo { Logger logger = LoggerFactory.getLogger(KafkaConsumerWithThreadsDemo.class.getName()); public KafkaConsumerWithThreadsDemo() { } public void run() { String bootstrapServer = \"localhost:29092\"; String groupId = \"my_group\"; String topic = \"first_topic\"; // Latch for dealing with multiple threads; CountDownLatch latch = new CountDownLatch(1); // Create Consumer Runnable; Runnable consumerRunnable = new ConsumerRunnable(bootstrapServer, groupId, topic, latch); // Starting Consumer Runnable Thread; Thread thread = new Thread(consumerRunnable); thread.start(); // Add a shutdown hook; Runtime.getRuntime().addShutdownHook(new Thread( () -\u003e { logger.info(\"Received a shutdown hook...\"); ((ConsumerRunnable) consumerRunnable).shutdown(); try { latch.await(); } catch (InterruptedException e) { throw new RuntimeException(e); } logger.info(\"Consumer application has exited...\"); })); try { latch.await(); } catch (InterruptedException e) { logger.error(\"Consumer application got interrupted\", e); } finally { logger.info(\"Consumer application is closing...\"); } } public class ConsumerRunnable implements Runnable { private CountDownLatch latch; KafkaConsumer\u003cString, String\u003e consumer; public ConsumerRunnable(String bootstrapServer, String groupId, String topic, CountDownLatch latch) { this.latch = latch; // Configure the consumer Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest // Create consumer consumer = new KafkaConsumer\u003cString, String\u003e(properties); // Subscribe the consumer to the Topic or Topics consumer.subscribe(Arrays.asList(topic)); } @Override public void run() { // Poll for new data try { while (true) { // Avoid in production - demo purpose only. ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100)); records.forEach(r -\u003e logger.info(\"Key: \" + r.key() + \" Value: \" + r.value() + \" Partition: \" + r.partition() + \" Offset: \" + r.offset() + \" Timestamp: \" + r.timestamp())); } } catch ( WakeupException exception) { logger.info(\"Received shutdown signal...\"); } finally { consumer.close(); latch.countDown(); // telling caller code that this thread is done. } } public void shutdown() { // to interrupt the consumer.poll() method // and will make consumer.poll() to throw an exception WakeupException consumer.wakeup(); } } public static void main(String[] args) { new KafkaConsumerWithThreadsDemo().run(); } } Assign and seek consumer The Assign and Seek is mostly used to replay data or fetch a specific message.\npublic class KafkaConsumerWithAssignAndSeekDemo { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(KafkaConsumerWithAssignAndSeekDemo.class.getName()); // Configure the consumer Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\"); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, \"my_group\"); No needs for group with assign and seek... properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest // Create consumer KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003cString, String\u003e(properties); // The Assign and Seek is mostly used to replay data or fetch a specific message. // Assign TopicPartition partition = new TopicPartition(\"first_topic\", 0); consumer.assign(Arrays.asList(partition)); // Seek consumer.seek(partition, 1L); // Poll for new data while(true) { // Avoid in production - demo purpose only. ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100)); records.forEach(r -\u003e logger.info(\"Key: \" + r.key() + \" Value: \" + r.value() + \" Partition: \" + r.partition() + \" Offset: \" + r.offset() + \" Timestamp: \" + r.timestamp())); } } } Kafka Bidirectional Compatibility As Kafka 0.10.2 (July 2017), the client and brokers hava a vapability called bi-directional compatibility (because API calls are nov versioned).\nIt means that the latest client library version should always be used as documented in the confluent documentation Upgrading Apache Kafka Clients Just Got Easier\nCreating a Producer for Twitter messages Needed packages:\ndependencies { implementation group: 'com.twitter', name: 'twitter-api-java-sdk', version: '1.2.4' } Creating the topic:\nkafka-topics --bootstrap-server localhost:9092 --create --topic twitter_tweets --partitions 6 --replication-factor 1 Adding a TwitterKafkaProducerInterface Interface:\npackage com.github.alainbouchard.kafka_demo.demo2; public interface TwitterKafkaProducerInterface { void send(String topic, String message); } Implementing the interface with TwitterKafkaProducer Class:\npublic class TwitterKafkaProducer implements TwitterKafkaProducerInterface { private final Logger logger = LoggerFactory.getLogger(TwitterKafkaProducer.class.getName()); private KafkaProducer\u003cString, String\u003e producer; public TwitterKafkaProducer() { Properties properties = new Properties(); // Set Producer Properties properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\"); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // Create Kafka Producer producer = new KafkaProducer\u003cString, String\u003e(properties); // Create topic: // kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_tweets --partitions 6 --replication-factor 1 } public Logger getLogger() { return logger; } @Override public void send(String topic, String message) { ProducerRecord producerRecord = new ProducerRecord\u003c\u003e(topic, null, message); producer.send(producerRecord, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (exception != null) { getLogger().error(\"Could not send the message to the producer.\", exception); } } }); } } Adding the TwitterListerner Class:\nSome TwitterApi v2 SDK methods have issues at the moment that this document is written.\npublic class TwitterListener { /*** * Ref: * https://developer.axonivy.com/api-browser?url=/market-cache/twitter/twitter-connector-product/9.3.0/openapi.json#/Tweets/addOrDeleteRules * https://github.com/twitterdev/twitter-api-java-sdk/tree/d0d6a8ce8db16faf4e3e1841c3a43bd5a56aa069 * https://developer.twitter.com/en/docs/twitter-api */ private Logger logger = LoggerFactory.getLogger(TwitterListener.class.getName()); // API V2 uses BEARER token. // TODO: Use environment variable for BEARER_TOKEN. private final String BEARER_TOKEN = \"\"; protected TwitterApi twitterApi; private TwitterKafkaProducerInterface twitterKafkaProducer; Function\u003cList\u003cRule\u003e, List\u003cString\u003e\u003e GetIdsFromRules = r -\u003e r.stream().map(Rule::getId).collect(Collectors.toList()); public TwitterListener() { twitterApi = new TwitterApi(); TwitterCredentialsBearer credentials = new TwitterCredentialsBearer(BEARER_TOKEN); twitterApi.setTwitterCredentials(credentials); } public Logger getLogger() { return logger; } public TwitterApi getTwitterApi() { return twitterApi; } public void setTwitterKafkaProducer(TwitterKafkaProducerInterface twitterKafkaProducer) { this.twitterKafkaProducer = twitterKafkaProducer; }; private void logApiExceptionToString(String description, ApiException e) { String text = description + \" Status code: \" + e.getCode() + \" Reason: \" + e.getResponseBody() + \" Response headers: \" + e.getResponseHeaders(); getLogger().error(text, e); } private List\u003cRule\u003e addRule(String value, String tag, boolean dryRun) { // Create rule RuleNoId ruleNoId = new RuleNoId(); ruleNoId.setValue(value); ruleNoId.setTag(tag); // Add rule to list of rules List\u003cRuleNoId\u003e ruleNoIds = new ArrayList\u003c\u003e(); ruleNoIds.add(ruleNoId); // Add the list of rules to the request AddRulesRequest addRulesRequest = new AddRulesRequest(); addRulesRequest.add(ruleNoIds); AddOrDeleteRulesRequest addOrDeleteRulesRequest = new AddOrDeleteRulesRequest(); addOrDeleteRulesRequest.setActualInstance(addRulesRequest); List\u003cRule\u003e rules = null; try { AddOrDeleteRulesResponse result = getTwitterApi().tweets().addOrDeleteRules(addOrDeleteRulesRequest, dryRun); getLogger().info(result.toString()); rules = result.getData(); } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#addOrDeleteRules\", e); } return rules; } private AddOrDeleteRulesResponse deleteRules(List\u003cRule\u003e rules, boolean dryRun) { DeleteRulesRequestDelete deleteRulesRequestDelete = new DeleteRulesRequestDelete(); deleteRulesRequestDelete.ids(GetIdsFromRules.apply(rules)); DeleteRulesRequest deleteRulesRequest = new DeleteRulesRequest(); deleteRulesRequest.setDelete(deleteRulesRequestDelete); AddOrDeleteRulesRequest addOrDeleteRulesRequestForDelete = new AddOrDeleteRulesRequest(); addOrDeleteRulesRequestForDelete.setActualInstance(deleteRulesRequest); AddOrDeleteRulesResponse result = null; try { result = this.getTwitterApi().tweets().addOrDeleteRules(addOrDeleteRulesRequestForDelete, dryRun); getLogger().info(result.toString()); } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#addOrDeleteRules\", e); } return result; } private GetRulesResponse getRules(String paginationToken, List\u003cRule\u003e rules, Integer maxResults) { List\u003cString\u003e ruleIds = rules != null? GetIdsFromRules.apply(rules) : null; GetRulesResponse result = null; try { result = getTwitterApi().tweets().getRules(ruleIds, maxResults, paginationToken); getLogger().info(result.toString()); } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#getRules\", e); } return result; } private GetRulesResponse getRules(String paginationToken, List\u003cRule\u003e rules) { return this.getRules(paginationToken, rules, 1000); } private GetRulesResponse getRules(String paginationToken) { return this.getRules(paginationToken, null); } private void searchStream() { Set\u003cString\u003e expansions = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e tweetFields = new HashSet\u003c\u003e(); tweetFields.add(\"id\"); tweetFields.add(\"author_id\"); tweetFields.add(\"created_at\"); tweetFields.add(\"text\"); Set\u003cString\u003e userFields = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e mediaFields = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e placeFields = new HashSet\u003c\u003e(Arrays.asList()); Set\u003cString\u003e pollFields = new HashSet\u003c\u003e(Arrays.asList()); Integer backfillMinutes = null; // There is a bug in the Twitter API v2 where any specified value will cause an error. InputStream result = null; try { result = getTwitterApi().tweets().searchStream(expansions, tweetFields, userFields, mediaFields, placeFields, pollFields, backfillMinutes); try { JSON json = new JSON(); Type localVarReturnType = new TypeToken\u003cFilteredStreamingTweet\u003e(){}.getType(); BufferedReader reader = new BufferedReader(new InputStreamReader(result)); String line = reader.readLine(); while (line != null) { if (line.isEmpty()) { getLogger().info(\"==\u003e Empty line\"); line = reader.readLine(); continue; } Object jsonObject = json.getGson().fromJson(line, localVarReturnType); String message = jsonObject != null ? jsonObject.toString() : \"Null object\"; getLogger().info(message); twitterKafkaProducer.send(\"twitter_tweets\", message); line = reader.readLine(); } } catch (Exception e) { e.printStackTrace(); } } catch (ApiException e) { logApiExceptionToString(\"Exception when calling TweetsApi#searchStream\", e); } } public static void main(String[] args) { TwitterListener twitterListener = new TwitterListener(); twitterListener.setTwitterKafkaProducer(new TwitterKafkaProducer()); boolean dryRun = false; // Delete all existing Rules; try { GetRulesResponse rulesResponse = twitterListener.getRules(null); AddOrDeleteRulesResponse result = twitterListener.deleteRules(rulesResponse.getData(), dryRun); twitterListener.getLogger().info(\"Deleted rules: \" + twitterListener.GetIdsFromRules.apply(result.getData())); } catch (Exception e) { twitterListener.getLogger().error(\"oops!\"); // bug in the TwitterApi SDK. e.printStackTrace(); } // Adding Rules; twitterListener.addRule( \"potus -is:retweet\", \"Non-retweeted potus tweets\", dryRun); twitterListener.addRule( \"hockey -is:retweet\", \"Non-retweeted hockey tweets\", dryRun); twitterListener.addRule( \"baseball -is:retweet\", \"Non-retweeted baseball tweets\", dryRun); twitterListener.addRule( \"bitcoin -is:retweet\", \"Non-retweeted bitcoin tweets\", dryRun); // Filter twitter stream; twitterListener.searchStream(); } } Fine-tuning the Kafka producer Producers Acks acks=0 no response is requested may loose data if broker goes offline it is okay when lost of data is acceptable: metrics collection log collection Producer Broker 101 partition 0 (leader) -------- ------------------------------- | | +-----[send data to leader]----\u003e+ | | acks=1 (default as Kafka v2.0) leader response is requestion (no guarantee of replication) the producer may retry if the tack isn’t received data loss is possible if leader broker goes offline and replcas haven’t replicated the data yet Producer Broker 101 partition 0 (leader) -------- ------------------------------- | | +----[send data to leader]-----\u003e+ | | +\u003c---[respond write reqs]-------+ | | acks=all (replicas acks) leader and replicas acks are requested adding latency and safety no data loss if enough replicas needed setting to avoid losing data Producer Broker 101 part0 (leader) Broker 102 part0 (replica) Broker 103 part0 (replica) -------- ------------------------- -------------------------- -------------------------- | | | | +---[send data to leader]---\u003e+ | | | | | | | +-----[send to replica]-----\u003e+ | | | | | | +-----[send to replicas]---------------------------------\u003e+ | | | | | +\u003c--------[ack write]--------+ | | | | | | +\u003c--------[ack write]-------------------------------------+ | | | | +\u003c---[respond write reqs]----+ | | | | min.insync.replicas the acks=all must be used along with min.insync.replicas it can be set at the proker or topic level (override) min.insync.replicas=2 means that at least 2 brokers that are ISR (incuding leader) must do the write aknowledgement e.g., with replication.factor=3, the min.insync.replicas=2 and acks=all then only 1 broker can go down or the producer will receive an exception on the send operation Producer retries developers are expected to handle the exceptions or the data may be lost: e.g., transcient failure: NotEnoughReplicasException a retries setting is available: default is 0 no limits to the retries, i.e., Interger.MAX_VALUE in case of retries, there is a change that the messages will be sent in wrong order relying on key-based ordering may be an issue max.in.flight.requests.per.connection (default=5) can be used to help fixing message ordering issue, where max.in.flight.requests.per.connection=1 will ensure ordering, and slow down the throughput. Idempotent producer can be used to help with ordering issues if Kafka version is \u003e= 1.0.0 Idempotent producer using kafka \u003e= 0.11 - an idempotent producer can be defined, which will solve the duplicates due to network issues (i.e., if ack is lost on the network) idempotent producers are great to guarantee a stable and safe pipeline it comes with when using producerProps.put(\"enable.idempotence\", true): retries=Integer.MAX_VALUE max.in.flight.requests=1 with Kafka \u003e= 0.11 and \u003c 1.1 max.in.flight.requests=5 with Kafka \u003e= 1.1 for higher performance acks=all The min.insync.replicas=2 must also be specified since enable.idempotence=true property doesn’t imply this configuration Note: running safe producer might impact the throughput or lantency, and therefore the use case must be verified to make sure the producer is within the NFR expectations An example of Producer settings to improve safeness:\n// Make a safer producer properties.setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\"); properties.setProperty(ProducerConfig.ACKS_CONFIG, \"all\"); // default value with Idempotence=true properties.setProperty(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE)); // default value with Idempotence=true properties.setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"5\"); // with kafka \u003e= 2.0, otherwise use 1. Message compression Some compression benchmarks can be found the this blog: Squeezing the firehose: getting the most from Kafka compression\nproducer usually send data text-based (json) messages compression is set at the producer configuration only no needs for consumer or broker configuration the compression.type can be none (default), gzip, lz4 and snappy compression will improve the throughput and performance +----------------+ +--------------------+ +---------------+ | Producer Batch |------\u003e| Compression of the |-----\u003e| Kafka Cluster | +----------------+ | batch of messages | +---------------+ +--------------------+ Compression advantages:\nsmaller producer request size (compression up to 4x) faster to tranfer data over the network better throughput better disk utilisation in Kafka cluster Compression disavantages:\nproducers and consumers must commit CPU time for compression/decompression Overall:\nsnappy and lz4 are optimal for speed/compression ratio recommendations are to try the algorithms in a given use case and environment always use compression in prod should use along with linger.ms and batch.size configuration settings Configuration: linger.ms and batch.size Kafka producer default behavior is to send records as soon as possible it will have up to 5 requests in flight (batch) batching messages are done simultaneously with messages are in-flight (no time wasted) smart batching allows kafka to increase throughput while maintaining very low latency linger.ms is the number of ms (default=0) that the producer is willing to wait to fully get a batch of messages by introducing some lag (e.g., linger.ms=5) then we increase the cahnges of messages being sent together in a batch at the cost of introducing a small delay (e.g., 5 ms) then the throughput can be increased, the compression is more efficient and the producer efficiency is better batch.size is the maximum mumber of bytes (default=16KB) that will be included in a batch increasing a batch size to higher number (32KB or 64KB) can help increasing the ompression, throughput, and efficiency of requests any message tha is bigger than a batch size will not be batched the producer will make or allocate a batch per partition the average batch size metric can be monitored by using the Kafka Producer Metrics An example of Producer settings to improve efficiency:\n// Improve throughput efficiency of the producer properties.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"snappy\"); properties.setProperty(ProducerConfig.LINGER_MS_CONFIG, \"20\"); properties.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, \"32768\"); // 32 KB Producer default partition and key hashing The default key are hashed using murmur2 algorithm it is possible - but maybe not suggested - to override the partitioner behavior using partitioner.class the formula from Kafka code: targetPartition = Utils.abs(Utils.murmur2(record.key())) % numPartitions; therfore the same key will always go to the same partition changing the number of partition will cause key vs partition issues and should be avoided Max.blocks.ms and buffer.memory These are advanced settings and it is probably better to avoid tweaking them unless necessary.\nwhen the producer prodeuces faster tahn the broker can handle then the records will be memory buffered the buffer.memory is the size of the buffer (default is 32MB) a full buffer (e.g., full 32MB) will cause the producer.send() method to block (or wait) the max.block.ms (default=60000ms) is the waiting time befre the producer.send() method throw an exception, and causes are: the producer has filled up the buffer the broker is not accepting any new data 60s has elapsed An exception may mean that the broker is down or overloaded as it can’t handle the requests Elastic Search Adding elasticsearch docker container The following must be added in order to add an elastic search container or server to the kafka-cluster:\ndocker-compose.yaml:\nelasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 ports: - 9200:9200 - 9300:9300 environment: discovery.type: single-node node.name: es01 cluster.name: kafka-cluster To start the service from docker-compose:\n\u003e docker-compose up elasticsearch -d \u003e docker-compose ps NAME COMMAND SERVICE STATUS PORTS kafka-cluster-elasticsearch-1 \"/usr/local/bin/dock…\" elasticsearch running 0.0.0.0:9200-\u003e9200/tcp, 0.0.0.0:9300-\u003e9300/tcp kafka-cluster-kafka-1 \"/etc/confluent/dock…\" kafka running 0.0.0.0:29092-\u003e29092/tcp kafka-cluster-zookeeper-1 \"/etc/confluent/dock…\" zookeeper running 0.0.0.0:22181-\u003e2181/tcp \u003e curl localhost:9200/ { \"name\" : \"es01\", \"cluster_name\" : \"kafka-cluster\", \"cluster_uuid\" : \"-w05UbWdSeOhc4nRGcy8yA\", \"version\" : { \"number\" : \"7.5.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"8bec50e1e0ad29dad5653712cf3bb580cd1afcdf\", \"build_date\" : \"2020-01-15T12:11:52.313576Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.3.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Java elasticsearch client The following packages are required:\nimplementation group: 'org.elasticsearch.client', name: 'elasticsearch-rest-high-level-client', version: '7.14.0' implementation group: 'org.apache.logging.log4j', name: 'log4j-core', version: '2.17.2' // Elastic search dependency public class ElasticsearchClient { Logger logger = LoggerFactory.getLogger(ElasticsearchClient.class.getName()); private RestHighLevelClient restHighLevelClient; public ElasticsearchClient() { restHighLevelClient = new RestHighLevelClient( // TODO: use configuration file or environment variables to set ip and ports, now configured for local docker containers. RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\"), new HttpHost(\"localhost\", 9300, \"http\"))); } public RestHighLevelClient getRestHighLevelClient() { return restHighLevelClient; } public boolean ping() { boolean result = false; try { result = getRestHighLevelClient().ping(RequestOptions.DEFAULT); } catch (IOException e) { logger.error(\"Elasticsearch client received an exception.\", e); } finally { logger.info(\"Elasticsearch aliveness: \" + result); } return result; } public IndexResponse toIndex(String index, String jsonSource, String id) { IndexRequest indexRequest = new IndexRequest(index) .id(id) // Make the entry idempotent. .source(jsonSource, XContentType.JSON); IndexResponse indexResponse = null; try { indexResponse = getRestHighLevelClient().index(indexRequest, RequestOptions.DEFAULT); logger.info(indexResponse.getId()); } catch (IOException e) { logger.error(\"Caught and exception.\", e); } return indexResponse; } public void close() { try { getRestHighLevelClient().close(); } catch (IOException e) { logger.error(\"Caught and exception.\", e); } } public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(ElasticsearchClient.class.getName()); // Setup Elasticsearch ElasticsearchClient elasticsearchClient = new ElasticsearchClient(); elasticsearchClient.ping(); // Setup Kafka Consumer TwitterKafkaConsumer twitterKafkaConsumer = new TwitterKafkaConsumer(); twitterKafkaConsumer.subscribe(\"twitter_tweets\"); // Get data using Kafka Consumer and insert data to the elasticsearch while(true) { // TODO: replace with better logic. ConsumerRecords\u003cString, String\u003e records = twitterKafkaConsumer.poll(Duration.ofMillis(100)); records.forEach(record -\u003e { FilteredStreamingTweetResponse tweet = twitterKafkaConsumer.mapTweetToObject(record.value()); elasticsearchClient.toIndex(\"twitter\", record.value(), tweet.getData().getId()); logger.debug(\"Key: \" + record.key() + \" Value: \" + record.value() + \" Partition: \" + record.partition() + \" Offset: \" + record.offset() + \" Timestamp: \" + record.timestamp()); try { Thread.sleep(1000); } catch (InterruptedException e) { logger.error(\"Error while waiting for sleep\", e); } }); } // Tear-down the Elasticsearch // elasticsearchClient.close(); // Tear-down the Kafka Consumer } } Kafka delivery semantics The at least once processing should used for most applications along with idempotent strategy.\nAt most once Offsets are committed as soon as the message batch is received. If the processing oes wrong, the message will be lost (it won’t be read again)\nAt least once Offsets are committed after the message is processed. If the processing goes wrong, the message will be read again. This can resilt in duplicate processing of messages. Idempotence of the messages is needed to avoid duplicates.\nExactly once Can be achieved for kafka-to-kafka workflows using kafka streams APIs. For the Kafka-to-Sink workflows, idempotent consumer is needed.\nIdempotentce and offset auto-commit The default configuration (ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG=\"enable.auto.commit\") setting is at-least-once if not specified otherwise.\nTo avoid duplicate entries, the usage of a unique key will be required. An example would be the Twitter ID.\nConsumer Poll Behaviours Default values should be correct in most cases but the following can be adjuested.\nfetch.min.bytes (default=1):\ncontrols the minimum data to pull on each request help improving throughput and decreasing request numbr cost is latency max.poll.records (default=500):\ncontrols the number of records to receive per poll request increase if the messages are very small and if RAM is available best practices tell to monitor the number of records per poll request and to adjust to increase the value if default value is often reached max-partitions.fetch.bytes (default=1MB):\nmaximum data returned by the broker per partition reading from many partions will require a lot of memory fetch.max.bytes (default 50MB):\nmax data returned for each fetch request (covers multiple partitions) the consumer performs multiple fetches in parallel Consumer Offset Commits Strategies The two strategies:\nenable.auto.commit=true and syncrhonous processing of record batches (default)\npseudocode:\nwhile(true) { List\u003cRecords\u003e records = consumer.poll(Duration.ofMillis(100)); doSomethingSunchronous(records); } offsets get automatically commited at regular interval auto.commit.interval.ms=5000 (default) Note: using asyncrhonous processing would make the delivery sementic to at-most-once since the offset will be committed before the data is processes enable.auto.commit=false and manual commit of offsets\npseudocode:\nwhile(true) { records += consumer.poll(Duration.ofMillis(100)); if isReady(records) { doSomethingSynchronous(records); consumer.commitSync(); } } offsets commit is controlled according to the expected conditions\nE.g., accumulation recortds into a buffer and then flushing the buffer to a DB, then offsets are committed\nManual commits Settings to be configured to avoid offsets auto commits:\nproperties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,\"false\"); // Will require manual offsets commits properties.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"10\"); // Only retrieve 10 records at the time The syncronous commit command for the consumer - pseudocode:\nwhile(true) { ConsumerRecords\u003cString, String\u003e records = twitterKafkaConsumer.poll(Duration.ofMillis(100)); logger.info(String.format(\"Received %s records\", records.count())); records.forEach(record -\u003e { // do something synchronous here... }); logger.info(\"Committing offsets\"); twitterKafkaConsumer.commitSync(); logger.info(\"Offsets were committed\"); } Using kafka-console-consumer command:\nlooking at the offsets for given group kafka-java-demo-elasticsearch: kafka-consumer-groups --bootstrap-server localhost:9092 --group kafka-java-demo-elasticsearch --describe Consumer group 'kafka-java-demo-elasticsearch' has no active members. GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID kafka-java-demo-elasticsearch twitter_tweets 3 0 39 39 - - - kafka-java-demo-elasticsearch twitter_tweets 0 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 4 0 63 63 - - - kafka-java-demo-elasticsearch twitter_tweets 5 0 43 43 - - - kafka-java-demo-elasticsearch twitter_tweets 2 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 1 0 58 58 - - - sh-4.4$ consuming records using Java consumer and verifying the offsets: sh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --group kafka-java-demo-elasticsearch --describe Consumer group 'kafka-java-demo-elasticsearch' has no active members. GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID kafka-java-demo-elasticsearch twitter_tweets 3 0 39 39 - - - kafka-java-demo-elasticsearch twitter_tweets 0 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 4 0 63 63 - - - kafka-java-demo-elasticsearch twitter_tweets 5 30 43 13 - - - kafka-java-demo-elasticsearch twitter_tweets 2 0 37 37 - - - kafka-java-demo-elasticsearch twitter_tweets 1 0 58 58 - - - sh-4.4$ Note: the current offset for the partition 5 (above example) should be a multiple of ConsumerConfig.MAX_POLL_RECORDS_CONFIG property value, any record (modulo) will be consumed again since the offset wasn’t committed Consumer offset reset behaviour The offset reset available behaviours are:\nauto.offset.reset=latest which will read from the end of the log auto.offset.reset=earliest which will read from start of the log auto.offset.reset=none which will throw exception if no offset is found The consumer offsets can be lost:\nif a consumer hasn’t read new data for 1 day (kafka \u003c 2.0) if a consumner hasn’t read new data for 7 days (kafka \u003e= 2.0) The retention delay can be set using the broker setting offset.retention.minutes. Proper data and offset retention period must be set to ensure no data loss if a server can go down for a while, e.g., 30 days.\nReplaying the data for a consumner group To reset kafka server offsets and replay messages from beginning:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --topic twitter_tweets --group kafka-java-demo-elasticse arch --reset-offsets --to-earliest --execute GROUP TOPIC PARTITION NEW-OFFSET kafka-java-demo-elasticsearch twitter_tweets 3 0 kafka-java-demo-elasticsearch twitter_tweets 0 0 kafka-java-demo-elasticsearch twitter_tweets 4 0 kafka-java-demo-elasticsearch twitter_tweets 5 0 kafka-java-demo-elasticsearch twitter_tweets 2 0 kafka-java-demo-elasticsearch twitter_tweets 1 0 sh-4.4$ This allows to restart the consumer and replay the messages; having an idempotent server will make replays safe.\nControlling Consumer Liveliness consumers in a group talk to a Consumer Group Coordinator there is a heartbeat and a pool mechanism to detect if consumers are down best practices tell that a process should process data fast and poll often Consumer Heartbeat Thread:\nsession.timeout.ms (default=10s)\nheardbeets are sent periodically to broker if no heartbeat is sent during that period, the consumer is considered dead set low value to faster consumer rebalancing heartbeat.interval.ms (default=3s)\nwait period between 2 heartbeats best pratices tell to set 1/3rd of the session.timeout.ms value both settings are set together to detect a dead consumer application (down)\nConsumer Poll Thread:\nmax.poll.interval.ms (default=5m)\nmaximum time between 2 poll() alls before declaring the consumer is dead relevant for Big Data frameworks (e.g., Spark) in case processing takes time mecahnism to detect a data processing issue with the consumer\nKafka Connect and Kafka Stream There are 4 Kafka use cases: Source to Kafka =\u003e Producer API =\u003e Kafka Connect Source Kafka to Kafka =\u003e Consumer API/Producer API =\u003e Kafka Streams Kafka =\u003e Sink =\u003e Consumer API =\u003e Kafka Connect Sink Kafka =\u003e Application =\u003e Consumer API =\u003e Kafka connect and Kafka stream will simplify and improve the in/out of Kafka Kafka connect and Kafka stream will simplify transforming data within kafka withou relying on external libraries The Kafka connectors can be found on the Confluent Kafka Connectors page.\nWhy Kafka Connect? developpers always want to import data from the same sources: DB, JDBC, Couchbase, Goldergate, SAP HANA, Blockchain, Cassandra, DynamoDB, FTP, IOT, MongoDB, MQTT, RethinkDB, Salesforce, Solr, SQS, Twitter, etc. developpers always want to store data in the same sinks: S3, ElasticSearch, HDFS, JDBC, SAP HANA, DocumentDB, Cassandra, DynamoDB, HBase, MongoDB, Redis, Solr, Splunk, Twitter, etc. an unexperimented developper may struggle to achieve fault tolerance, idempotence, distribution, ordering, etc. an experimented developper already did the work for others. Sources/Destinations Connect Cluster Kafka Cluster Stream Apps -------------------- --------------- ------------- ----------- [source1]-------------\u003e[worker] [broker]--------------\u003e[app1] [worker]------------------\u003e[broker]\u003c-------------- [source2] [worker] [broker] [worker]\u003c------------------[broker]--------------\u003e[app2] [sinks]\u003c--------------[worker] \u003c-------------- Kafka Connect: High level source connectors to get data from Common Data Sources sink connectors to publish data in common data stores make it easy for non-experienced developpers to quickly get data in kafka part of the ETL pipeline (Extract, Transform and Load) scaling made easy from small pipelines to company-wide pipelines re-usable code Adding a connector to a Java project The connector kafka-connect-twitter will be used as an example to show file structure:\n+ [my-java-project] +-+ [kafka-connect] (to be created with sub folders) | +-+ [connectors] | | +-+ [kafka-connect-twitter] (downloaded connector project from github) | | | +-+ *.jar | | +-+ connector-standalone.properties | | | + run.sh | | | + twitter.properties Connect standalone CLI command sh-4.4$ connect-standalone USAGE: /usr/bin/connect-standalone [-daemon] connect-standalone.properties sh-4.4$ Creating a new topic for the kafka connector:\n$ kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_status_connect --parti tions 3 --replication-factor 1 WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic twitter_status_connect. $ kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_deletes_connect --part itions 3 --replication-factor 1 WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic twitter_deletes_connect. $",
    "description": "Warning 2022-07-11: This is a WORK IN PROGRESS document (WIP) and need to be reviewed.\nApache Kafka Topics, partitions and offsets Brokers Brokers and topics Topic replication factor Producers Consumers Consumer offsets Kafka broker discovery Zookeeper Kafka guarantees Install Kafka using Docker images Use Kafka Topics CLI Topics CLI Kafka console producer Kafka console consumer Kafka-Client with Java Kafka Connect and Kafka Stream Apache Kafka Why Apache Kafka?",
    "tags": [],
    "title": "Apache Kafka (WIP)",
    "uri": "/notes/apache-kafka/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "Test Flow by Environment Test Flow Diagram Test Description Software Development Life Cycle (SDLC) What is SDLC? The Test Strategy within the SDLC Quality and SDLC Flow Conclusion We often discuss concepts like SDLC (Software Development Life Cycle) or a Test Strategy, but rarely do we focus on how to seamlessly integrate the test strategy into the SDLC. This is crucial because a test strategy isn’t just a standalone document — it’s a roadmap that ensures quality throughout each phase of development.\nWhile many people understand what the SDLC is, they may not fully grasp the goals, activities, or expected outcomes of each phase. Each phase, from requirements to maintenance, plays a unique role in shaping the final product, and testing must align with these stages to maximize efficiency and quality.\nTo make this integration clear, we need to focus on test types, where to run them, and who is responsible. It will be covered later in this article.\nBy aligning the test strategy with the SDLC, everyone - from developers to QA, including DevOps, Security, IT and Compliance people — knows their role in ensuring quality, and testing becomes an integral part of the development process, not an afterthought.\nTest Flow by Environment The Test Pyramid has become a buzzword in testing conversations, often paired with trendy concepts like shift left and shift right. While these ideas are valuable, let’s take a step back and explore testing through the lens of a typical deployment flow, grounding the discussion in practical terms.\nIn this section, we’ll dive into an exhaustive list of test types that ensure a robust testing strategy. These will include:\nFunctional Tests: Focus on verifying the application’s functionality, including unit, integration, and end-to-end tests. Non-Functional Tests: Cover aspects like performance, scalability, usability, and accessibility. Security Tests: Assess the application’s resilience to threats and ensure data protection. Analytic Tests: Validate data accuracy, tracking, and reporting mechanisms. Compliance Tests: Ensure adherence to regulatory, legal, and organizational standards. Business Continuity Tests: Test disaster recovery, backup, rollback mechanisms, and other continuity measures. Deployment Strategy Tests: Validate deployment approaches such as blue/green, canary, and rolling deployments. Compatibility Tests: Ensure the application works across various browsers, devices, and operating systems. Data Integrity Tests: Verify the accuracy, consistency, and reliability of data across systems. Localization and Internationalization Tests: Ensure the application functions correctly in different regions and languages. By aligning these tests with SDLC, we aim to create a clear, actionable roadmap for delivering high-quality software.\nTest Flow Diagram The diagram below represents a per-environment test flow, outlining the environments, test types, and their owners. While this flow is designed around typical industry practices, it’s important to note that every organization may have different environments, naming conventions, or even responsibilities assigned to teams.\nThe responsibilities listed in this flow align with standard industry roles, but these may vary depending on a company’s structure or specific needs. Additionally, adding quality and test coverage is an iterative process, meaning that no single test strategy will cover every possible test type from the start. Instead, test strategies evolve over time to adapt to new challenges and ensure comprehensive quality.\ngraph TD %% Subgraph: Local subgraph Local [Local Environment] subgraph \"\" \"\" UnitTests_Local[\"Unit Tests - Dev\"] InProcessComponentTests_Local[\"In-Process Component Tests - Dev\"] NarrowIntegrationTests_Local[\"Narrow Integration Tests - Dev, QA\"] end subgraph \"\" \"\" StaticCodeAnalysis_Local[\"Static Code Analysis - Dev\"] CodeCoverage_Local[\"Code Coverage - Dev, QA\"] end subgraph \"\" \"\" SmokeTests_Local[\"Smoke Tests - Dev, QA\"] EndToEndTests_Local[\"End-to-End (E2E) Tests - QA\"] end end %% Arrow to CI Local --\u003e CI %% Subgraph: CI subgraph CI [Continuous Integration - CI] subgraph \"\" \"\" InProcessComponentTests_CI[\"In-Process Component Tests - Dev\"] OutOfProcessComponentTests_CI[\"Out-of-Process Component Tests - Dev, QA\"] NarrowIntegrationTests_CI[\"Narrow Integration Tests - Dev, QA\"] BroadIntegrationTests_CI[\"Broad Integration Tests - QA, DevOps\"] APIContractTests_CI[\"API Contract Tests - Dev, QA\"] end subgraph \"\" \"\" StaticCodeAnalysis_CI[\"Static Code Analysis - Dev\"] CodeCoverage_CI[\"Code Coverage - Dev, QA\"] SecurityScans_CI[\"Security Scans - Security, DevOps\"] CrossBrowserTests_CI[\"Cross-Browser Tests - QA\"] AccessibilityTests_CI[\"Accessibility Tests - QA\"] end subgraph \"\" \"\" EndToEndTests_CI[\"End-to-End (E2E) Tests - QA\"] SmokeTests_CI[\"Smoke Tests - Dev, QA\"] ConfigurationTests_CI[\"Configuration Tests - DevOps\"] ComplianceTests_CI[\"Compliance Tests - Compliance, Security\"] end end %% Arrow to Preprod CI --\u003e Preprod %% Subgraph: Preprod subgraph Preprod [Pre-production Environment] subgraph \"\" \"\" OutOfProcessComponentTests_Preprod[\"Out-of-Process Component Tests - Dev, QA\"] BroadIntegrationTests_Preprod[\"Broad Integration Tests - QA, DevOps\"] APIContractTests_Preprod[\"API Contract Tests - Dev, QA\"] SecurityScans_Preprod[\"Security Scans - Security, DevOps\"] LoadTests_Preprod[\"Load Tests - QA, DevOps\"] end subgraph \"\" \"\" PerformanceTests_Preprod[\"Performance Tests - QA, DevOps\"] CrossBrowserTests_Preprod[\"Cross-Browser Tests - QA\"] AccessibilityTests_Preprod[\"Accessibility Tests - QA\"] EndToEndTests_Preprod[\"End-to-End (E2E) Tests - QA\"] SmokeTests_Preprod[\"Smoke Tests - Dev, QA\"] end subgraph \"\" \"\" DatabaseMigrationTests_Preprod[\"Database Migration Tests - Dev, DBA\"] RollbackTests_Preprod[\"Rollback Tests - DevOps, DBA\"] DisasterRecoveryTests_Preprod[\"Disaster Recovery Tests - DevOps, IT\"] ComplianceTests_Preprod[\"Compliance Tests - Compliance, Security\"] ConfigurationTests_Preprod[\"Configuration Tests - DevOps\"] end end %% Arrow to Prod Preprod --\u003e Prod %% Subgraph: Prod subgraph Prod [Production Environment] subgraph BlueGreen [Blue/Green Deployment] SmokeTestsBG_Prod[\"Smoke Tests - Dev, DevOps, QA\"] PostDeploymentTests_Prod[\"Post-Deployment Tests - QA, DevOps\"] end subgraph \"\" \"\" SmokeTests_Prod[\"Smoke Tests - Dev, QA\"] DisasterRecoveryTests_Prod[\"Disaster Recovery Tests - DevOps, IT\"] ComplianceTests_Prod[\"Compliance Tests - Compliance, Security\"] end subgraph \"\" \"\" SecurityScans_Prod[\"Security Scans - Security, DevOps\"] LoadTests_Prod[\"Load Tests - QA, DevOps\"] PerformanceTests_Prod[\"Performance Tests - QA, DevOps\"] end end Test Description This section presents test types in a clear and concise table format. The table includes the test type, the environments where it is typically executed, the owners responsible for the tests, and a short description of each test type.\nWhile the information provided reflects industry standards, it is not a strict rule. Every organization may adapt these test types, environments, and responsibilities to fit their unique workflows and quality assurance strategies.\nTest Type Environment(s) Who Description Unit Tests Local, CI Developers Verify individual functions or modules. Should run quickly in isolation and are ideal for CI. In-Process Component Tests Local, CI Developers Test individual components within the same process (e.g., an API controller with a mock database layer). This verifies component behavior without external dependencies. Out-of-Process Component Tests CI, Preprod Developers, QA Engineers Test components with actual dependencies in separate processes (e.g., an API calling a real database or microservice). Verifies real interactions. Narrow Integration Tests Local, CI Developers, QA Engineers Test interactions between a few closely-related components or services within the same system boundary. Validates integrations, such as between modules or a service and its database. Broad Integration Tests CI, Preprod QA Engineers, DevOps Test larger cross-system or cross-boundary integrations, like microservices or third-party APIs. Ensures multiple components interact correctly. API Contract Tests Local, CI, Preprod Developers, QA Engineers Ensure APIs adhere to specified contracts, helping prevent breaking changes. Static Code Analysis Local, CI Developers Analyze code quality (e.g., linting). Typically run in CI and locally to catch style issues early. Code Coverage Local, CI Developers, QA Engineers Assess the percentage of code exercised by tests, typically monitored in CI. Security Scans CI, Preprod, Prod Security Engineers, DevOps Detect security vulnerabilities; best run in CI and preprod, with continuous monitoring in prod. Load Tests Preprod, Prod (if isolated) QA Engineers, DevOps Evaluate system performance under load; run in preprod to avoid impact, validated in production when feasible. Performance Tests Preprod, Prod (controlled) QA Engineers, DevOps Test system speed and response under normal load. Can run in preprod, occasionally in production. Cross-Browser Tests CI, Preprod QA Engineers Verify UI functionality across browsers; CI for regular checks, preprod for critical flows. Accessibility Tests CI, Preprod QA Engineers Ensure app is accessible (e.g., WCAG standards). Run in CI for consistency, preprod for final check. End-to-End (E2E) Tests Local, CI, Preprod QA Engineers Simulate full user flows; run in CI for main paths, preprod for final verification. Smoke Tests Local, CI, Preprod, Prod (Blue/Green) Developers, DevOps, QA Engineers Basic functionality check to verify stability in each environment. Often executed on both blue and green environments in production to validate stability before and after deployment. Post-Deployment Tests Prod (Blue/Green) QA Engineers, DevOps Comprehensive testing (e.g., E2E, performance) conducted on the newly deployed environment to confirm stability and functionality before the switch. Database Migration Tests Preprod Developers, Database Administrators Ensure database changes deploy correctly; should run in preprod to confirm migration stability. Rollback Tests Preprod DevOps, Database Administrators Test rollback strategy if deployment fails; typically run in preprod for validation. Disaster Recovery Tests Preprod, Prod (if feasible) DevOps, IT Operations Ensure recovery process works; ideal in preprod, with periodic validation in production. Compliance Tests CI, Preprod, Prod Compliance Officers, Security Engineers Confirm compliance requirements are met (e.g., GDPR). Run in CI/preprod and monitor in prod. Configuration Tests CI, Preprod DevOps Check for proper configuration settings; primarily CI with validation in preprod. Software Development Life Cycle (SDLC) What is SDLC? The Software Development Life Cycle (SDLC) is a structured process guiding software projects from start to finish. Each phase has specific goals, activities, and deliverables that build on each other to ensure successful development and maintenance. The following table are the key SDLC phases:\nPhase Goal Activities Outcome Requirements Define what the software needs to do. Gather and document requirements with stakeholders. Clear requirements for design and development. Design Create a blueprint for the software. Define architecture, interfaces, databases, and plan security and scalability. Detailed design documents and diagrams. Development Build the software. Write code, create unit tests, and conduct code reviews. Working prototype or complete software. Testing Ensure the software meets quality. Perform unit, integration, system, and user acceptance testing (UAT). Fix any issues. Stable version ready for deployment. Deployment Release the software for use. Deploy software to production, monitor, and run acceptance tests. Live, functioning product accessible to users. Maintenance \u0026 Support Keep the software functional post-release. Monitor, fix bugs, apply patches, and implement enhancements based on feedback. Continuous support and updates until retirement. This table provides a concise overview of each phase, including its purpose, activities, and outcomes.\nThe Test Strategy within the SDLC Mapping the Test Strategy over the SDLC phases can create a comprehensive quality framework, ensuring that quality standards are integrated at each step. This approach establishes quality checkpoints and testing strategies throughout the development process. Here’s how each phase can incorporate elements of the quality architecture:\n1. Requirements Gathering and Analysis Quality Objectives: Define quality standards, acceptance criteria, and regulatory requirements with input from all stakeholders. Test Strategy and Planning: Outline an initial test plan, including types of testing required (functional, security, performance) and establish early quality metrics. Roles and Responsibilities: Product Managers (PM) and QA collaborate to create clear, testable requirements and acceptance criteria. 2. System Design Test Design and Coverage: Develop test cases and design documents that map directly to requirements, focusing on both functional and non-functional requirements. Quality Metrics: Define metrics like test coverage, complexity, and maintainability as part of design specifications. Automation Planning: Determine areas for automated testing (such as unit and integration tests) and establish CI/CD integration points. Roles and Responsibilities: QA and DevOps engage with developers to ensure testability, and DevOps designs CI/CD pipeline for early test integration. 3. Development Code Quality and Reviews: Developers follow coding standards and conduct code reviews for quality assurance. Automated tools, like linters and static code analysis, can be incorporated here. Unit Testing and Test Coverage: Implement unit tests for individual components and monitor test coverage as code is developed. Continuous Integration (CI): Use CI tools to automatically build and run tests with each code commit to catch issues early. Roles and Responsibilities: Developers write and run unit tests, and QA may validate test case coverage. 4. Testing Functional Testing: QA executes functional, integration, and end-to-end tests to verify that all requirements are met. Non-Functional Testing: Perform performance, security, and usability testing to evaluate software robustness under different scenarios. Automated Regression Testing: Run automated regression tests on each new build to ensure new changes don’t introduce defects. Quality Metrics and Monitoring: Track defect density, test coverage, and test pass rates. Analyze results to inform potential fixes or improvements. Roles and Responsibilities: QA leads testing efforts, with developers addressing reported issues, and DevOps maintaining test environments. 5. Deployment Environment Validation: Ensure the production environment is consistent and validated against pre-deployment criteria. Final Acceptance Testing: Conduct final smoke tests or sanity checks to confirm functionality in the production-like environment. Monitoring Setup: Set up application and quality monitoring for post-deployment analysis. Roles and Responsibilities: DevOps manages deployment and monitoring setup, while QA ensures final tests are successful. 6. Maintenance and Support Continuous Quality Monitoring: Track production metrics like uptime, response time, and error rates. Use monitoring tools to identify issues and trends. Defect Management: Prioritize and fix bugs identified in production, using insights to refine tests and improve future releases. Feedback Loops: Implement customer feedback and production incident reports to continually improve quality standards and test cases. Roles and Responsibilities: QA and DevOps collaborate on monitoring, with developers addressing production issues, and PM integrating customer feedback. Quality and SDLC Flow flowchart TD A[Requirements] --\u003e B[Design] B --\u003e C[Development] C --\u003e D[Testing] D --\u003e E[Deployment] E --\u003e F[Maintenance] F --\u003e A %% Quality Architecture with Responsibilities subgraph Quality_Architecture QA1[Define Quality Objectives - PM, QA] --\u003e A QA2[Develop Test Strategy - PM, QA] --\u003e A QA3[Design Test Cases and Coverage - QA] --\u003e B QA4[Set Quality Metrics - PM, QA] --\u003e B QA5[Code Reviews and Unit Testing - Developers] --\u003e C QA6[Functional, Integration Testing - QA] --\u003e D QA7[Non-Functional Testing - QA, Developers] --\u003e D QA8[Final Acceptance Testing - QA] --\u003e E QA9[Environment Validation and Monitoring - DevOps, QA] --\u003e E QA10[Continuous Quality Monitoring - DevOps, QA] --\u003e F QA11[Feedback Loops for Improvement - PM, QA, DevOps] --\u003e F end Conclusion This article is focusing on how to align Testing Strategies with the Software Development Life Cycle (SDLC) to ensure quality at every stage of development. We looked at how different test types fit into the SDLC and how responsibilities are distributed among teams.\nKey points covered:\nThe SDLC provides a clear roadmap with goals, activities, and outcomes for each phase. Tests can be grouped into categories like functional, non-functional, security, and compliance to make sure all areas are covered. Responsibilities and environments may differ across organizations, so it’s important to adapt industry standards to your specific needs. Ensuring quality is essential at every phase of the SDLC. Testing is a process that evolves over time, and this article insights highlight the value of collaboration and continuous improvement to build high-quality software.",
    "description": "Test Flow by Environment Test Flow Diagram Test Description Software Development Life Cycle (SDLC) What is SDLC? The Test Strategy within the SDLC Quality and SDLC Flow Conclusion We often discuss concepts like SDLC (Software Development Life Cycle) or a Test Strategy, but rarely do we focus on how to seamlessly integrate the test strategy into the SDLC. This is crucial because a test strategy isn’t just a standalone document — it’s a roadmap that ensures quality throughout each phase of development.",
    "tags": [],
    "title": "Building Quality: Aligning Test Strategies with the SDLC",
    "uri": "/blogs/sdlc-building-quality/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Install Cypress with npm Open Cypress from CLI Using TypeScript with Cypress Dependencies installation Configure TypeScript tsconfig.ts File ESLint Installation for Cypress and TypeScript Install Cypress with npm From terminal:\n\u003e npm init -y \u003e npm install cypress --save-dev Expect package.json file to be created an contain the following:\n\u003e cat package.json { \"devDependencies\": { \"cypress\": \"^13.15.0\", }, \"dependencies\": { } } Open Cypress from CLI \u003e npx cypress open Using TypeScript with Cypress Information may be found on Cypress TypeScript documentation page.\nDependencies installation Install TypeScript locally:\nnpm install typescript --save-dev Expect the following:\n\u003e cat package.json { \"devDependencies\": { \"cypress\": \"^13.15.0\", \"typescript\": \"^5.6.3\" }, \"dependencies\": { } } Optionally, install the ESLinter for TypesSript:\n\u003e npm install eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin --save-dev \u003e npx eslint --init Pick the right configuration:\n✔ How would you like to use ESLint? · problems ✔ What type of modules does your project use? · esm ✔ Which framework does your project use? · none ✔ Does your project use TypeScript? · typescript ✔ Where does your code run? · browser The config that you've selected requires the following dependencies: eslint, globals, @eslint/js, typescript-eslint ✔ Would you like to install them now? · Yes ✔ Which package manager do you want to use? · npm Install Cypress Plugin:\n\u003e npm install eslint-plugin-cypress --save-dev Configure TypeScript tsconfig.ts File Configure TypeScript for Cypress:\nFrom the cypress directory, run the following command.\n\u003e npx tsc --init --types cypress,node,jquery --lib dom,es6 Expect a tsconfig.json file to be created at the project’s cypress directory:\nCreated a new tsconfig.json with: target: es2016 module: commonjs lib: dom,es6 strict: true types: cypress esModuleInterop: true skipLibCheck: true forceConsistentCasingInFileNames: true Note: the tsconfig.json should be in the cypress directory, therefore it should be moved if it was created at the project root level.\n{ \"compilerOptions\": { \"types\": [\"cypress\", \"jquery\"], \"target\": \"es6\", \"lib\": [\"es6\", \"dom\"], // Ensure DOM types are included \"moduleResolution\": \"node\", \"esModuleInterop\": true }, \"include\": [\"cypress/**/*.ts\", \"cypress/**/*.d.ts\"] // Ensure it includes Cypress TypeScript files } ESLint Installation for Cypress and TypeScript You’ll need to install the following npm packages to configure ESLint for TypeScript and Cypress:\nCore ESLint Packages:\neslint: The core ESLint package. @eslint/js: JavaScript-specific ESLint configurations (recommended rules). @typescript-eslint/parser: ESLint parser for TypeScript. @typescript-eslint/eslint-plugin: Plugin for TypeScript-specific ESLint rules. eslint-plugin-cypress: ESLint plugin for Cypress-specific rules. Globals Package:\nglobals: Provides global variables for environments like browsers, Node.js, Cypress, Mocha, etc. \u003e npm install eslint @eslint/js @typescript-eslint/parser @typescript-eslint/eslint-plugin eslint-plugin-cypress globals --save-dev Installation command:\n\u003e npm install eslint @eslint/js @typescript-eslint/parser @typescript-eslint/eslint-plugin eslint-plugin-cypress globals --save-dev ESLint Configuration File:\nimport globals from 'globals'; import pluginJs from '@eslint/js'; import tseslint from '@typescript-eslint/eslint-plugin'; // Import TypeScript plugin import tsParser from '@typescript-eslint/parser'; // Use TypeScript parser import cypressPlugin from 'eslint-plugin-cypress'; // Import Cypress plugin export default [ // Apply to general JavaScript and TypeScript files { files: ['**/*.{js,mjs,cjs,ts,tsx}'], // Apply to all JS and TS files languageOptions: { parser: tsParser, // Use TypeScript parser for TS files globals: { ...globals.browser, // Include browser-specific global variables ...globals.node, // Include Node.js globals JQuery: 'readonly', // Add JQuery global for Cypress files }, }, plugins: { '@typescript-eslint': tseslint, }, rules: { 'quotes': ['error', 'single'], // Enforce single quotes 'semi': ['error', 'always'], // Enforce semicolons '@typescript-eslint/no-unused-vars': 'error', // Disallow unused variables }, }, // Apply specifically to Cypress test files { files: ['**/*.cy.{js,ts,tsx}', 'cypress/**/*.{js,ts,tsx}'], // Target Cypress test files languageOptions: { parser: tsParser, // Use TypeScript parser for Cypress files globals: { ...globals.browser, // Include browser-specific global variables cy: 'readonly', // Explicitly define 'cy' as a global variable Cypress: 'readonly', // Explicitly define 'Cypress' as a global variable ...globals.cypress, // Include Cypress globals (cy, Cypress, etc.) ...globals.mocha, // Include Mocha globals (describe, it, beforeEach, etc.) JQuery: 'readonly', // Add JQuery global for Cypress files }, }, plugins: { '@typescript-eslint': tseslint, 'cypress': cypressPlugin, // Enable Cypress plugin }, rules: { 'quotes': ['error', 'single'], // Enforce single quotes 'semi': ['error', 'always'], // Enforce semicolons '@typescript-eslint/no-unused-vars': 'error', // Disallow unused variables 'cypress/no-unnecessary-waiting': 'error', // Enforce Cypress rule 'cypress/assertion-before-screenshot': 'warn', // Enforce Cypress rule }, }, pluginJs.configs.recommended, // Recommended JavaScript rules { rules: tseslint.configs.recommended.rules, // Include recommended TypeScript rules }, ]; ESLint Commands:\n\u003e npx eslint '**/*.cy.{ts,js,tsx}' \u003e npx eslint '**/*.{ts,js,mjs,tsx}' Adding Eslint to the package.json scripts section:\nThe following must be added to the ‘package.json’ (comments are for doc only, please remove them):\n\"scripts\": { \"lint\": \"eslint '**/*.{js,ts,tsx}'\", // Lint all JS/TS files \"lint:fix\": \"eslint '**/*.{js,ts,tsx}' --fix\", // Fix linting issues \"lint:cypress\": \"eslint 'cypress/**/*.{js,ts,tsx}'\" // Lint only Cypress test files }, The npm command to run it:\n\u003e npm run lint \u003e npm run lint:fix \u003e npm run lint:cypress",
    "description": "Install Cypress with npm Open Cypress from CLI Using TypeScript with Cypress Dependencies installation Configure TypeScript tsconfig.ts File ESLint Installation for Cypress and TypeScript Install Cypress with npm From terminal:\n\u003e npm init -y \u003e npm install cypress --save-dev Expect package.json file to be created an contain the following:\n\u003e cat package.json { \"devDependencies\": { \"cypress\": \"^13.15.0\", }, \"dependencies\": { } } Open Cypress from CLI \u003e npx cypress open Using TypeScript with Cypress Information may be found on Cypress TypeScript documentation page.",
    "tags": [],
    "title": "Cypress with TypeScript",
    "uri": "/notes/cypress/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Data Quality and Data Management 101 What is Data Quality Definition? What is Data Quality Management (DQM) and its pillars? What is the Impact of Poor Data Quality Real-life Examples Causes of Bad Data Data Quality Dimensions Overview Example of Accuracy Issue Example of Completeness Issue Example of Consistency Issue Example of Timelessness Issue Example of Validity Issue Example for Uniqueness Issue Example of Integrity Issues Multiple Data Quality Dimensions Issues Data Quality Rules What? Examples of Data Quality Rules Why? How to define the Data Quality Rules Data Quality Techniques Data Profiling Data Parsing Data Standardization Identify Resolution Data Linkage Data Cleansing Data Enhancement Conclusion *Data Inspection and Monitoring Who? Data Quality Roles Key Data Quality Roles and Their Responsibilities Why Are Data Quality Roles Important? In a Nutshell Data Quality Roles Data Quality Manager Data Analyst Data Engineer Data Owner Data Steward Data Custodian Data Consumer Data Quality Process Data Quality Improvement Process Step 1: Defining Data Quality Improvement Goals Step 2: Data Profiling Step 3: Conducting Data Quality Assessment and Using Root Cause Analysis (RCA) Tools Step 4: Resolving Data Quality Issues Step 5: Monitoring and Controlling Data Quality Tools Why Data Quality Tools Are Important Features to Look for When Selecting a Data Quality Tool Why Choose the Right Tool Early Data Quality Tools Categorization Comparative Table of Data Quality Tools In a Nutshell Data Quality Tools from AWS and Google Cloud Data Governance or Data Quality Management Best Practices Data Quality and Data Management 101 What is Data Quality Definition? In the Data and Business Intelligence domain, Data Quality refers to the overall accuracy, completeness, reliability, and relevance of data, ensuring that it is fit for its intended use In essence, Data Quality ensures that the data used in business intelligence efforts is trustworthy, allowing for accurate analysis, reporting, and decision-making Data quality is defined by how well a given dataset meets a user’s need. Data quality is an important criteria for ensuring that data-driven decisions are made as accurately as possible What is Data Quality Management (DQM) and its pillars? People: the involvement of the data stewards, analyst and business users who are responsible for setting data standards, monitoring quality, and resolving issues. These roles ensure alignment alignment between business needs and the data used to support them. Data Profiling: a critical step that involves analyzing the current state of the data by examining its structure, patterns, and anomalies. Data profiling helps uncover quality issues such as duplicates, missing values, and inconsistencies, enabling organizations to identify areas needing improvement. It is initiated to understand the current state of existing data by comparing data to data standards as set by the DQM, used to define the benchmarks to evaluate the improvements. Defining Data Quality: establishing clear, measurable data quality dimensions such as accuracy, completeness, timeliness and consistency. These criteria are developed based on business needs and objectives, ensuring that data supports decision-making and operations effectively. What the data should look like, and it is based on the business goals Data Reporting: providing regular insights and metrics on the state of data quality through dashboards audits, and scorecards. This reporting enable stakeholders to monitor progress, identify trends, and make informed decisions about improvements and corrective actions. It will return the DQM “return on the investment” (ROI), and how data compares to the defined data quality benchmarks. Data Fixing: implementing corrective actions to resolve data quality issues, including data cleansing, standardization, and deduplication. It also involves root cause analysis to prevent recurring issues by addressing underlying process or system flaws. It is intended to repair the data that doesn’t meet the defined data quality benchmarks and standards. Improving data to the required standards. Most important pillar is probably the People one.\nWhat is the Impact of Poor Data Quality Poor data quality can have significant negative impacts on a business across various dimensions:\ndecision-making, operational efficiency, financial performance/missed opportunities, and customer trust/reputation risk. Key impacts include:\nInaccurate Decision-Making: poor data quality leads to faulty analysis and reporting, causing executives and managers to base strategic and operation decisions on incorrect or incomplete information. This can result in missed opportunities, flawed strategies, or inappropriate response to market conditions. The data based decisions and policies are only as good as the data they are based on. Reduced Operational Efficiency: when data is inaccurate or inconsistent, processes such ad data integration, analysis, and reporting take longer and require more manual intervention. This leads to inefficiencies, increased labor costs, and delayed operations, affecting productivity. Financial Losses: poor data quality can directly impact revenue by causing billing errors, incorrect pricing, or inventory mismanagement. It also increases costs due to rework compliance fines, or failed marking campaigns based on bad data. For example, failure to deliver service to customers or failure to sale the relevant contacts. Customer dissatisfaction: errors in customer data, such as wrong addresses or incorrect orders, can result in poor customer experiences, damaged relationships, and loss of trust. This can lead to increased customer churn and negative brand reputation. Regulatory and Compliance risks:: in industries with strict regulatory requirements (e.g., finance, healthcare, etc), poor data quality can lead to non-compliance, legal penalties, and damage to the organization’s credibility. For example, GDPR issues and negative media coverage. Loss of competitive advantage: businesses that fail to manage data quality effectively may fall behind competitors that leverage clean, accurate data for better market insights, customer targeting, and innovation. In conclusion, poor data quality undermines the reliability and value of business intelligence (BI)., impacting nearly every aspect of the business, from dat-to-day operations to long-term growth and competitiveness.\nReal-life Examples There are several real-life examples where poor data quality had a significant impact on businesses:\nKnight Capital Group: in 2012, a software glitch caused the firm to make unintended stock trades, resulting in a $ 440 million loss within 45 minutes. This error was linked to poor data handling and led to the firm’s eventual bankruptcy. [GetRightData] **Boeing 747 Max Crashes: Faulty sensor data triggered the automated flight control system, contributing to two fatal crashes in 2018 and 2019. Boeing lost billions, and the incidents resulted in the grounding of all 737 Max planes. [GetRightData] UK Passport Agency: in 1999, data migration errors during a system upgrade delayed the insurance of 500K passports. The fallout from these data issues led to public outrage and a cost of around £12.6 millions to resolve. [GetRightData] NASA Mars Orbiter: NASA lost a $125 million Mars orbiter because a Lockheed Martin engineering team used English units of measurement while the agency’s team used the more conventional metric system for a key spacecraft operation. [CNN_NASA] These examples demonstrate how critical accurate data is in preventing financial losses, reputation damage, and even human safety risks.\nMore cases are documented on [GetRightData].\nCauses of Bad Data Bad data can arise due to due to various reasons, typically stemming from issues in data collection, management, and governance. Here are some reasons why we encounter poor data quality:\nHuman Error Manual Data Entry: people can make mistakes when entering data, leading to inaccuracies, misspellings, and incomplete records. For example, typographical errors or incorrect formatting can lead to inconsistencies in databases. Lack of Training: when data entry personnel aren’t adequately trained, they may unintentionally introduce systemic errors. This happens when employees unknowingly enter incomplete or incorrect data, which accumulates and impacts the overall quality of the dataset. Inadequate Data Validation Missing Validation Rules: without proper validation checks, systems can let through invalid data like incorrect formats or data types (for instance, entering letters when numbers are expected). These gaps can lead to faulty or unusable data being stored. Inconsistent Standards: when different departments or systems use varying data formats and standards, it results in inconsistencies across the organization, making it difficult to maintain clean and reliable data. Integration Errors Data from Multiple Sources: merging data from various sources, such as legacy systems or third-party vendors, often introduces discrepancies. Differences in data structures or formats can lead to duplicated records, missing information, or incorrect data being captured. Incorrect Mapping or Transformation: during migrations or updates, if data fields aren’t mapped properly, or if transformation rules are incorrect, it can lead to distorted or corrupted data. Outdated Information Stale Data: information can become outdated if not updated regularly. This is especially common with customer records like contact details, which change frequently over time. Lag in Data Updates: some systems don’t update data in real time, leading to discrepancies between what’s recorded and actual events, which can cause issues in decision-making. Lack of Data Governance No Clear Ownership: when there’s no assigned responsibility for data management, inconsistent practices across departments can result in poor data quality. Without ownership, it’s hard to maintain data standards. Unstructured Data Management: without clear policies and structured processes for managing storing, and cleaning data, its quality tends to degrade, leading to inaccurate or complete data. Incomplete Data Partial Data Entry: sometimes, important data fields are left out during entry, leading to incomplete records. This can happen when systems don’t enforce required fields, making the data unreliable. Legacy Systems: older systems might not capture all necessary data, leaving gaps when this information i used in modern analytics or business intelligence platforms. Technical Errors System Failures: technical issues like system crashes, corrupted files, or failed data transfers can lead to incomplete or inaccurate data. Software Bugs: bugs in data processing tools or software can introduce errors, reading faulty data that affects overall insights and decision-making. In conclusion, each of these factors highlights the need for solid DQM practices, such as robust validation, regular data cleaning, and strong governance to keep data accurate and reliable.\nData Quality Dimensions Overview Data Quality Dimensions refer to the different ways we measure how good our data is.\nflowchart TB DQD(Data Quality Dimensions) \u003c--\u003e Validity DQD \u003c--\u003e Timeliness DQD \u003c--\u003e Completeness DQD \u003c--\u003e Uniqueness DQD \u003c--\u003e Consistency DQD \u003c--\u003e Accuracy DQD \u003c--\u003e Integrity Here’s some of the most recognized dimensions:\nAccuracy: this is all about how closely the data matches real-world facts. If your data doesn’t reflect reality, it can lead to bad decisions. Completeness: it checks if all the necessary information is present. Missing data can leave important gaps, making the data unreliable. Consistency: data should be the same across all systems. For example, if one database says a customer lives in New York and another says they live in California, that’s inconsistent and can cause confusion. Timelessness: how current is the data? Old or delayed information can lead to missed opportunities or wrong conclusions. Validity: this looks at whether the data follows the required rules or formats. For instance, you wouldn’t want letters in a field that should only have numbers. Uniqueness: there shouldn’t be duplicates. If the same person or thing is entered twice, it skews the results and creates inefficiencies. Integrity: this ensures that relationships within the data are properly maintained. For example, if a customer is linked to an order, that link should always stay intact. In summary, these dimensions help ensure that your data is trustworthy and usable for making informed business decisions.\nExample of Accuracy Issue Here’s an example of a data accuracy issue using a customer list:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih 123 Elm Street Springfield 1105 104 Emma Johnson 789 Maple Blvd Capital City 54321 Issue: In this table, Customer ID 103 has multiple accuracy errors:\nFirst Name: “Jonh” should be “John.” Last Name: “Smtih” should be “Smith.” Address: “123 Elm Street” is an incorrect variation of “123 Elm St.” Zip Code: The entry “1105” is missing a digit; it should be “01105.” These inaccuracies could lead to problems like sending mail to the wrong address, duplicate entries for the same person, or errors in customer service interactions.\nThis highlights how inaccurate data can cause operational inefficiencies and poor customer experiences.\nExample of Completeness Issue Here’s an example of a data completeness issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih Springfield 1105 104 Emma 789 Maple Blvd Capital City 54321 Completeness Issues:\nCustomer 103: The Address field is missing, leaving out critical information needed for communications or shipments. Customer 104: The Last Name is missing, making it impossible to fully identify this customer, especially if there are multiple people named “Emma.” Incomplete data like this can cause issues such as failed deliveries, incorrect customer segmentation, or difficulties in contacting the customer.\nExample of Consistency Issue Here’s an example of a data consistency issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih 123 Elm Street Springfield 1105 104 Emma Johnson 789 Maple Blvd Capital City 54321 Consistency Issues:\nCustomer 103: The Address is listed as “123 Elm Street” instead of “123 Elm St,” which is an inconsistency in the format of the same address as Customer 101. Customer 103: The Zip Code is “1105” instead of “01105,” showing inconsistency in the zip code format (missing leading zero). Inconsistent data like this creates confusion and inefficiencies, as different systems may not recognize the same entity when information is presented in various formats. This can lead to duplicate records or incorrect data processing.\nExample of Timelessness Issue Here’s an example of a data timeliness issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code Last Updated 101 John Smith 123 Elm St Springfield 01105 2024-09-01 102 Jane Doe 456 Oak Ave Shelbyville 12345 2024-09-01 103 Jonh Smtih 123 Elm Street Springfield 01105 2021-06-15 104 Emma Johnson 789 Maple Blvd Capital City 54321 2022-03-10 Timeliness Issues:\nCustomer 103: The Last Updated date is from 2021, making this record outdated. Since the address and contact details haven’t been verified or updated in over two years, it’s possible that the data is no longer accurate. Customer 104: The data was last updated in early 2022, which also makes it outdated for a customer who might have changed address or contact information since then. Outdated records like these could lead to problems such as sending communications or deliveries to the wrong address, missing customer preferences, or failing to capture changes that impact business decisions. Timely data updates are crucial to maintaining the reliability of customer information.\nExample of Validity Issue Here’s an example of a data validity issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code Email 101 John Smith 123 Elm St Springfield 01105 john.smith@email.com 102 Jane Doe 456 Oak Ave Shelbyville 12345 jane.doe@email.com 103 Jonh Smtih 123 Elm Street Springfield 1105 jonh.smtih@email..com 104 Emma Johnson 789 Maple Blvd Capital City 54321 emma@invalidemail@com Validity Issues:\nCustomer 103: The email address “jonh.smtih@email..com” contains a formatting error (extra period before “.com”), which is invalid. Customer 104: The email “emma@invalidemail@com” has two “@” symbols, which is not allowed in email formatting, making it invalid. In cases like these, data validity rules (e.g., proper email formats, correct zip code length, or valid data types) must be enforced. Invalid data such as incorrect email formats could result in failed communications and missed customer interactions, highlighting the importance of ensuring data conforms to required standards.\nExample for Uniqueness Issue Here’s an example of a data uniqueness issue using the customer list:\nCustomer ID First Name Last Name Address City Zip Code Email 101 John Smith 123 Elm St Springfield 01105 john.smith@email.com 102 Jane Doe 456 Oak Ave Shelbyville 12345 jane.doe@email.com 103 Jonh Smtih 123 Elm Street Springfield 01105 jonh.smtih@email.com 104 Emma Johnson 789 Maple Blvd Capital City 54321 emma.johnson@email.com 105 John Smith 123 Elm St Springfield 01105 john.smith@email.com Uniqueness Issues:\nCustomer 101 and Customer 105: These entries are duplicates, representing the same individual (John Smith) with identical information (address, email, etc.). However, they have different Customer IDs, which indicates a lack of uniqueness in the dataset. Duplicate records like this can lead to inefficiencies, such as multiple mailings to the same person, errors in customer segmentation, and skewed analytics. Ensuring uniqueness, especially in key fields like Customer ID or email, is crucial to maintaining data integrity.\nExample of Integrity Issues Here’s an example of a data integrity issue using a customer list and an associated order table:\nCustomer Table:\nCustomer ID First Name Last Name Address City Zip Code 101 John Smith 123 Elm St Springfield 01105 102 Jane Doe 456 Oak Ave Shelbyville 12345 103 Jonh Smtih 123 Elm Street Springfield 01105 104 Emma Johnson 789 Maple Blvd Capital City 54321 Order Table:\nOrder ID Customer ID Order Date Order Amount 5001 101 2024-09-01 $150 5002 105 2024-09-02 $200 5003 104 2024-09-03 $250 Integrity Issues:\nCustomer ID in Order Table: In the Order Table, Order 5002 refers to Customer ID 105, which does not exist in the Customer Table. This indicates a referential integrity problem, where a record in the Order Table is pointing to a non-existent customer. Mismatch of Addresses: In the Customer Table, Customer 103 has the same address as Customer 101 but with variations (e.g., “Elm Street” vs. “Elm St.”), which could also indicate an integrity issue in maintaining consistent data relationships. Data integrity ensures that relationships between tables (e.g., customer and order data) are accurate and complete. Integrity issues like these can lead to broken reporting, incorrect data analysis, and operational inefficiencies.\nMultiple Data Quality Dimensions Issues A single issue, like an address problem, can involve multiple data quality dimensions at once. Here’s how:\nAccuracy: if the address is wrong (e.g., “123 Elm St” instead of “321 Elm St”), it’s an accuracy issue. This can cause real problems, like sending packages to the wrong place or communicating with the customer at the wrong address. Completeness: if part of the address is missing, such as no zip code or an incomplete street name (e.g., “Elm St” without the house number), that’s a completeness issue. Without the full details, the address may not be useful, and the business can’t function properly. Integrity: data integrity is about making sure information is consistent across systems. If one system shows “123 Elm St” and another has “456 Oak Ave” for the same person, it’s an integrity problem, as these records don’t match. So, a simple issue like an incorrect or incomplete address can affect accuracy, completeness, and integrity all at the same time, making it crucial to fix the issue from multiple angles to ensure the data is reliable.\nData Quality Rules The Data Quality Rules, also called Data Validation Rules, are guidelines that defines what good data looks like. They specify the conditions data must meet to be accurate, complete, consistent and usable for its purpose.\nThe Data Quality rules are the technical implementation of the Business rules designed to meet stakeholders’ expectations for reliable, consistent and useful data. They ensure that the data supports both business operations and decision-making effectively.\nWhat? The following rules make sure the data stays useful and trustworthy.\nClear and Specific: the rules clearly define what good data looks like, like “every email must included an @ symbol”. Relevant: The rules are based on what matters for your business or how the data will be used. Helpful: The rules make it easy to spot and fix issues when data doesn’t meet the standards. Automatic: The rules can be applied using tools or scripts to regularly check the data. Examples of Data Quality Rules Completeness rules: No important fields should be empty (e.g., every customer must have an email address). Uniqueness rule: No duplicates (e.g., every OrderID must be unique). Accuracy rule: Data should match real-world values (e.g., a zip code must match its city). Consistency rule: Related data should align across systems (e.g., a user status should be the same in all databases). Validity rule: Data should follow rules or formats (e.g., dates must be in YYYY-MM-DD). Timeliness: Data should be up-to-date (e.g., transaction must be processed within 24 hours). Why? These rules help identify errors, maintain trust in the data, and ensure it’s useful for decision-making and compliance. They are checklist to keep the data clean and reliable.\nHow to define the Data Quality Rules The step-by-step process The following is a simplified process to define the Data Quality Rules, step by step.\nUnderstanding the data and its purpose Why is the data is used? Understand its role in business process, reports or decision-making. Who uses it? Identify key stakeholders and their expectations for the data. Identify common issues Review the data for frequent errors (e.g., duplicates, missing fields, invalid formats, etc.). Gather feedback from users about problems they encounter. Define key data quality dimensions Focus on aspects like: Completeness: are all required fields filed? Accuracy: is the data correct and reliable? Consistency: does it match across systems? Timeliness: is it up-to-date? Uniqueness: are there any duplicates? Write rules that address the issues you’ve identified. Make them clear and actionable. E.g.: Customer email not be empty. Order IDs must be unique. Dates must follow the format YYYY-MM-DD. Validate and Test the Rules Apply the rules to sample data to ensure they work as intended. Involve stakeholders to confirm the rules align with the business needs. Automate and monitor Implement the rules in tools like ETL processes, data validation scripts, or monitoring systems. Set up alerts to flag issues when data doesn’t meet the rules. Review and Improve Regularly review the rules to keep them relevant as business needs and data evolve. Incorporate new feedback and insights over time. All the above steps and process ensure that the data quality rules are practical relevant and effective.\nExample This example is using previous section defined table, but some errors were added to the email column. The example will only focus on the email column, but this process should be applied to all columns.\nCustomer ID First Name Last Name Email 101 John Smith john.smith@email.com 102 Jane Doe jane.doe@email.com 103 Jonh Smtih jonh.smtih@email..com 104 Emma Johnson emma@invalidemail@com Bellow is the analysis of the table data using the previously defined step-by-step process.\nUnderstanding the Data and its purpose Why is the data used? The email addresses are used for customer communication, such a sensing invoices, marketing emails, and the account notifications. Who uses it? The key stakeholders include the sales team (for promotions), the customer support team (for communication), and the marketing team (for campaigns). Identify common issues Review the data issues: Invalid email formats (e.g., jonh.smtih@email..com or emma@invalidemail@com). Missing or empty email fields. Duplicate emails, which can cause communication errors. Gather feedback Marketing and customer support report failed email deliveries due to incorrect email formats. Define key Data Quality dimensions Completeness: all customers must have an email address Accuracy: emails must follow the standard email format (e.g., username@domain.com). Uniqueness: email address should not be duplicated across records. Consistency: email format chould match the expected syntax and avoid errors (e.g., no multiple @ or ..). Timeliness: email address should be updated regularly if customers change them. Write rules to address the issues Rule 1: email addresses must not be empty or null. Rule 2: email addresses must follow the pattern username@domain.com. Rule 3: each email must be unique across all customer records. Rule 4: emails should not contain consecutive dots (e.g., ..) or multiple @ symbol. Validate and test the rules Apply to sample data: john.smith@email.com → Pass. jonh.smtih@email..com → Fail (double dot). emma@invalidemail@com → Fail (multiple @). Confirm with marketing and support teams that these rules align with their expectations and address their reported issues. Automate and monitor Implement these rules in a data pipeline or validation tools: ETL: add a validation step (or task) to check email format, uniqueness and completeness. Alerts: setup monitoring to flag invalid or missing emails, during data ingestion. Automation: Use a regex pattern to validate email formats: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$. Deduplicate email addresses with a database query or script. Review and improve Regular reviews: Periodically audit email addresses for new issues (e.g., obsolete domains, or temporary email services, etc). Feedback loop: Collect input from stakeholders about any new challenges, like domain-specific formatting quirks or newly encountered delivery failures. By following this process, the email column remains clean, valid and reliable for all stakeholders and business needs. The following code block in an example of Great Expectations script for the email column data validation according to the defined rules:\nfrom great_expectations.dataset import PandasDataset import pandas as pd # Table transformed into a dataframe, for example purpose only. data = { \"Customer ID\": [101, 102, 103, 104], \"First Name\": [\"John\", \"Jane\", \"Jonh\", \"Emma\"], \"Last Name\": [\"Smith\", \"Doe\", \"Smtih\", \"Johnson\"], \"Email\": [ \"john.smith@email.com\", \"jane.doe@email.com\", \"jonh.smtih@email..com\", \"emma@invalidemail@com\" ] } df = pd.DataFrame(data) dataset = PandasDataset(df) # Define Great Expectations validations # Rule 1: Email must not be null or empty result_not_null = dataset.expect_column_values_to_not_be_null(\"Email\") print(\"Rule 1 - Not Null Result:\", result_not_null) # Rule 2: Email must match the standard email pattern email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\" result_valid_format = dataset.expect_column_values_to_match_regex(\"Email\", email_regex) print(\"Rule 2 - Valid Format Result:\", result_valid_format) # Rule 3: Email addresses must be unique result_unique = dataset.expect_column_values_to_be_unique(\"Email\") print(\"Rule 3 - Unique Result:\", result_unique) The script output should look like:\nRule 1 - Not Null Result: {'success': True, 'result': {'unexpected_count': 0, ...}} Rule 2 - Valid Format Result: {'success': False, 'result': {'unexpected_list': ['jonh.smtih@email..com', 'emma@invalidemail@com'], ...}} Rule 3 - Unique Result: {'success': True, 'result': {'unexpected_count': 0, ...}} This ensure that the email data meets the defined quality rules and flags issues for further review or correction. These expectations can be integrated into a Great Expectations validation suite for the Automation and monitoring step.\nData Quality Techniques Data Profiling The Data Profiling is the process of analyzing and understanding the structure, content, and quality within a dataset. It involves examining the data to uncover insights, patterns, anomalies, and potential issues. It is helping organizations assess its fitness for use in decision making, reporting an other purposes.\nWhat? The following are the main aspects of Data Profiling:\nStructure analysis: identifying the format, schema, and types of data (e.g., columns, data types and constraints). Example: ensuring column expected to hold dates contains valid date formats. Content analysis: examining the actual values in the data to detect patterns, distributions, or outliers. Example: checking the range of values in zip code colum to ensure validity. Quality Assessment: assessing dimensions like completeness, uniqueness, consistency, and accuracy. Example: identifying duplicate records or null values in critical fields. Relationships and dependencies: Evaluating relationships between columns or datasets, such as foreign key dependencies. Example: ensuring Customer ID is one table matches valid entries in a Customer Details table. Why? Why Data Profiling is so important? Because it plays a crucial role in improving data quality by identifying errors, inconsistencies, and gaps that require correction. It support decision-making by ensuring that decisions are based on reliable and accurate data. Additionally, data profiling enhances data integration by preparing datasets for merging or transformation in ETL pipelines. It also facilitates compliance by ensuring that data meets regulatory or business requirements.\nExample Results from a Data Profiling investigation would return this, as an example:\n5% of Email fields are null. 3% of Phone Numbers are in incorrect format. 10% of the customers share duplicate entries. This report or information will help to guide the following actions in order to clean and standardize the data.\nWho? Who would benefit of data profiling? It would benefit to anyone relying on accurate, reliable data. Data Analysts and Scientists use it to prepare clean datasets for analysis or modeling. Data Engineers and Database Administrators leverage it to maintain ETL pipelines and database health. BI teams and Business Stakeholders gain trusts in reports and decisions. Compliance teams use it to ensure regulatory adherence, while the Data Governance teams enforce quality standards. Even Developers benefit by validating data used in applications. In essence, data profiling ensures data quality for all roles across an organization.\nData Parsing Data Parsing is the process of breaking down raw data into smaller, more manageable components to extract meaningful information and organize it for further processing or analysis. Parsing often involves converting unstructured or semi-structured data into a structured format that can be easily understood and utilized by systems or humans.\nWhat? flowchart LR A[Input] --\u003e B[Process] --\u003e C[Output] The following is the flow of the Data Parsing:\nInput: takes raw data (e.g., text files, JSON, XML, logs, or database entries). Process: Analyzes and splits the data into defined parts or fields based on specific rules or patterns. Output: Produces structured data, such as rows and columns in a table, or a well-formed JSON object, Examples The following are real life Data Parsing examples:\nLog Files: parsing a log file to extract timestamps, error codes, and messages form system logs. JSON and XML files: parsing a JSON or XML file to break down hierarchical data into key:value pairs or relational tables. Text: parsing text or string like jon snow, jon.snow@doomain.com, 555-123-4567into separate fields, for example name, email and phone. Why? The Data Parsing is important because it prepares raw data for analysis, storage or precessing. It is enabling systems to handle large dataset efficiently. It also ensure data is standardized, making it consistent and usable across different platform and applications.\nData Standardization The Data Standardization is the process of transforming data into a consistent format to ensure uniformity and compatibility across different systems, datasets, or applications.It involves applying predefined rules to align data structure, naming conventions, units, format and values, making the data easier to integrate, analyze and interpret.\nWhat? flowchart LR A[Raw Data] --\u003e B[Standardized Data] The main aspects of the Data Standardization include:\nConsistent Formats: aligns data format, e.g., dates as YYYY-MM-DD, phone as +1-555-123-4567. Uniform Units: convert measurements to standard unit, e.g., miles to kilometers, Fahrenheit to Celsius, etc. Naming Conventions: ensures uniform naming for fields or categories, e.g., Customer ID instead of CustID or C_ID. Value Alignment: maps synonyms or variations to a single value, e.g., all the values US, USA and United States would be standardized to United States. Examples The following are some examples:\nRaw Data Standardized Data DOB: 12-31-2020 Date of Birth: 2020-12-31 Country: us Country: United States Height: 5 ft 10 in Height: 70 inches Why? The Data Standardization is important because it reduces errors, inconsistencies, and ambiguities. It is improving overall data quality. By ensuring compatibility between systems and datasets, it enables seamless integration and data sharing across platforms. Standardized data is also easier to query and interpret, simplifying analysis and decision-making processes. Additionally, it supports automation by facilitating smooth workflows and efficient processing in automated systems, ensuring reliable and consistent results.\nIdentify Resolution The Identity Resolution is the process of identifying and linking records that refer to the same entity, (e.g. person, customer, or organization) across multiple data sources, even if the data is inconsistent, incomplete, or formatted differently. It involves matching and consolidating information to create a unified and accurate view of the entity.\nWhat? The main aspects if Identify Resolution are:\nData Matching to identify similarities across records, such as names, addresses, or email addresses. Data Linking to connect matching records from different datasets to recognize them as the same entity. Deduplication to remove duplicates records to maintain a single, consistent profile. Contextual Analysis to use contextual clues, such as account activity or purchase history, to resolve ambiguous matches. How? The identity resolution process looks as the following:\nflowchart LR A[Identify] --\u003e B[Connect] --\u003e C[Match] --\u003e D[Validate] --\u003e E[Activate] Where each phase are defined as the following:\nIdentify: channels, platforms and devices. Connect: connect the dots between the different channels, platforms and devices. Match: based on a defined set of attributes (e.g., same household, IP, WiFi network, timing patterns, etc). Validate: validate that it’s the samxe identity. Activate: create a single, data rich profile Example Raw Records Resolved Identity Record 1: John Smith, john.smith@email.com, 123 Elm St. Unified profile: John Smith, Emails: [john.smith@email.com, john.smith123@email.com], Address: 123 Elm Street. Record 2: J. Smith, john.smith123@email.com, 123 Elm Street. Why? The Identity Resolution is important because it provides a unified view of a customers, enabling personalized interactions and improving the overall customer experience. By reducing duplicates and inconsistencies across datasets, ite enhance data quality and ensures that records are accurate and reliable. It also plays a critical role in fraud detection by linking related records to identify suspicious activity. Additionally, identity resolution supports compliance by ensuring accurate reporting for regulatory requirements and enhances analytics and insights, enabling better decision-making and more effective business strategies.\nBenefits of Data Identity Resolution Improved Data Quality: Eliminates duplicates and inconsistencies, ensuring accurate and reliable data.\nExample: Two records for the same customer are merged into one complete profile.\nBetter Customer Experience: Creates a unified customer view, enabling personalized services and consistent interactions.\nExample: A bank can offer tailored financial products based on a customer’s full history.\nContextual Marketing: Helps deliver personalized marketing messages based on the customer’s current context and behavior.\nExample: A retail store sends a discount for a product a customer recently viewed online.\nUnderstanding Customer Networks: Maps relationships between customers (e.g., families or business groups), allowing group-level strategies.\nExample: A telecom provider offers a family plan after linking individual accounts.\nFraud Detection: Links records to detect duplicate or suspicious activity, reducing fraud risks.\nExample: Identifying multiple loan applications by the same person with different details.\nRegulatory Compliance: Ensures accurate reporting for audits and privacy regulations.\nExample: Helps correctly identify and manage personal data under GDPR.\nEmployee Satisfaction: Makes it easier for employees (e.g., support agents) to handle customer requests by providing a single, unified profile instead of multiple versions.\nExample: A support agent quickly resolves an issue by viewing one complete record of a customer’s interactions.\nCost Savings: Reduces storage and processing costs by eliminating redundant data.\nExample: Cleaning up duplicate records in a CRM system saves database space.\nData identity resolution ensures clean, unified data, improving customer engagement, marketing, compliance, and internal operations.\nData Linkage What? Data linkage is the process of connecting related records from different datasets using common identifiers or attributes. It helps create a broader, connected view of data without merging or altering individual records.\nWhy? Combines data from multiple sources for better insights. Enables integration across systems, ensuring consistency in information. Supports analysis by linking different types of data (e.g., customer profiles with transaction history). Example A hospital links patient records from different departments (e.g., lab results and doctor visits) using the patient’s ID to provide a complete medical history.\nDifference Between Data Linkage and Identity Resolution The difference between data linkage and identity resolution lies in their purpose and approach. Data linkage connects related records across different datasets, focusing on establishing relationships while keeping the records separate. In contrast, identity resolution aims to merge duplicate or fragmented records into a single, unified profile of an entity. For example, data linkage might involve linking a customer’s purchase history with their support tickets, while identity resolution would combine multiple variations of that customer’s profile (e.g., with different email addresses) into one complete record.\nData Cleansing What? Data cleansing (or data cleaning) is the process of detecting and fixing errors, inconsistencies, and inaccuracies in a dataset to improve its quality. It involves tasks like removing duplicates, correcting incorrect data, filling in missing values, and ensuring data is in the correct format.\nWhy? Improves Data Quality: Ensures data is accurate, complete, and reliable. Enhances Decision-Making: Clean data leads to better insights and business decisions. Increases Efficiency: Reduces errors in downstream processes like reporting, analysis, and automation. Ensures Compliance: Helps meet regulatory and business standards by maintaining accurate records. Examples of Data Cleansing Tasks Task Example Cleaned Data Removing duplicates Two identical customer records One unique customer record Correcting data errors jonh.smtih@email..com (invalid email) john.smith@email.com Filling missing values Missing City in an address Adding Springfield as the correct city Standardizing formats Dates as 12/31/2020 and 31-12-2020 Standardized to 2020-12-31 How? Step Description Example Identify Data Issues Analyze the dataset to find errors, inconsistencies, duplicates, missing values, or incorrect formats. Check for null fields or duplicate entries. Define Data Quality Rules Establish rules the data must follow to be considered clean and valid. Ensure emails have a valid format. Remove Duplicates Identify and eliminate duplicate records to maintain data uniqueness. Keep only one record for identical customers. Correct Data Errors Fix incorrect or inconsistent values based on defined rules. Correct misspelled names or invalid emails. Fill Missing Values Handle missing data by filling with appropriate values or removing incomplete records. Fill missing City fields with “Unknown”. Standardize Data Formats Ensure consistent formats for dates, phone numbers, and other fields. Convert all dates to YYYY-MM-DD. Delete Irreparable or Irrelevant Data Remove records that cannot be corrected or are no longer needed for analysis. Delete records with missing critical fields. Validate Cleansed Data Recheck the data to confirm all issues are resolved and the data meets quality standards. Use automated scripts to verify accuracy. Document the Process Keep a record of changes and rules applied for future reference. Maintain a log of corrected and deleted records. Conclusion Data cleansing is a crucial step in ensuring that data is accurate, consistent, and ready for analysis or decision-making. Clean data improves business efficiency, reduces errors, and enhances trust in the data being used. It’s a key practice for any organization that relies on data-driven processes.\nData Enhancement What? Data enhancement is the process of enriching existing data by adding more information from internal or external sources. The goal is to improve the data’s value by making it more complete, accurate, or insightful.\nWhy? Improves Data Completeness: Adds missing information to make data more useful. Enhances Decision-Making: Provides deeper insights by supplementing existing data with relevant details. Boosts Customer Understanding: Helps create detailed customer profiles for better targeting and personalization. Supports Business Growth: Enables more accurate analysis, predictions, and tailored services. How? Internal Data Enrichment: Combines data from different internal sources (e.g., linking CRM data with sales data). External Data Enrichment: Integrates data from external sources like third-party providers, social media, or public datasets. Automated Data Enhancement: Uses tools and APIs to automatically fetch additional data (e.g., address verification tools, market data APIs). Manual Data Enhancement: Involves human intervention to review and add data (used when automation isn’t feasible). When? When data is incomplete: To fill in missing fields, such as contact details or geographic information. For customer profiling: To gain a more comprehensive view of customers for personalized marketing. Before analysis: To ensure datasets are rich enough to support accurate insights and reporting. Examples Original Data Enhanced Data John Doe, john.doe@email.com John Doe, john.doe@email.com, 35 years old, Springfield, IL Product ID: 123, Sales: $1,000 Product ID: 123, Sales: $1,000, Category: Electronics Company A, Revenue: $2M Company A, Revenue: $2M, Industry: Healthcare, Employees: 500 Conclusion Data enhancement adds significant value by enriching datasets, enabling better decision-making, improving customer insights, and supporting business growth. It’s a key practice for organizations that want to maximize the potential of their data.\n*Data Inspection and Monitoring What? Data inspection is the process of reviewing and validating data to ensure it is accurate, complete, and consistent before it is used. Data monitoring involves continuously tracking data quality over time to detect issues, such as missing values, errors, or anomalies, in real-time or through scheduled checks.\nWhy? Ensures Data Quality: Regular inspection and monitoring help catch errors early, ensuring data remains accurate and reliable. Prevents Operational Issues: Detecting data anomalies in real-time prevents issues that could disrupt business operations. Supports Compliance: Consistent data quality ensures adherence to regulatory requirements and industry standards. Improves Decision-Making: High-quality data enables better, more reliable decisions across the organization. Builds Trust: Continuous monitoring builds confidence in data, ensuring stakeholders trust the insights derived from it. How? Method Description Example Manual Inspection Reviewing data samples manually to identify errors and inconsistencies. A data analyst inspects customer records for missing or incorrect information. Automated Monitoring Using tools to track data quality in real-time or at scheduled intervals, setting up alerts for anomalies or threshold breaches. An automated system alerts when a specific field exceeds acceptable error rates. Defining Data Quality Metrics Metrics like completeness, accuracy, consistency, and timeliness are defined and tracked. Monitoring whether all required fields are populated and whether data is updated on time. Using Data Quality Tools Tools like Great Expectations, Apache Airflow (with custom checks), or cloud-native monitoring solutions are used to automate and streamline the process. Employing tools to automate checks and alerts for data issues. When? During data ingestion: Ensure data entering the system is clean and valid. Before data analysis: Inspect data to verify it is fit for analysis. In real-time data flows: Continuously monitor streaming or transactional data to catch issues immediately. After data updates or migrations: Validate and monitor data post-changes to prevent issues caused by transformations or moves. Examples Scenario Action Null values detected in a critical field Set up an alert and trigger a review of the affected records. Inconsistent customer IDs across systems Inspect records and synchronize IDs to ensure consistency. Data not updated as expected Monitor timeliness and investigate delays in data pipelines. Duplicate records found in a dataset Inspect duplicates and apply deduplication rules. Who? Role Why They Care Example Data Engineers To ensure smooth data pipeline operations. Monitor ETL jobs for errors or delays. Data Analysts To ensure data used for analysis is accurate and complete. Inspect datasets before generating reports. Data Scientists To ensure clean data for reliable models. Monitor input data to prevent model bias. Business Intelligence Teams To produce reliable reports and dashboards. Set up alerts for missing or anomalous metrics. Compliance and Risk Teams To meet regulatory and audit requirements. Monitor data integrity for legal compliance. Product Managers To maintain accurate data in customer-facing products. Monitor data flows in user apps. Customer Support Teams To ensure accurate and up-to-date customer data. Inspect customer records when resolving issues. Leadership To make informed decisions based on trustworthy data and ensure regulatory compliance. Rely on reports built on consistently monitored data. Conclusion Data inspection and monitoring are essential for ensuring ongoing data quality, preventing operational issues, supporting compliance, and enabling better decision-making. By combining manual inspection with automated monitoring, organizations can maintain high data standards, reduce risks, and build trust in their data-driven processes. It benefits everyone from technical teams to business leaders, ensuring smooth operations and reliable insights.\nData Quality Roles Ensuring high data quality requires multiple roles working together. Each role has specific responsibilities in maintaining, monitoring, and improving the quality of data within an organization.\nKey Data Quality Roles and Their Responsibilities Role Responsibilities Example Tasks Data Owner Responsible for the overall quality, accuracy, and integrity of the data. Define data quality standards, approve data access, and ensure data compliance. Data Custodian Manages the technical environment where data is stored, ensuring security and availability. Maintain data storage systems, ensure backups, and control technical access to data. Data Customer Uses the data for business decisions, analysis, or reporting. Consume data via dashboards or reports and provide feedback on data quality issues. Data Quality Analyst Monitors data quality, identifies issues, and reports on data quality metrics. Perform data profiling, create data quality reports, and recommend corrective actions. Data Engineer Designs and maintains data pipelines, ensuring data is clean, consistent, and ready for analysis. Build ETL processes with data quality checks and implement deduplication or validation rules. Data Quality Manager Oversees data quality initiatives, defines quality standards, and coordinates across teams to maintain data integrity. Develop and enforce data quality frameworks, oversee data cleansing projects, and track quality metrics. Data Steward Ensures data governance policies are followed, maintains data definitions, and manages data access. Approve data quality rules, maintain metadata, and enforce governance policies. Data Scientist Uses data for modeling and insights, ensuring clean data is used to prevent biased results. Validate datasets for machine learning models and collaborate with data engineers on data quality. Business Analyst Ensures data is reliable for decision-making and reports. Validate data in reports and dashboards, ensuring completeness and accuracy. Compliance Officer Ensures data meets regulatory requirements and company policies. Monitor data privacy, check adherence to regulations like GDPR, and audit data handling processes. Product Manager Ensures data used in products meets quality standards for a good user experience. Work with data teams to resolve product data issues and define quality requirements for features. Leadership (C-level, VPs) Sets data quality goals, allocates resources, and ensures the organization prioritizes data quality. Define high-level data quality KPIs and invest in data quality improvement initiatives. Why Are Data Quality Roles Important? Accountability: Assigning roles ensures that everyone knows their responsibility for maintaining data quality. Collaboration: Different roles collaborate to ensure end-to-end data quality—from data collection to analysis. Continuous Improvement: With clear roles, organizations can continuously monitor, improve, and enforce data quality standards. In a Nutshell Data quality is a shared responsibility involving various roles, from technical teams like data engineers and analysts to business roles like product managers and compliance officers. By clearly defining responsibilities, organizations can maintain high data quality, improve decision-making, and ensure compliance with regulations.\nData Quality Roles Data Quality Manager What? A Data Quality Manager is responsible for overseeing data quality initiatives across the organization. They ensure that data is accurate, consistent, complete, and reliable by defining quality standards, implementing processes, and coordinating with various teams.\nIt is important to note that Data Quality Management and the Data Quality Manager are not the same. While data quality management refers to the overarching process of ensuring data is accurate, consistent, and reliable, the Data Quality Manager is a specific role responsible for executing and overseeing these initiatives.\nStrategical Roles Defining Data Quality Standards: Establish clear standards and metrics for data quality, such as accuracy, completeness, timeliness, and consistency. Developing Data Quality Frameworks: Design frameworks and processes for maintaining and improving data quality throughout the data lifecycle. Ensuring Compliance: Ensure that data handling processes meet regulatory and industry standards, such as GDPR and HIPAA. Tactical Roles Leading Data Quality Initiatives: Lead initiatives like data cleansing, data enrichment, and deduplication projects to improve data quality. Collaboration Across Teams: Work closely with data engineers, analysts, data stewards, and business teams to implement data quality processes and resolve issues. Monitoring and Reporting Data Quality: Set up monitoring systems and create regular reports on data quality metrics, providing insights to leadership and stakeholders. Task and Project Examples Task Description Define quality metrics Establish metrics for accuracy, completeness, and timeliness of key datasets. Implement data validation rules Work with data engineers to set up automated validation rules in ETL pipelines. Lead data cleansing projects Oversee efforts to remove duplicates, correct errors, and fill missing values. Create data quality dashboards Develop dashboards to track data quality metrics and provide insights to leadership. Conduct data audits Regularly audit data to ensure it meets established quality standards. Coordinate with compliance teams Ensure data handling processes align with regulatory requirements like GDPR or CCPA. In a Nutshell A Data Quality Manager ensures data quality by leading initiatives, defining standards, and coordinating with various teams. They play both strategic and tactical roles, ensuring compliance and executing data quality improvement projects. This role is vital for maintaining trust in data, preventing data issues, supporting business growth, and ensuring regulatory compliance. Together, data quality management as a framework and the Data Quality Manager as a role ensure that data remains a reliable and valuable asset for the organization.\nData Analyst What? A Data Analyst is responsible for analyzing data to generate insights that help organizations make informed decisions. They work with large datasets, apply statistical techniques, and create visualizations to communicate findings to stakeholders.\nData Analysts play a crucial role in ensuring that decision-makers have access to accurate, timely, and actionable data.\nStrategical Roles Defining Data Requirements: Collaborate with business teams to understand data needs and define the requirements for analysis. Driving Data-Driven Decisions: Provide key insights that inform strategic business decisions and long-term planning. Ensuring Data Integrity: Advocate for data quality by identifying issues and working with data quality managers and engineers to resolve them. Tactical Roles Data Collection and Preparation: Extract, clean, and transform data from various sources to prepare it for analysis. Performing Analysis: Apply statistical and analytical methods to identify patterns, trends, and anomalies in the data. Creating Reports and Dashboards: Build reports and interactive dashboards to present findings in a clear and actionable manner. Collaborating with Teams: Work closely with data engineers, product managers, and other stakeholders to support data needs and improve data processes. Task and Project Examples Task Description Build sales performance dashboards Create dashboards to track key sales metrics and trends over time. Conduct customer segmentation analysis Analyze customer data to group customers based on behavior or demographics for targeted marketing. Identify operational bottlenecks Analyze operational data to find inefficiencies and recommend improvements. Develop financial reports Generate reports that provide insights into financial performance and trends. Support data-driven product development Analyze user data to provide feedback on product features and usage patterns. In a Nutshell A Data Analyst transforms raw data into meaningful insights, enabling organizations to make data-driven decisions. They play both strategic roles (defining requirements and driving decisions) and tactical roles (performing analysis and creating reports). This role is critical for turning data into a valuable asset that drives business growth, improves efficiency, and enhances decision-making. Data Engineer What? A Data Engineer is responsible for designing, building, and maintaining the infrastructure and systems that allow data to be collected, stored, processed, and accessed efficiently. They ensure that data flows smoothly from various sources to data warehouses, lakes, and analytical tools, enabling data analysts and scientists to perform their work effectively.\nData Engineers play a crucial role in ensuring that data is available, reliable, and ready for analysis.\nStrategical Roles Defining Data Architecture: Design scalable data pipelines and storage solutions that support current and future business needs. Ensuring Data Quality and Governance: Work with data quality managers and stewards to implement data quality rules and ensure compliance with governance policies. Optimizing Data Performance: Define strategies for efficient data processing, minimizing latency and maximizing throughput. Tactical Roles Building Data Pipelines: Develop and maintain ETL (Extract, Transform, Load) pipelines that move data from various sources to target destinations. Maintaining Data Infrastructure: Manage databases, data lakes, and cloud-based storage systems to ensure high availability and performance. Implementing Data Validation: Integrate data validation checks in pipelines to catch errors and inconsistencies during data processing. Collaborating with Data Teams: Work closely with data analysts, scientists, and quality managers to understand data requirements and deliver solutions. Task and Project Examples Task Description Develop scalable ETL pipelines Build pipelines to extract data from multiple sources, transform it, and load it into a data warehouse. Implement data partitioning strategies Optimize large datasets by partitioning data for faster querying and processing. Migrate data to cloud platforms Move on-premise data systems to cloud-based solutions for scalability and cost-efficiency. Set up real-time data streaming Create streaming data pipelines for real-time analytics and monitoring. Automate data validation processes Develop automated checks to ensure data accuracy and completeness during ingestion. In a Nutshell A Data Engineer is responsible for building and maintaining the systems that enable efficient data collection, storage, and processing. They play both strategic roles, such as defining data architecture and ensuring data quality, and tactical roles, such as developing pipelines and managing infrastructure. This role is crucial for ensuring that data is accessible, reliable, and ready for use in analysis, decision-making, and product development.\nData Owner What? A Data Owner is responsible for the overall management and governance of a specific set of data assets within an organization. They have the authority to define how data is used, ensure its quality, and control access. Data Owners are typically senior-level stakeholders or business leaders who understand the value of data and its role in achieving business objectives.\nThe Data Owner ensures that data is treated as a valuable asset by setting policies and guidelines for its proper usage.\nStrategical Roles Defining Data Policies and Standards: Establish rules for data usage, access, and quality, ensuring alignment with business goals. Ensuring Data Compliance: Ensure that data practices adhere to regulatory and legal requirements, such as GDPR or HIPAA. Setting Data Access Guidelines: Define who can access data and under what conditions, ensuring proper governance. Tactical Roles Approving Data Changes: Review and approve changes to the structure, content, or use of data within their domain. Collaborating with Data Teams: Work closely with data stewards, analysts, and engineers to implement data policies and resolve issues. Monitoring Data Quality: Oversee data quality metrics and ensure corrective actions are taken when standards are not met. Task and Project Examples Task Description Define data access policies Establish guidelines on who can access specific datasets and under what circumstances. Approve data usage requests Review and approve requests for new data usage or sharing across departments. Ensure regulatory compliance Monitor data practices to ensure they comply with relevant regulations. Lead data governance initiatives Drive initiatives aimed at improving data governance and management across the organization. Resolve data ownership disputes Act as the final authority in resolving conflicts related to data ownership or usage. In a Nutshell A Data Owner is responsible for managing and governing specific data assets within an organization. They play strategic roles by defining policies, ensuring compliance, and setting access guidelines, as well as tactical roles by approving changes, collaborating with data teams, and monitoring data quality. This role is essential for ensuring that data is properly managed, secure, and used effectively to drive business value.\nData Steward What? A Data Steward is responsible for the day-to-day management and oversight of data assets to ensure they meet quality, governance, and compliance standards. Unlike the Data Owner, who defines policies and has decision-making authority, the Data Steward ensures these policies are implemented and followed consistently across the organization.\nThe Data Steward acts as a bridge between technical teams and business stakeholders by ensuring that data is well-defined, properly maintained, and accessible.\nStrategical Roles Enforcing Data Governance Policies: Ensure that the data policies defined by Data Owners are implemented across all relevant datasets. Maintaining Data Consistency: Work towards ensuring consistency in data definitions, formats, and usage across different departments. Supporting Regulatory Compliance: Collaborate with compliance teams to ensure that data handling adheres to legal and regulatory standards. Tactical Roles Monitoring Data Quality: Regularly check data for accuracy, completeness, and consistency, and report issues when they arise. Managing Data Definitions: Maintain a data dictionary or catalog to ensure clear definitions and understanding of data elements. Collaborating with Data Teams: Work with data engineers, analysts, and quality managers to resolve data issues and improve data quality. Handling Data Requests: Assist with requests for data access or clarification by ensuring proper documentation and metadata are available. Task and Project Examples Task Description Maintain the data catalog Ensure that the data catalog is up-to-date with accurate definitions and metadata. Monitor data quality metrics Track key metrics such as completeness, accuracy, and timeliness, and report any anomalies. Resolve data discrepancies Investigate and resolve inconsistencies in data across different systems or departments. Coordinate data governance meetings Facilitate regular meetings with Data Owners and other stakeholders to discuss data policies. Assist in audits and compliance reviews Provide documentation and support for internal or external audits of data practices. Relationship with Data Owner While the Data Owner defines the policies, standards, and access rules for data, the Data Steward ensures these are implemented and adhered to on a daily basis. The Data Steward works under the guidance of the Data Owner and collaborates closely with them to:\nEnsure data policies are applied consistently. Monitor and report on data quality issues. Act as a liaison between technical teams and business users to resolve data-related issues. In summary, the Data Owner provides the strategic direction, while the Data Steward handles the tactical execution.\nIn a Nutshell A Data Steward ensures the proper implementation of data policies and standards on a daily basis. They play strategic roles by enforcing governance policies and supporting compliance, while their tactical roles include monitoring data quality, managing data definitions, and handling requests. The Data Steward works closely with the Data Owner to ensure data is accurate, consistent, and properly governed, making this role essential for maintaining high data quality and adherence to organizational standards.\nData Custodian What? A Data Custodian is responsible for the technical management and safeguarding of data assets. They focus on maintaining the infrastructure that stores and processes data, ensuring its availability, security, and integrity. Unlike the Data Owner, who defines data policies and usage guidelines, the Data Custodian implements these policies by managing the technical environment where data resides.\nData Custodians are typically IT professionals, such as Database Administrators (DBAs), System Administrators, or Cloud Infrastructure Engineers.\nStrategical Roles Ensuring Data Security: Implement security measures to protect data from unauthorized access, breaches, and other threats. Maintaining Data Availability: Ensure that data systems are reliable and available to users when needed. Supporting Data Governance: Work with Data Owners and Data Stewards to enforce governance policies through technical controls. Tactical Roles Managing Data Storage: Oversee databases, data warehouses, and cloud storage solutions to ensure they are properly maintained and performant. Implementing Access Controls: Configure role-based access controls (RBAC) and other mechanisms to ensure that only authorized users can access specific data. Monitoring Data Systems: Continuously monitor data systems for performance, errors, and potential security issues. Backing Up and Restoring Data: Implement and manage backup procedures to ensure data can be recovered in case of failure or loss. Task and Project Examples Task Description Set up role-based access controls (RBAC) Configure access permissions based on user roles to ensure proper data security. Monitor database performance Use monitoring tools to track database performance and resolve any issues affecting availability. Implement data encryption Ensure that data is encrypted both at rest and in transit to protect sensitive information. Manage data backups Develop and maintain backup and recovery procedures to prevent data loss. Assist in data migrations Support data migration projects by ensuring proper handling and integrity of data during transfers. Relationship with Data Owner The Data Owner defines the policies, standards, and guidelines for how data should be managed and accessed. The Data Custodian ensures these policies are enforced through technical means.\nKey differences and relationships include:\nResponsibility: The Data Owner is responsible for defining what should be done, while the Data Custodian is responsible for how it is done. Collaboration: The Data Custodian works closely with the Data Owner to implement access controls, ensure data security, and maintain data availability. Focus Area: Data Owners focus on business requirements and compliance, while Data Custodians focus on technical implementation and operational stability. In a Nutshell A Data Custodian is responsible for the technical management and protection of data assets. They play strategic roles by ensuring data security, availability, and governance support, while their tactical roles include managing data storage, implementing access controls, and handling backups. Data Custodians work closely with Data Owners to enforce policies and ensure data is stored, processed, and accessed securely, making this role essential for maintaining the technical integrity of data systems.\nData Consumer What? A Data Consumer is anyone who uses data to make decisions, gain insights, or create value. They rely on data provided by data engineers, data stewards, and other roles to perform their tasks. Data Consumers can be internal or external to an organization and include a wide range of users with different purposes and expertise levels.\nWho? Data Consumers include:\nBusiness Analysts: Use data to analyze business performance, identify trends, and make recommendations. Data Scientists: Perform complex analyses and build models to solve problems and predict outcomes. Executives and Managers: Use data-driven reports and dashboards to make strategic and operational decisions. Product Teams: Use customer data to improve product features and user experiences. Marketing Teams: Analyze customer behavior data to target campaigns and measure effectiveness. External Partners: Use shared data for collaborative purposes, such as joint ventures or supply chain optimization. End Customers: In some cases, customers may consume data directly through reports, portals, or APIs. Strategical Roles Identifying Key Data Needs: Work with data teams to define what data is necessary for business growth and success. Driving Data-Driven Culture: Promote the use of data in decision-making processes across the organization. Contributing to Data Governance: Provide feedback on data quality, accessibility, and relevance to improve data governance practices. Tactical Roles Requesting Data Access: Submit requests for data access or new datasets to support specific analyses or projects. Interpreting Data and Reports: Analyze the data presented in reports or dashboards to draw actionable insights. Collaborating with Data Teams: Work closely with data engineers, stewards, and analysts to ensure the data meets their needs. Providing Feedback on Data Quality: Report any issues or inconsistencies they encounter during data usage. Task and Project Examples Task Description Analyze sales performance Review sales data to identify high-performing products and regions. Evaluate marketing campaign results Analyze data from a marketing campaign to measure its ROI and effectiveness. Support product development Use customer feedback and usage data to recommend new product features. Prepare executive reports Generate regular reports for executives, summarizing key business metrics. Collaborate on data integration projects Work with data engineers to integrate new data sources into existing systems. In a Nutshell A Data Consumer is anyone who relies on data to perform their role and drive business outcomes. They play strategic roles by identifying key data needs and promoting data-driven decisions, as well as tactical roles by analyzing data, collaborating with data teams, and providing feedback on data quality. This role is essential for turning data into actionable insights that support business growth and operational efficiency.\nData Quality Process Data Quality Improvement Process Ensuring high-quality data is a continuous process that involves identifying issues, setting goals, implementing solutions, and monitoring results. This section will outline the key steps involved in improving data quality, each described in detail with sections covering what, who, when, why, how, examples, and a summary.\nSteps Overview Defining Data Quality Improvement Goals\nEstablish clear, measurable goals for improving data quality. Ensure alignment with business needs and compliance requirements. Data Profiling\nAnalyze data to assess its structure, content, and quality. Identify patterns, anomalies, and areas requiring attention. Conducting Data Quality Assessment and Using Root Cause Analysis (RCA) Tools\nAssess data quality against established standards. Use RCA tools to identify the underlying causes of data quality issues. Resolving Data Quality Issues\nImplement corrective actions to fix identified issues. Ensure solutions are sustainable and prevent recurrence. Monitoring and Controlling\nContinuously track data quality metrics. Establish control mechanisms to maintain high data quality over time. Each step plays a critical role in ensuring that data remains accurate, complete, consistent, and reliable. The following sections will provide a detailed breakdown of each step, including actionable guidance and real-world examples.\ngraph TD A[Defining Data Quality Improvement Goals] --\u003e B[Data Profiling] B --\u003e C[Conducting Data Quality Assessment and RCA Tools] C --\u003e D[Resolving Data Quality Issues] D --\u003e E[Monitoring and Controlling] E --\u003e A Step 1: Defining Data Quality Improvement Goals Defining data quality improvement goals is the first and most crucial step in any data quality initiative. This step involves setting clear, measurable objectives that guide the entire process, ensuring alignment with business needs and regulatory requirements.\nWhat? Defining data quality improvement goals means establishing objectives that address key data issues, improve data reliability, and support business outcomes. These goals help ensure that data quality efforts are focused and measurable.\nWho? Data Owners: Define high-level objectives based on business priorities. Data Quality Managers: Translate business objectives into actionable data quality goals. Data Stewards: Provide insights on operational data issues and assist in setting priorities. Data Consumers: Business users who rely on data for operations, analysis, and decision-making. When? At the start of a data quality initiative. After identifying recurring data issues impacting operations. Before major events like regulatory audits or system migrations. Why? Ensures alignment with business goals and operational needs. Helps prioritize the most critical data quality issues. Provides measurable benchmarks to track improvement. Facilitates stakeholder engagement by demonstrating clear value. How? Identify Key Data Needs: Collaborate with stakeholders to understand critical data requirements. Assess Current Data Quality: Perform initial data profiling to detect major issues. Set SMART Goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound. Example: “Reduce duplicate customer records by 90% within three months.” Define Data Rules: Establish clear data rules that outline acceptable data formats, ranges, and relationships to ensure consistent quality. Example: Define a rule that customer email addresses must follow a valid email format (e.g., user@domain.com). These rules serve as a basis for setting measurable goals and applying automated validation checks during data processing. Align Goals with Business Metrics: Ensure that data quality goals directly support key business metrics. Obtain Data Owner Approval: Ensure that Data Owners review and approve the proposed goals to guarantee alignment with business priorities and governance policies. This step ensures accountability and promotes cross-functional support for the data quality initiative. Document and Communicate: Clearly document goals and share them with relevant teams for alignment. Examples Goal Description Increase data accuracy Ensure key customer data fields have an accuracy rate of 98% or higher. Reduce data duplication Decrease duplicate records by 90% within three months. Improve data completeness Ensure mandatory fields in product data are 100% complete within six months. Enhance data timeliness Reduce the lag in updating sales data from 48 hours to 12 hours. In a Nutshell Defining data quality improvement goals ensures that efforts are aligned with business needs, measurable, and actionable. This step provides a clear roadmap for addressing data quality issues, improving decision-making, and delivering tangible business value.\nStep 2: Data Profiling Data profiling is a critical step in the data quality improvement process. It involves analyzing datasets to understand their structure, content, and quality. By identifying patterns, anomalies, and inconsistencies, data profiling provides the foundation for setting data quality improvement goals and resolving issues.\nWhat? Data profiling is the process of examining data from various sources to gather statistics and summaries about its structure, quality, and content. It helps in identifying data issues such as missing values, duplicates, incorrect formats, and outliers.\nWho? Data Quality Managers: Oversee the data profiling process and interpret results to guide quality improvement initiatives. Data Stewards: Perform the actual profiling by using tools and techniques to analyze datasets. Data Engineers: Support the process by providing access to data and assisting in automated profiling. Data Consumers: Provide feedback on data usability based on profiling results. When? Before setting data quality improvement goals: To understand the current state of data. During data migrations or integrations: To ensure the data being moved or combined meets quality standards. As part of regular data audits: To continuously monitor and maintain data quality. Why? Understand Data Characteristics: Helps in understanding the structure and patterns in data. Identify Data Issues: Detects errors, inconsistencies, and anomalies early in the process. Improve Data Quality: Provides insights that guide data cleansing and enhancement efforts. Support Business Processes: Ensures data reliability for critical operations and decision-making. How? Collect Data Samples: Select representative datasets from various sources. Use Profiling Tools: Leverage data profiling tools (e.g., Talend, Informatica, or SQL-based tools) to generate statistical summaries. Analyze Key Metrics: Focus on metrics such as: Completeness: Percentage of non-missing values. Uniqueness: Number of unique values in key fields. Consistency: Cross-check data across systems for consistency. Accuracy: Compare data against known standards or reference data. Identify Issues: Document anomalies, such as missing values, duplicates, or incorrect formats. Report Findings: Summarize the profiling results and share them with stakeholders for further action. Examples Data Profiling Metric Description Completeness Ensures that mandatory fields are filled in and calculates the percentage of missing values. Uniqueness Measures the number of distinct values in a dataset, helping to detect duplicates. Pattern Analysis Identifies whether fields follow expected formats, such as date or email patterns. Distribution Analysis Analyzes the distribution of values to detect outliers or anomalies. In a Nutshell Data profiling is essential for understanding the current state of data and identifying issues that impact its quality. By collecting and analyzing key metrics, organizations can detect and address data problems early, ensuring data is reliable and ready for use. This step lays the groundwork for effective data quality management and subsequent improvement efforts.\nStep 3: Conducting Data Quality Assessment and Using Root Cause Analysis (RCA) Tools Conducting a data quality assessment and applying root cause analysis tools is a crucial step in the data quality improvement process. This step helps in evaluating the data against predefined standards and identifying the underlying causes of any issues detected during data profiling.\nWhat? Data Quality Assessment: The process of evaluating datasets to determine whether they meet established data quality standards, such as accuracy, completeness, consistency, and timeliness. Root Cause Analysis (RCA): A systematic approach to identifying the root causes of data quality issues and determining how to address them effectively. Who? Data Quality Managers: Lead the assessment process and ensure findings are documented. Data Stewards: Perform assessments and collaborate on identifying root causes. Data Engineers: Assist in diagnosing technical issues related to data pipelines, storage, and integration. Data Consumers: Provide feedback on the impact of data issues on business processes. When? After Data Profiling: Once profiling has identified potential issues. Before Data Quality Goals Refinement: To ensure goals are based on a thorough understanding of the issues. During Regular Audits: As part of ongoing data governance and quality monitoring. Why? Understand Data Issues: Gain a deeper understanding of the issues affecting data quality. Prioritize Actions: Focus on the most critical issues that have the highest impact on business operations. Prevent Recurrence: Identifying root causes ensures that corrective actions address the underlying problem, preventing similar issues in the future. Improve Data Reliability: Enhances trust in data by ensuring issues are resolved effectively. How? Define Assessment Criteria: Establish clear criteria for evaluating data quality, such as accuracy thresholds and completeness rates. Conduct Data Quality Assessment: Use tools to compare actual data quality against the defined criteria. Example Tools: Informatica, Talend, Great Expectations, or custom SQL scripts. Analyze Findings: Document all identified issues, including their severity and potential impact on business processes. Apply RCA Tools: 5 Whys Method: Ask “Why?” repeatedly to trace the issue back to its root cause. Fishbone Diagram: Visualize the possible causes of a problem to identify the primary root cause. Pareto Analysis: Focus on the issues that contribute most to the problem (the 80/20 rule). Document Root Causes: Clearly document each root cause and associated findings. Collaborate on Solutions: Work with data owners, engineers, and stewards to develop corrective actions. Examples Issue Root Cause Corrective Action Duplicate customer records Lack of unique constraints in the database Implement unique constraints and deduplication processes. Missing product descriptions Incomplete data entry during onboarding Enhance data entry processes and provide training to staff. Inconsistent sales data Data integration issues across systems Standardize data formats and improve integration processes. RCA Examples 5 Whys Example Issue: Duplicate customer records in the CRM system.\nWhy? Duplicate records were entered manually. Why? There was no automated check for existing customers. Why? The data entry form did not include a lookup feature. Why? The CRM system was not integrated with the master database. Why? Integration was not prioritized during implementation. Root Cause: Lack of integration between CRM and master database. Fishbone Diagram Example Issue: Missing product descriptions in the product catalog. Categories:\nProcess: No standardized process for data entry. People: Insufficient training for data entry staff. Technology: Lack of validation checks in the system. Environment: High-pressure onboarding deadlines causing rushed entries. Root Cause: Combination of inadequate process and insufficient training. Pareto Analysis Example Issue: Data inconsistencies across systems. Analysis: 80% of inconsistencies were caused by 20% of the data sources. Root Cause: A few critical systems were not following the standard data format. Action: Focus efforts on aligning the data formats of the critical systems first. |\nIn a Nutshell Conducting a data quality assessment and applying root cause analysis tools is essential for identifying and addressing the true causes of data issues. By thoroughly evaluating data and diagnosing root causes, organizations can implement targeted solutions that improve data reliability and prevent future problems. This step ensures a structured approach to resolving data quality issues and lays the groundwork for long-term data quality management.\nStep 4: Resolving Data Quality Issues Resolving data quality issues involves implementing corrective actions based on the findings from data quality assessments and root cause analysis. This step ensures that identified issues are fixed and that measures are in place to prevent future occurrences.\nWhat? Resolving data quality issues means taking actionable steps to correct data problems, improve data reliability, and maintain data integrity. This includes fixing errors, enhancing processes, and ensuring sustainable improvements.\nWho? Data Quality Managers: Oversee the resolution process and ensure proper documentation. Data Engineers: Implement technical solutions such as updates to ETL pipelines or database constraints. Software Development Teams: Play a role in implementing corrective actions, such as adding mandatory fields or setting form field masks in applications. Data Stewards: Validate data corrections and ensure compliance with data governance policies. Training and Formation Teams: Contribute by introducing mandatory training sessions on Learning Management Systems (LMS) to prevent recurring issues caused by lack of knowledge. Data Owners: Approve major changes and ensure alignment with business needs. When? Immediately after identifying critical issues: To prevent operational disruptions or compliance risks. During scheduled maintenance windows: For changes requiring downtime or significant system updates. As part of ongoing data quality improvement initiatives: To continuously enhance data quality over time. Why? Ensure Data Accuracy: Correcting errors ensures that business decisions are based on accurate information. Maintain Compliance: Resolving issues helps meet regulatory requirements and avoid penalties. Improve Efficiency: Clean, high-quality data reduces the time spent on manual corrections and rework. Enhance Trust in Data: Consistently resolving issues builds confidence among data consumers. How? Step Description How Prioritize Issues Classify issues based on severity and business impact. Prioritize fixing duplicate customer records over minor formatting inconsistencies. Develop Corrective Actions Design solutions for each issue. Implement data deduplication scripts to remove duplicate records. Implement Solutions Apply the corrective actions using appropriate tools and techniques. Some solutions, like adding form field masks or mandatory fields, may require collaboration with software developers. Use SQL scripts, ETL updates, or tools like Talend or Informatica. Validate Corrections Ensure that the implemented solutions have resolved the issues without introducing new problems. Data Stewards review corrected data and run validation checks. Document Changes Record all changes made, including the issue, solution, and results. This ensures traceability and supports future audits. Maintain logs of updates and provide documentation for audits. Prevent Recurrence Implement preventive measures such as automated validation rules or process improvements. Add mandatory fields and validation rules during data entry to prevent incomplete records. Incorporate Training Work with training teams to introduce mandatory learning modules that address recurring knowledge gaps. Use LMS to ensure data entry staff are trained on new validation rules. Examples Issue Corrective Action Preventive Measure Duplicate customer records Implement deduplication script Add unique constraints and lookup checks during data entry. Missing product descriptions Enhance data entry process Introduce mandatory fields for product descriptions. Inconsistent sales data Standardize data formats Implement format validation in ETL pipelines. In a Nutshell Resolving data quality issues is a critical step in ensuring data accuracy, reliability, and usability. By prioritizing issues, implementing corrective actions, and preventing recurrence, organizations can maintain high data quality and improve overall operational efficiency. Proper documentation, validation, and training ensure that resolutions are sustainable and auditable, supporting long-term data quality management.\nStep 5: Monitoring and Controlling Monitoring and controlling data quality is the final step in the data quality improvement process. It involves continuously tracking data quality metrics, identifying new issues, and ensuring that corrective actions remain effective over time. This step ensures that data quality standards are maintained as part of ongoing operations.\nWhat? Monitoring and controlling data quality means implementing systems and processes to track data quality metrics, detect issues proactively, and enforce data governance policies. It ensures data remains accurate, consistent, and reliable.\nWho? Data Quality Managers: Oversee monitoring processes and ensure alignment with organizational standards. Data Stewards: Conduct regular checks, analyze metrics, and report issues. Data Engineers: Maintain and enhance automated monitoring tools. Data Owners: Approve changes to monitoring strategies and tools. Business Stakeholders: Provide feedback on the impact of data quality issues on operations. When? Continuously: As part of routine operations to maintain data quality. After Implementing Corrective Actions: To ensure the effectiveness of resolutions. During Major Data Changes: Such as migrations, integrations, or system updates. Why? Ensure Sustained Quality: Maintains the reliability of data over time. Proactively Detect Issues: Identifies potential problems before they impact operations. Support Compliance: Ensures ongoing adherence to regulatory requirements. Enhance Decision-Making: Provides stakeholders with confidence in the data used for decisions. How? Step Description How Define Metrics Identify key data quality metrics such as accuracy, completeness, and timeliness. Track the percentage of missing values in critical fields. Automate Monitoring Set up automated tools to track data quality and alert teams to anomalies. Use tools like Great Expectations or custom scripts for validation and alerts. Perform Regular Audits Schedule routine checks to validate data quality across systems. Conduct quarterly audits to ensure compliance with data governance standards. Establish Alerts Configure alerts for predefined thresholds to flag potential issues. Notify teams if data accuracy falls below 95%. Collaborate with Teams Work with data engineers and business teams to address recurring issues. Partner with data engineers to adjust ETL processes causing inconsistencies. Review and Update Policies Periodically revisit data quality rules and governance policies to ensure relevance. Update validation rules to reflect new regulatory requirements. Examples Metric Monitoring Tool Action When Issue Detected Completeness SQL-based dashboards Alert data stewards to investigate and fill missing values in real-time. Accuracy Great Expectations scripts Notify data engineers to correct incorrect records identified during validation. Timeliness Workflow monitoring tools Trigger alerts when data ingestion pipelines exceed acceptable latency thresholds. In a Nutshell Monitoring and controlling data quality ensures that high standards are consistently maintained. By defining metrics, automating monitoring, and responding proactively to issues, organizations can sustain data reliability and compliance. This step transforms data quality from a one-time initiative into a continuous, manageable process that supports long-term operational excellence.\nData Quality Tools Why Data Quality Tools Are Important Data quality tools are essential for organizations to maintain and improve the quality of their data. These tools automate critical tasks, enhance operational efficiency, and ensure compliance with regulatory requirements. Here’s a detailed look at why they are indispensable:\nEnsuring Accuracy and Reliability Automate the detection and correction of issues such as duplicates, missing values, and inconsistencies. Improve the accuracy of datasets, ensuring reliable insights for decision-making. Enhancing Efficiency Reduce manual efforts involved in data profiling, cleansing, and validation. Speed up processes, freeing teams to focus on strategic tasks. Supporting Compliance Ensure adherence to regulatory requirements such as GDPR and HIPAA. Provide comprehensive audit trails and documentation to meet compliance standards. Enabling Scalability Handle large datasets and complex operations seamlessly. Adapt to growing organizational data needs without significant manual intervention. Tasks Data Quality Tools Can Perform Data quality tools offer a wide range of functionalities to ensure data integrity and reliability. Key tasks include:\nData Profiling: Analyze datasets to assess their structure, content, and quality. Data Cleansing: Identify and correct errors such as duplicates, inconsistencies, and missing values. Data Validation: Apply rules to ensure data meets predefined quality standards. Data Deduplication: Merge duplicate records to ensure uniqueness in datasets. Data Enrichment: Augment datasets by adding missing or supplementary information from external sources. Monitoring and Alerts: Continuously track data quality metrics and alert teams to anomalies. Compliance Reporting: Generate reports to demonstrate adherence to regulatory requirements such as GDPR or HIPAA. Reconciliation: Compare and align data across multiple systems to maintain consistency. Data Quality Tools vs. Excel Spreadsheets While Excel spreadsheets are widely used for data management, they often fall short when it comes to handling complex and large-scale data quality requirements. Data quality tools, on the other hand, are purpose-built for automating and streamlining data quality tasks, making them more suitable for organizations aiming to maintain high data standards.\nAspect Data Quality Tools Excel Spreadsheets Scalability Handles large-scale data and complex operations effectively. Struggles with large datasets and lacks automation capabilities. Automation Provides automated processes for cleansing, validation, and monitoring. Relies heavily on manual operations, increasing the risk of errors. Accuracy Reduces human error through built-in validation rules and workflows. Prone to errors due to manual data entry and handling. Collaboration Offers features like role-based access and version control for team collaboration. Limited collaboration features; prone to versioning issues. Auditability Provides robust logging and reporting essential for compliance. Difficult to achieve comprehensive audit trails. Example Scenarios Scenario How Tools Help Customer data duplication Deduplication tools identify and merge duplicate customer records automatically. Missing values in product data Data imputation features fill gaps using logical or statistical methods. Regulatory reporting Compliance tools ensure data formats and documentation meet legal requirements. Cross-system inconsistencies Reconciliation features compare and align data across systems. In a Nutshell Data quality tools are indispensable for automating and streamlining data management tasks. They ensure accuracy, compliance, and scalability, making them far superior to manual methods like Excel spreadsheets. By leveraging these tools, organizations can maintain high data quality and drive better business outcomes.\nFeatures to Look for When Selecting a Data Quality Tool Selecting the right data quality tool is critical for ensuring that your organization’s data management needs are met effectively. Below is a table summarizing the key features to consider:\nFeature Why It Matters Key Features Data Profiling Capabilities Understanding the structure, patterns, and anomalies in your data. Automated profiling, summary statistics, detection of missing values and outliers. Data Cleansing and Transformation Correcting errors and standardizing data for consistency and usability. Automated error detection, format standardization, advanced transformation options. Data Validation Ensuring data meets predefined quality standards before use. Rule-based validation, real-time validation, prebuilt templates for common use cases. Data Deduplication and Matching Removing redundancy and ensuring unique records. Fuzzy matching, customizable deduplication thresholds, merge functionality. Scalability and Performance Handling growing data volumes and complexity effectively. High-speed processing, support for distributed environments, scalability. Integration Capabilities Ensuring tools work seamlessly with existing systems. Connectivity with databases and APIs, support for ETL pipelines, compatibility with BI tools. Monitoring and Reporting Continuously tracking data quality to address issues promptly. Real-time monitoring, customizable dashboards, audit trails for compliance. Compliance Support Ensuring adherence to industry regulations and standards. Prebuilt compliance templates, data masking, encryption, comprehensive audit reports. Usability and Collaboration Promoting adoption across teams with an intuitive interface. Low-code options, role-based access control, multi-user support with real-time changes. Vendor Support and Documentation Ensuring smooth implementation and ongoing success. Comprehensive guides, responsive customer support, regular updates and enhancements. Why Choose the Right Tool Early Choosing the right data quality tool as early as possible is crucial for the following reasons:\nAvoid Costly Rework Implementing the wrong tool can result in wasted resources, as systems may need to be rebuilt or replaced later. Correcting initial poor decisions increases project costs and delays. Ensure Scalability Early selection of a scalable tool ensures that the system can grow alongside your organization’s needs. A non-scalable tool may lead to limitations as data volume and complexity increase. Streamline Implementation Choosing a tool early allows teams to align processes, training, and workflows around the selected system. Delays in selection can create inefficiencies as teams work without a consistent framework. Support Stakeholder Alignment Early decisions foster collaboration by ensuring all stakeholders agree on the tools and processes. Late-stage changes can disrupt workflows and reduce buy-in. Avoid Data Quality Debt Starting with the right tool minimizes the accumulation of data quality issues that are harder to resolve later. Proactive decisions lead to long-term improvements in data integrity. Examples of Early Benefits Scenario Impact of Choosing the Right Tool Early Scalability Needs Identified Early Selecting a scalable tool prevents future disruptions as data volumes grow. Integration with BI Tools Ensures smooth workflows and analytics from the start. Compliance Requirements Early adherence to compliance standards avoids legal risks and penalties. In a Nutshell When selecting a data quality tool, focus on features that align with your organizational needs, including profiling, cleansing, validation, scalability, and compliance. A well-chosen tool not only improves data quality but also enhances operational efficiency and decision-making, ensuring long-term value for your organization.\nData Quality Tools Categorization Leaders SAP LeanIX Ardoq Orbus Software Challengers ValueBlue Visionaries Capsifi Bizzdesign MEGA Niche Players Bee360 North Highland UNICOM Systems Comparative Table of Data Quality Tools Tool Description Cost Range Pros Cons Key Features SAP LeanIX Enterprise architecture and data quality management tool. High (Enterprise-level) Comprehensive features; strong integrations. High cost; steep learning curve. Supports large-scale enterprises; regulatory compliance tools. Ardoq Tool for collaborative enterprise architecture modeling. Medium to High User-friendly; great for team collaboration. Limited advanced data profiling capabilities. Collaboration tools; process visualization. Orbus Software Focused on business process and architecture alignment. Medium Good for aligning data and business processes. Limited scalability for very large datasets. Business process alignment; customizable workflows. ValueBlue Business process and architecture integration tool. Medium Excellent business alignment; budget-friendly. Lacks robust data quality cleansing features. Lightweight and fast; process improvement focus. Capsifi Strategic planning and data alignment tool. Medium Strong vision alignment tools. Less mature in execution tools. Strategic alignment; roadmap generation. Bizzdesign Comprehensive enterprise architecture tool. Medium to High Great for vision planning and execution monitoring. Complex for beginners. Visionary planning; model integration tools. MEGA Data governance and compliance-focused tool. High Excellent for compliance and regulatory needs. Higher cost; limited flexibility for smaller setups. Data governance; compliance tracking. Bee360 Business workflow management with niche features. Medium Tailored for specific industry processes. Limited general-purpose data tools. Industry-specific workflows; lightweight design. North Highland Consultancy-oriented tool for niche solutions. Variable Customizable for unique business needs. Dependent on consultancy engagement. Industry-specific customization. UNICOM Systems Business and IT integration tools with data alignment. Medium Good integration capabilities. Limited scalability. Business and IT alignment tools. In a Nutshell This table highlights the key characteristics of various data quality tools categorized into Leaders, Challengers, Visionaries, and Niche Players. Choosing the right tool depends on your organizational needs, budget, and specific use cases. Leaders offer robust solutions for large enterprises, while Niche Players cater to specialized needs.\nData Quality Tools from AWS and Google Cloud Both AWS and Google Cloud offer robust data quality tools as part of their cloud ecosystems. These tools provide features for data profiling, cleansing, validation, and monitoring, making them suitable for organizations looking to ensure high data quality in their cloud environments.\nAWS Data Quality Tools AWS Glue DataBrew\nDescription: A visual data preparation tool for cleaning and normalizing data without writing code. Key Features: Data profiling, cleaning, transformation, and validation rules. Cost Range: Pay-as-you-go, based on data processed. Pros: Fully integrated with AWS services; user-friendly; no coding required. Cons: Limited to AWS ecosystems; less flexible for external integrations. Amazon Redshift Data Quality\nDescription: Offers data quality checks and monitoring for datasets stored in Amazon Redshift. Key Features: Built-in quality checks, anomaly detection, and automated alerts. Cost Range: Included with Redshift usage. Pros: Seamless integration with Redshift; automated monitoring. Cons: Limited to Redshift-based data. Google Cloud Data Quality Tools Cloud Dataprep (Trifacta)\nDescription: A data preparation tool for cleaning, enriching, and transforming data. Key Features: Data profiling, cleansing, and enrichment; strong collaboration features. Cost Range: Pay-as-you-go pricing model based on data usage. Pros: Easy to use; integrates with Google Cloud services like BigQuery and Dataflow. Cons: Limited to Google Cloud; can be costly for large datasets. BigQuery Data Quality and Monitoring\nDescription: Data quality solutions built into BigQuery for ensuring clean and reliable data. Key Features: Data validation, monitoring, and rule-based anomaly detection. Cost Range: Included in BigQuery pricing (pay-as-you-go). Pros: Integrated with BigQuery; highly scalable for large datasets. Cons: Limited functionality for external systems. Comparison Table Tool Cloud Provider Description Cost Range Pros Cons AWS Glue DataBrew AWS Visual data preparation and profiling. Pay-as-you-go Integrated with AWS services; user-friendly. Limited to AWS ecosystem. Amazon Redshift Data Quality AWS Data quality checks within Redshift. Included Automated monitoring; Redshift integration. Limited to Redshift datasets. Cloud Dataprep (Trifacta) Google Cloud Data preparation and transformation tool. Pay-as-you-go Strong collaboration; easy to use. Limited to Google Cloud; high costs. BigQuery Data Quality Google Cloud Built-in data quality for BigQuery datasets. Included Scalable; tightly integrated with BigQuery. Limited to BigQuery datasets. In a Nutshell AWS and Google Cloud offer powerful tools for managing data quality in cloud environments. While AWS Glue DataBrew and Amazon Redshift Data Quality provide strong options within AWS, Google Cloud’s Dataprep and BigQuery Data Quality solutions excel in collaboration and scalability. The choice of tool depends on your organization’s cloud platform, budget, and specific data quality requirements.\nData Governance or Data Quality Management Best Practices",
    "description": "Data Quality and Data Management 101 What is Data Quality Definition? What is Data Quality Management (DQM) and its pillars? What is the Impact of Poor Data Quality Real-life Examples Causes of Bad Data Data Quality Dimensions Overview Example of Accuracy Issue Example of Completeness Issue Example of Consistency Issue Example of Timelessness Issue Example of Validity Issue Example for Uniqueness Issue Example of Integrity Issues Multiple Data Quality Dimensions Issues Data Quality Rules What? Examples of Data Quality Rules Why? How to define the Data Quality Rules Data Quality Techniques Data Profiling Data Parsing Data Standardization Identify Resolution Data Linkage Data Cleansing Data Enhancement Conclusion *Data Inspection and Monitoring Who? Data Quality Roles Key Data Quality Roles and Their Responsibilities Why Are Data Quality Roles Important? In a Nutshell Data Quality Roles Data Quality Manager Data Analyst Data Engineer Data Owner Data Steward Data Custodian Data Consumer Data Quality Process Data Quality Improvement Process Step 1: Defining Data Quality Improvement Goals Step 2: Data Profiling Step 3: Conducting Data Quality Assessment and Using Root Cause Analysis (RCA) Tools Step 4: Resolving Data Quality Issues Step 5: Monitoring and Controlling Data Quality Tools Why Data Quality Tools Are Important Features to Look for When Selecting a Data Quality Tool Why Choose the Right Tool Early Data Quality Tools Categorization Comparative Table of Data Quality Tools In a Nutshell Data Quality Tools from AWS and Google Cloud Data Governance or Data Quality Management Best Practices Data Quality and Data Management 101 What is Data Quality Definition? In the Data and Business Intelligence domain, Data Quality refers to the overall accuracy, completeness, reliability, and relevance of data, ensuring that it is fit for its intended use In essence, Data Quality ensures that the data used in business intelligence efforts is trustworthy, allowing for accurate analysis, reporting, and decision-making Data quality is defined by how well a given dataset meets a user’s need. Data quality is an important criteria for ensuring that data-driven decisions are made as accurately as possible What is Data Quality Management (DQM) and its pillars? People: the involvement of the data stewards, analyst and business users who are responsible for setting data standards, monitoring quality, and resolving issues. These roles ensure alignment alignment between business needs and the data used to support them. Data Profiling: a critical step that involves analyzing the current state of the data by examining its structure, patterns, and anomalies. Data profiling helps uncover quality issues such as duplicates, missing values, and inconsistencies, enabling organizations to identify areas needing improvement. It is initiated to understand the current state of existing data by comparing data to data standards as set by the DQM, used to define the benchmarks to evaluate the improvements. Defining Data Quality: establishing clear, measurable data quality dimensions such as accuracy, completeness, timeliness and consistency. These criteria are developed based on business needs and objectives, ensuring that data supports decision-making and operations effectively. What the data should look like, and it is based on the business goals Data Reporting: providing regular insights and metrics on the state of data quality through dashboards audits, and scorecards. This reporting enable stakeholders to monitor progress, identify trends, and make informed decisions about improvements and corrective actions. It will return the DQM “return on the investment” (ROI), and how data compares to the defined data quality benchmarks. Data Fixing: implementing corrective actions to resolve data quality issues, including data cleansing, standardization, and deduplication. It also involves root cause analysis to prevent recurring issues by addressing underlying process or system flaws. It is intended to repair the data that doesn’t meet the defined data quality benchmarks and standards. Improving data to the required standards. Most important pillar is probably the People one.",
    "tags": [],
    "title": "Data Quality",
    "uri": "/notes/data-quality/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "Design for Testability Background Applying Design for Testability to Software Development Test Strategy Requirements 1. Continuous Testing Across CI/CD (Shift-Left \u0026 Shift-Right) 2. Repeatability \u0026 Reliability 3. Comprehensive Logging \u0026 Reporting 4. Self-Cleaning Tests 5. No Testing Limitations 6. Decentralized Testing Responsibility Infrastructure and Configuration for Testability 1. Testing in Any Environment (Including Production, If Needed) 2. Self-Cleaning Tests 3. Domain-Isolated Testing 4. Multi-Interface Testing 5. Feature Flag (FF) Control 6. Capturing System Metrics 7. Production-Representative Test Environments Software Development for Testability 1. CRUD API Support 2. Database Management for Testing 3. Feature Flag (FF) Control 4. Time Manipulation for Testing In a Nutshell Background “Design for Testability” is a principle I frequently use, and it comes from my early experience in electrical engineering. In hardware development, especially when designing Printed Circuit Boards (PCBs) and Printed Circuit Board Assemblies (PCBAs), testing isn’t just a step—it’s a necessity.\nWhy? Because once hardware is built, retrofitting to fix a problem is costly and complex. To mitigate this, engineers integrate testability into the design process itself:\nVias – These small drilled holes connect metal layers in a PCB, allowing electrical signals and power to travel between them. They also provide test points for validating functionality. Fixtures (or jigs) – These tools leverage vias and other test points to systematically assess the board’s electrical characteristics. The more test points available, the higher the test coverage, making troubleshooting easier and more efficient. By designing with testability in mind, engineers ensure that defects can be identified and fixed before mass production. Instead of scrapping faulty PCBs, they can be repaired, saving time, resources, and money.\nApplying Design for Testability to Software Development In software development and the Software Development Life Cycle (SDLC), design follows the planning and requirements phase. Just like in hardware engineering, testability should be a core consideration from the start.\nTest planning and test requirements should be integrated into the design phase, not treated as an afterthought. A well-designed system simplifies development, testing, deployment, and maintenance, ensuring that later stages don’t suffer from poor testability. For software to be truly testable, every aspect—code, configuration, infrastructure, and tests—should be built with testability as a guiding principle. This leads to:\nEasier debugging and troubleshooting More efficient automation Faster feedback loops Reduced long-term maintenance costs By designing for testability from the start, teams create more resilient, maintainable, and high-quality software.\nTest Strategy Requirements A robust test strategy ensures that testing is seamless, repeatable, and reliable across the entire CI/CD pipeline, without disrupting production. To achieve this, the strategy must support:\n1. Continuous Testing Across CI/CD (Shift-Left \u0026 Shift-Right) Tests should be triggered at any stage of the pipeline, from early development (shift-left) to post-deployment monitoring (shift-right). Testing must not interfere with production stability. 2. Repeatability \u0026 Reliability Tests should be designed to run multiple times without side effects. Execution should be consistent and automated, ensuring results are reliable across different runs. 3. Comprehensive Logging \u0026 Reporting Test results must be logged and published in relevant platforms such as dashboards, test management tools, Slack notifications, and databases. Visibility into test outcomes ensures faster debugging and better decision-making. 4. Self-Cleaning Tests Each test should clean up after itself, preventing test data pollution and flaky results. 5. No Testing Limitations Test strategy should support all types of testing, including:\nParallel execution for speed and efficiency. Multi-version testing, both independently and simultaneously. Load testing at different scales, ensuring no impact on production. Feature-flag testing, allowing validation with and without specific features enabled. Concurrent test execution, with minimal interference between tests. 6. Decentralized Testing Responsibility Teams own their domain, services, and dependencies, ensuring accountability and autonomy. Failing tests in one domain should not block other teams from deploying or running their own tests. Contract Testing is used to verify service integration while maintaining isolation between teams. By following these principles, teams can build a scalable, resilient, and efficient test strategy that supports both rapid development and production stability.\nInfrastructure and Configuration for Testability A test-ready infrastructure ensures that testing is flexible, reliable, and reflective of real-world conditions. To support this, the infrastructure and configuration must enable:\n1. Testing in Any Environment (Including Production, If Needed) Testing in production should be a strategic choice, not a risk-driven fear. The system should support safe, controlled production testing when required. 2. Self-Cleaning Tests Tests must manage their own data cleanup, ensuring they don’t leave residual data. Infrastructure should support terminating test pods or sessions after execution. 3. Domain-Isolated Testing Tests should run without interfering with other domains, ensuring controlled validation and minimal cross-impact. 4. Multi-Interface Testing The ability to test through different interfaces, including: User Interface (UI) APIs Messaging Streams (e.g., Kafka) 5. Feature Flag (FF) Control Tests should be able to enable/disable feature flags dynamically, allowing for flexible test scenarios. Granular feature control enables: Running multiple tests with different users and feature sets simultaneously. Conducting A/B testing to verify that: Enabled features work as expected. Disabled features don’t break existing functionality. 6. Capturing System Metrics Tests should log key resource metrics (CPU, memory, etc.) to monitor system performance and detect anomalies. 7. Production-Representative Test Environments Test environments should be configured to mirror production as closely as possible. Feature Testing: Ensuring same release versions and feature flags as production. Non-Feature Testing: Supporting load, stress, security, and configuration tests that provide meaningful insights. By ensuring these capabilities, the infrastructure enables reliable, scalable, and high-impact testing, reducing deployment risks and improving software quality.\nSoftware Development for Testability In a testable software development approach, APIs, databases, and feature flags must be designed to support seamless testing while maintaining security and control in production.\n1. CRUD API Support A test should be able to Create, Read, Update, and Delete (CRUD) resources via APIs. Some API methods (e.g., DELETE, PUT, POST) may need to be disabled in production for security reasons but should remain accessible in test environments. 2. Database Management for Testing Testing requires the ability to identify, disable, or delete test data as needed. Collaboration with infrastructure or platform teams may be necessary to ensure proper test data management. 3. Feature Flag (FF) Control Feature Flags enable tests to toggle features on or off dynamically. Granular FF control allows: Running tests with and without a feature enabled simultaneously. Controlled experimentation and validation in both pre-production and production environments. 4. Time Manipulation for Testing Some features may require tests to simulate past or future dates. The system should support controlled time travel testing, ensuring functionality behaves correctly across different time conditions. By embedding these capabilities into software development, teams can ensure flexibility, security, and robust testability across environments.\nIn a Nutshell The efficiency, speed, and reliability of a test strategy depend on how well Design for Testability is implemented. The earlier this mindset is adopted, the greater the benefits.\nBy embedding testability from the start—whether in a new product or a growing company—teams can:\nReduce costs – It’s cheaper to build testability early than to retrofit later. Accelerate development – A testable design enables faster iterations, debugging, and automation. Improve reliability – A well-structured system ensures smoother deployments and fewer surprises in production. Waiting too long to introduce testability leads to higher costs, technical debt, and slower releases. A Design for Testability mindset from day one helps create scalable, maintainable, and high-quality software while keeping business agility intact.",
    "description": "Design for Testability Background Applying Design for Testability to Software Development Test Strategy Requirements 1. Continuous Testing Across CI/CD (Shift-Left \u0026 Shift-Right) 2. Repeatability \u0026 Reliability 3. Comprehensive Logging \u0026 Reporting 4. Self-Cleaning Tests 5. No Testing Limitations 6. Decentralized Testing Responsibility Infrastructure and Configuration for Testability 1. Testing in Any Environment (Including Production, If Needed) 2. Self-Cleaning Tests 3. Domain-Isolated Testing 4. Multi-Interface Testing 5. Feature Flag (FF) Control 6. Capturing System Metrics 7. Production-Representative Test Environments Software Development for Testability 1. CRUD API Support 2. Database Management for Testing 3. Feature Flag (FF) Control 4. Time Manipulation for Testing In a Nutshell Background “Design for Testability” is a principle I frequently use, and it comes from my early experience in electrical engineering. In hardware development, especially when designing Printed Circuit Boards (PCBs) and Printed Circuit Board Assemblies (PCBAs), testing isn’t just a step—it’s a necessity.",
    "tags": [],
    "title": "Design for Testability",
    "uri": "/blogs/design-for-testability/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Gradle (6.6.1) Terminology Actions Files structure for a “Multi-module build” Commands Example of Typed Task Plugins Gradle (6.6.1) requires JDK download from [https://gradle.org/releases/] can use either Groovy or Kotlin DSL (Domain-Specific Language) Terminology Project: models a software component Build script: contains automation instructions for a project Task: defines executable automation instructions Ad hoc task: implements one-off, simplistic action code by defining doFirst or doLast, automatically extends DefaultTaslk without having to declare it Typed task: Explicitly declares type (for example, Copy); does not need to define actions as they are already provided by type Wrapper: set of files checked into SCM alongside source code standardizes compatible Gradle version for a project automatically downloads the Gradle distribution with defined version Actions doLast, doFirst, … Files structure for a “Multi-module build” root + build.gradle + moduleA/ | + build.gradle | + src/ + moduleB/ + build.gradle + src/ Commands gradle \u003ctask name\u003e -\u003e run the task from the build.gradle\ngradle wrapper -\u003e create the wrapper files\ncreates gradlew, gradlew.bat and the gradle directories…\ngradle/wrapper/gradle-wrapper.properties\nexample:\ndistributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https\\://services.gradle.org/distributions/gradle-6.6.1-bin.zip zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists ./gradlew \u003ctask name\u003e -\u003e download the wrapper gradle version and execute the task with it\ngradle projects -\u003e create settings for the project\nUse settings.gradle do define information, example:\nrootProject.name = \"gradle-training\" gradle \u003ctask name\u003e --dry-run -\u003e show the tasks from the build.gradle but does not execute\ngradle tasks --all -\u003e show the full list of available tasks\nExample of Typed Task example:\ntask copyFiles(type: Copy) { from \"sourceFiles\" into \"target\" include \"**/*md\" includeEmptyDirs = false } task createZip(type: Zip) { from \"build/docs\" archiveFileName = \"docs.zip\" destinationDirectory = file(\"build/dist\") dependsOn CopyFiles } Plugins create a reusable or sharable \u003cplugin-name\u003e.gradle file with defined tasks apply the plugin to the local project build.gradle file, example: plugin with tasks: myPlugin.gradle in build.gradle: apply from: \"myPlugin.gradle\" Notes:\navailable plugins are Core Plugins (from Gradle) and Community Plugins (not from Gradle) to apply Core plugin Base (for example), in build.gradle: apply plugin: 'base'",
    "description": "Gradle (6.6.1) Terminology Actions Files structure for a “Multi-module build” Commands Example of Typed Task Plugins Gradle (6.6.1) requires JDK download from [https://gradle.org/releases/] can use either Groovy or Kotlin DSL (Domain-Specific Language) Terminology Project: models a software component Build script: contains automation instructions for a project Task: defines executable automation instructions Ad hoc task: implements one-off, simplistic action code by defining doFirst or doLast, automatically extends DefaultTaslk without having to declare it Typed task: Explicitly declares type (for example, Copy); does not need to define actions as they are already provided by type Wrapper: set of files checked into SCM alongside source code standardizes compatible Gradle version for a project automatically downloads the Gradle distribution with defined version Actions doLast, doFirst, … Files structure for a “Multi-module build” root + build.gradle + moduleA/ | + build.gradle | + src/ + moduleB/ + build.gradle + src/ Commands gradle \u003ctask name\u003e -\u003e run the task from the build.gradle",
    "tags": [],
    "title": "Gradle",
    "uri": "/notes/gradle/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Project structure for gradle project in Java In build.gradle JAVA plugin APPLICATION plugin Commands Maven Dependencies Testing with Gradle JUnit 5 dependencies Project structure for gradle project in Java single Java project with Gradle\nexample: root + src + main + java + resources + test + java + resources + build + classes -\u003e compiled class files + libs -\u003e generated JAR files A multi-modules project file structure:\nexample: root/ + appA/ + build.gradle + src/ + main/ + java/ + resources/ + test/ + java/ + resources/ + build/ + classes/ -\u003e compiled class files + libs/ -\u003e generated JAR files + appB/ + build.gradle + src/ ... (same as above subproject) In build.gradle JAVA plugin add plugins:\nplugins { id 'java' } set the Java compatibility:\njava { sourceCompatibility = JavaVersion.VERSION_11 targetCompatibility = JavaVersion.VERSION_11 } compiler arguments:\ncompileJava { // example: to terminate compilation if a warning occures) options.compilerArgs \u003c\u003c '-Werror' } set JAR filename explicitly:\nversion = '1.0.0' jar { archiveBaseName = '\u003cyour-project-name\u003e' } Set javadoc options:\njavadoc { options.header = '\u003cyour java doc title here\u003e' options.verbose() } APPLICATION plugin in build.gradle file:\nplugins { id 'application' } set base class to run for the application as needed by plugin:\napplication { mainClass = 'com.\u003corganization\u003e.\u003cproject\u003e.Main' } in settings.gradle file:\nrootProject.name = '\u003cproject_name\u003e' include ':appA', ':appB' Commands gradle wrapper -\u003e Create Gradle Wrapper\ngradle clean -\u003e Clean dist output?\nwith java plugin:\n./gradlew compileJava --console=verbose ./gradlew processResources --console=verbose or both above command agragated command:\n./gradlew classes --console=verbose see dependencies tree (and if it can be found)\n./gradlew dependencies generate JAR file\n./gradlew jar -\u003e dropped JAR in build/libs directory with application plugin:\n./gradlew run --args=\"add 1 2\" -\u003e specifies arguments ./gradlew installDist -\u003e generate shippable application with scripts ./gradlew distZip distTar -\u003e bundle distributhe appliation ./gradlew javadocs -\u003e genetate java doc in build/docs/javadoc (index.html) project or multi-modules project:\n./gradlew project -\u003e show project and show projects structure Maven Dependencies where to find? [https://search.maven.org]\ndependency coordinates:\nexample: commons-cli:commons-cli:1.4 -\u003e \u003cgroup\u003e:\u003cartifact\u003e:\u003cversion\u003e or GAV set the repositories (where to get from) and dependencies:\nrepositories { mavenCentral() } dependencies { implementation 'commons-cli:commons-cli:1.4' // get from maven repo search results implementation project(':appA') // add project dependencies (other modules from this project) } Testing with Gradle JUnit 5 dependencies from search.maven.org:\nsearch org.junit.jupiter (latest version is 5)\nminimum needed is junit-jupiter-api and junit-jupiter-engine declaring test dependencies:\ntestImplementation -\u003e work on compilation and test execution testRuntime -\u003e work on runtime only adding dependencies:\ncopy \u0026 paste Gradle Groovy DSL path for both depenencies dependencies { implementation 'commons-cli:commons-cli:1.4' // not a testImplementation! testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.0' // test dependency testRuntime 'org.junit.jupiter:junit-jupiter-engine:5.7.0' // test dependency on run time only } commands:\n./gradlew compileTestJava -\u003e compile tests ./gradlew test -\u003e run the tests (see useJUnitPlatform comment in order to use JUnit 5) go to build/reports/tests/test the automaticaly gradle generated HTML report (index.html) go to build/test-reults/test for the autmaticaly gradle generated XML test report adding test task in build.gradle:\ntest { useJUnitPlatform() // Java plugin expect Java 4 by default. Add useJUnitPlatform to indicates to use JUnit5 instead testLogging { events 'started', 'skipped', 'failed' // will display specified events on run time exceptionFormat 'full' // show stack trace on failures } }",
    "description": "Project structure for gradle project in Java In build.gradle JAVA plugin APPLICATION plugin Commands Maven Dependencies Testing with Gradle JUnit 5 dependencies Project structure for gradle project in Java single Java project with Gradle\nexample: root + src + main + java + resources + test + java + resources + build + classes -\u003e compiled class files + libs -\u003e generated JAR files A multi-modules project file structure:",
    "tags": [],
    "title": "Gradle for Java-Based applications and libraries",
    "uri": "/notes/gradle-for-java-based-applications-and-libraries/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "How to promote the Quality Best Practices Influence or evangelize Data-Driven decisions Enforce How to promote the Quality Best Practices Two ways to promote the quality best practices are: Influencing the teams or to enforce the best practices. A periodical team reality check about the current practices is the beginning of the process. A team can easily live in their own Ivory Tower or to be biased by this statement: “We always did it that way so why changing it?”.\nInfluence or evangelize Discussions: gather information about current pain points (retrospectives and lessons learned) Involve QA Specialists in all SDLC (or Software Development Life Cycle) 1:1 meetings to get information and expose point of views, share ideas and explain the best practices: it may be easier to convince on person than a group Talk to the teams Provide existing articles and blog posts Tech talk to expose the vision Create training material: guidelines, processes, videos (e.g.: Pluralsight, etc.) Involve people in quality solutions: Organize Workshops (on short term) so that the targeted public becomes active participants Organize Work Groups (on long term) so that the concerned people become active participants To use data as leverage; more information is available in the data-driven decision section Data-Driven decisions To show metrics of the current situation (or challenge the “no metrics” or lack of metrics). It is the also called “Shift-Right” approach in the Software Quality world.\nEnd-users or Customers survey can be used (e.g.: use a post-transaction experience survey on the mobile app or web site) White-Label partner surveys Use production dashboards (examples: Datadog or SignalFX metrics) and performance tools (e.g.: WebVitals, etc.) to monitor APIs, SQLs, etc. Daily performance emails can be sent to the engineering teams NPS and CSAT surveys: NPS vs CSAT definitions Real Customers Journey (traces of the end-users through the applications) Use support CIM dashboards: customer tickets, production tickets, ticket aging/bug fix time, tickets by services or teams, RCA (find the issues caused by quality process) CIM stands for Customer Interaction Management A way to generate CIM data is to use a tool like eazy bi Correlation between test coverage (static code) and production issues It is also possible to get test coverage from a live service using a tool like Sealights Compile the functional testers (manual tests) created Tickets/JIRA vs current team or service test coverage Create Success Stories: start with one team and use the before/after statistics GIT statistics (e.g. with a tool like GitPrime (now Pluralsight Flow)) in order to correlate bugs/tickets and developers stats (e.g.: number of commits, active days, etc.) I.e.: low stats may be caused by developers not working on features or test automations because they are working on manual tests or production support Correlate the release cycle (how often the team releases or time to market) and test coverage; better/reliable test coverage and test regression suite will shorten the release cycle. Gather statistic about how much time it takes to refactor an existing feature? Better test coverage will result in faster refactoring Enforce Contact the best practices stakeholders and ask for support (I.e.: to push the best practices downward) To fix deadlines (must be adopted by specific dates) Enforce with Gates, checklists, DoD, etc. Set Quality specific OKRs Assign Quality trainings",
    "description": "How to promote the Quality Best Practices Influence or evangelize Data-Driven decisions Enforce How to promote the Quality Best Practices Two ways to promote the quality best practices are: Influencing the teams or to enforce the best practices. A periodical team reality check about the current practices is the beginning of the process. A team can easily live in their own Ivory Tower or to be biased by this statement: “We always did it that way so why changing it?”.",
    "tags": [],
    "title": "How to promote the Quality Best Practices",
    "uri": "/blogs/how-to-promote-the-quality-best-practices/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac From Windows Create GitHub Page Create or Clone exiting Hugo repository Clone the GitHub repo to publish on the hugo site Create a new Hugo site Clone the Theme directory Update the Hugo config.toml file content with team Build Verify if the configuration is good Update the logo Use github documentation repo as content Try locally Build for GitHub Page Links and references Training References Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac brew install hugo\nFrom Windows choco install hugo -confirm\nReference: [https://gohugo.io/getting-started/installing#chocolatey-windows]\nCreate GitHub Page From GitHub.com -\u003e Create a new Repository, e.g., AlainBouchard.github.io\nCreate or Clone exiting Hugo repository In this example, we’ll use project engineering-notes-site\nClone the GitHub repo to publish on the hugo site engineering-notes-site\u003e git clone git@github.com:AlainBouchard/engineering-notes-site.git Create a new Hugo site engineering-notes-site\u003e hugo new site --force engineering-notes-site\u003e ls archetypes config.toml content data layouts public static themes Clone the Theme directory Example of theme, many can be found on Hugo site.\nHugo Theme Source: [https://themes.gohugo.io/themes/hugo-theme-relearn/]\ncd themes themes\u003e git clone git@github.com:McShelby/hugo-theme-relearn.git Update the Hugo config.toml file content with team \u003e vi config.toml baseURL = \"https://AlainBouchard.github.io/\" languageCode = \"en-us\" title = \"Alain Bouchard's Engineering Notes\" theme = \"hugo-theme-relearn\" [outputs] home = [ \"HTML\", \"RSS\", \"JSON\"] [module] [[module.imports]] path = \"github.com/alain-bouchard-quality/engineering-notes\" [[module.imports.mounts]] source = \"content\" target = \"content\" Build \u003e hugo -t hugo-theme-relearn Verify if the configuration is good \u003e hugo server Update the logo Create statics/logo.png\nCreate layouts/partials/logo.html\n\u003cimg src=\"logo.png\"\u003e Link (git submodule) the documents into the GitHub Page\n\u003e rm -Rf public \u003e git submodule add -b master git@github.com:AlainBouchard/AlainBouchard.github.io.git public \u003e git remote -v Expect the public directory to get created with the repo content\nUse github documentation repo as content In document source github repo Source repo example: github.com/AlainBouchard/engineering-notes\n\u003e hugo mod init github.com/AlainBouchard/engineering-notes In hugo project github repo Hugo repo example: github.com/AlainBouchard/engineering-notes-site\n\u003e hugo mod init github.com/AlainBouchard/engineering-notes-site Verify if the module work:\n\u003e hugo mod get github.com/AlainBouchard/engineering-notes Expect go.sum to get created:\ngithub.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6 h1:0s4tNjEN0+jGCkNEObTbnW+akRJD5RdjJ8pPsUU5ROU= github.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6/go.mod h1:Z6BTmpjCul+gI1Za7PY2E0LgyfIJpyeII/zcRE3e654= Expect go.mod to get updated:\nmodule engineering-notes-site go 1.18 require github.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6 // indirect Try locally Build\n\u003e hugo \u003e hugo server --disableFastRender --ignoreCache Expect Hugo to run on [http://localhost:1313/]\nBuild for GitHub Page build for theme\n\u003e hugo -t hugo-theme-relearn \u003e go the `/public` \u003e git status \u003e git add . \u003e git commit -m \"xyz\" \u003e git push expect the GitHub Page repo to be updated.\nLinks and references Training (10 min)\nReferences Master Hugo Modules: Handle Content Or Assets As Modules/ Working with Hugo Module Locally Hugo Relearn Theme \u003e Content \u003e Pages organization Learn Theme for Hugo \u003e Shortcodes \u003e Children How to Add Table Of Contents to a Hugo Blog",
    "description": "Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac From Windows Create GitHub Page Create or Clone exiting Hugo repository Clone the GitHub repo to publish on the hugo site Create a new Hugo site Clone the Theme directory Update the Hugo config.toml file content with team Build Verify if the configuration is good Update the logo Use github documentation repo as content Try locally Build for GitHub Page Links and references Training References Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac brew install hugo",
    "tags": [],
    "title": "Hugo",
    "uri": "/notes/hugo/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "IntelliJ cheat sheet IntelliJ cheat sheet Create Main Method Create Main Method Create the main method: write psvm and select the method to create:\npublic static void main(String[] args) { }",
    "description": "IntelliJ cheat sheet IntelliJ cheat sheet Create Main Method Create Main Method Create the main method: write psvm and select the method to create:\npublic static void main(String[] args) { }",
    "tags": [],
    "title": "IntelliJ Cheat Sheet",
    "uri": "/notes/intellij-ide/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Java Object-Oriented Programming Class blueprint Static vs Non-Static members Enum blueprint Main class blueprint Princibles Encapsulation Inheritance Polymorphism Abstraction Usefull build-in JAVA commands and other information Method Reference Operator (or ::) The Arbitrary Number of Arguments (or ... as a function argument) Predicate The final keyword Generics Java Object-Oriented Programming Class blueprint // Class blueprint public class MyClass { // Can have attributes... private String attribute1; // private attribute member, getter/setter are needed MyEnum myEnum; static string staticAttribute1 = \"Static Attribute\"; // belongs to the class private final Integer myFinalInteger; // can't be modified or be overridden by any subclasses // Constructor MyClass() { // Initiate the instance of MyClass class. this(\"'this' is used to call alternate constructors from within a constructor\"); // optional code lines... } // Constructor MyClass(String message) { // Initiate the instance of MyClass class. } // Setter for private attribute public void setAttribute1(attribute1) { this.attribute1 = attribute1; // note: \"this\" keyword is needed to disambuguate the variable reference. } // Getter for private attribute public String getAttribute1() { return attribute1; } // Methods or behaviours: void myMethod() { // no modifier so the method is accessible from its package only. this.attribute1 = 'xyz'; } private void myPrivateMethod() { // private so only accessible from this classs } protected void myProtectedMethod() { // protected method so accessible from this class and sub-classes } public void myPublicMethod() { // public method so accessible from anywhere; } // Static method static void myStaticMethod() { // A static method does not rely on any non-static attribute or member. } } Static vs Non-Static members Non-Static Member: is accessible from an instance and belongs to that instance Static Member: is accessible through the class and belongs to that class Static members can be accessed using the class name, example: Enum blueprint // Enum blueprint public enum MyEnum { CONSTANT1, CONSTANT2, ..., CONSTANTN } Main class blueprint // Main class public class Main { public static void main(String[] arg) { MyClass myClass = new MyClass('abc'); // Create an object of type MyClass myClass.myMethod('xyz'); // Call a method from my object } } Princibles Encapsulation allow us to bind together data and related functionality prevent classes from becoming tightly coupled easily modify the inner workings of one class without affecting the rest of the program restrictions we need a clear interface between a given class and the rest of the program everything can’t have direct access make the class attributes hideen from other classes using encapsulation provide a clear interface through public methods benefits clear pathways for classes to communicate less code changes required for a refactoring change less likely for an attribute to be overwritten with an invalid or null value unexpectedly Access Modifiers in Java: private: only visible in class that the member lives in no modifier: only visible in package the member lives in protected: visible to the package and all subclasses public: accessible everywhere within the program Inheritance allow us to create class hierarchies Subclass (child class) inherits properties referred to as the child class Superclass (parent class) is inherited from referred to as the parent class promotes code reusability and scalability leveraging inheritance: a class can only have one superclass but multiple subclasses if multiple super classes is needed then multilevel inheritance is required public class MySuperClass { protected String a1; private String a2; MySuperClass(String a1, String a2) { // Super Class constructor this.a1 = a1; this.a2 = a2; } public void myMethod() { // Method of the Super Class } } public class MySubClass extends MySuperClass { MySubClass(String arg1, String arg2) { super(arg1, arg2); // call superclass with arguments // Sub Class constructor } public String getA1() { return super.a1; // this.a1 would also work for this Superclass protected variable; } @Override public void myMethod() { // Override the Super Class Method within the Sub Class } } Polymorphism the ability for an object or function to take amny different forms Java supports two types of polymorphism: run time and compile-time polymorphism it helps to reduce complexity and write reusable code Abstraction helps us with hide implementation complexity\nJava supports abstact classes and interfaces\nhelps by fixing inputs and outputs and giving general idea of what the system does\nan abstract class:\nalmost like a template can’t be instencied other classes can extend the abstract calss and implement the appropriate functionality an example of an abstract class:\npublic abstract class myAbstractClass { // The class requires the `abstract` keyword since it contains an abstract method. private final String myString; // final String (aka constant) protected abstract void myAbstractMethod(); // an abstract method is not implemented! } public class myOtherClass extends MyAbstractClass { // variables, constructors... etc. @Override protected abstract void myAbstractMethod() { // The abstract method from the abstract class must be implemented by the sub-class. } } an interface is:\na set of method signatures for to-be-implemented functionality a specification for a set of behaviors without implementation can’t be instencied public interface MyInterface { Long myMethod1(); // No method implementation void myMethod2(); } public class myClassImplementingMyInterface implements MyInterface { @Override public Long myMethod1() { // the implementation for myMethod1 method } @Override public void myMethod2() { // the implementation for myMethod2 method } } Consider using abstract classes if any of these statements apply to your situation:\nIn the java application, there are some related classes that need to share some lines of code then you can put these lines of code within the abstract class and this abstract class should be extended by all these related classes. You can define the non-static or non-final field(s) in the abstract class so that via a method you can access and modify the state of the Object to which they belong. You can expect that the classes that extend an abstract class have many common methods or fields, or require access modifiers other than public (such as protected and private). Consider using interfaces if any of these statements apply to your situation:\nIt is a total abstraction, All methods declared within an interface must be implemented by the class(es) that implements this interface. A class can implement more than one interface. It is called multiple inheritances. You want to specify the behaviour of a particular data type, but not concerned about who implements its behaviour. Usefull build-in JAVA commands and other information System.out.println(“string…”); Method Reference Operator (or ::) a method reference operator (or ::) is used to call a method by referring to it with the help of its class directly\nlike using a lambda expression, example: // Get the stream Stream\u003cString\u003e stream = Stream.of(\"Geeks\", \"For\", \"Geeks\", \"A\", \"Computer\", \"Portal\"); // Print the stream using lambda method: stream.forEach(s -\u003e System.out.println(s)); // Print the stream using double colon operator stream.forEach(System.out::println); // Both lambda and :: will do the same thing. The Arbitrary Number of Arguments (or ... as a function argument) it means that zero or more String objects (or a single array of them) may be passed as the argument(s) for that method Reference : [http://java.sun.com/docs/books/tutorial/java/javaOO/arguments.html#varargs] important note: the argument(s) passed in this way is always an array - even if there’s just one. Make sure you treat it that way in the method body the argument that gets the ... must be the last in the method signature. So, myMethod(int i, String… strings) is okay, but myMethod(String… strings, int i) is not okay example: public static int myFunction (int ... a) { int sum = 0; for (int i : a) sum += i; return sum; } public static void main( String args[] ) { int ans = myFunction(1,1,1); // could have any number of arguments System.out.println( \"Result is \"+ ans ); } Predicate a Predicate in general meaning is a statement about something that is either true or false. In programming, predicates represent single argument functions that return a boolean value\nexample:\n@FunctionalInterface public interface Predicate\u003cT\u003e { boolean test(T t); } An example with filter() since it does accept a Predicate as parameter:\n// With lambda function: List\u003cInteger\u003e list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); List\u003cInteger\u003e collect = list.stream().filter(x -\u003e x \u003e 5).collect(Collectors.toList()); System.out.println(collect); // [6, 7, 8, 9, 10] // With predicate: Predicate\u003cInteger\u003e noGreaterThan5 = x -\u003e x \u003e 5; List\u003cInteger\u003e list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); List\u003cInteger\u003e collect = list.stream().filter(noGreaterThan5).collect(Collectors.toList()); System.out.println(collect); // [6, 7, 8, 9, 10] More examples with Java 8 Predicate Examples\nThe final keyword Java final keyword is a non-access specifier that is used to restrict a class, variable, and method. If we initialize a variable with the final keyword, then we cannot modify its value if we declare a method as final, then it cannot be overridden by any subclasses if we declare a class as final, we restrict the other classes to inherit or extend it example: final variables: to create constants final classes: to prevent inheritance final methods: to prevent method overriding Generics using generics enable types (classes and interfaces) to be parameters when defining classes, interfaces and methods.\nusing generics give many benefits over using non-generic code\nstronger type checks at compile time:\nJava compiler applies strong type checking to generic code and issues errors if the code violates type safety. Fixing compile-time errors is easier than fixing runtime errors, which can be difficult to find example: // Without Generics List list = new ArrayList(); list.add(\"hello\"); // With Generics List\u003cInteger\u003e list = new ArrayList\u003cInteger\u003e(); list.add(\"hello\"); // will not compile enabling programmers to implement generic algorithms\nby using generics, programmers can implement generic algorithms that work on collections of different types, can be customized, and are type safe and easier to read. elimination of casts\nexample: // Without Generics: List list = new ArrayList(); list.add(\"hello\"); String s = (String) list.get(0); // Need to cast return value to String // With Generics: List\u003cString\u003e list = new ArrayList\u003cString\u003e(); list.add(\"hello\"); String s = list.get(0); // no cast needed Generics type parameters:\nT: Type E: Element K: Key (used in Map) N: Number V: Value (used in Map) Reference: [https://www.journaldev.com/1663/java-generics-example-method-class-interface]",
    "description": "Java Object-Oriented Programming Class blueprint Static vs Non-Static members Enum blueprint Main class blueprint Princibles Encapsulation Inheritance Polymorphism Abstraction Usefull build-in JAVA commands and other information Method Reference Operator (or ::) The Arbitrary Number of Arguments (or ... as a function argument) Predicate The final keyword Generics Java Object-Oriented Programming Class blueprint // Class blueprint public class MyClass { // Can have attributes... private String attribute1; // private attribute member, getter/setter are needed MyEnum myEnum; static string staticAttribute1 = \"Static Attribute\"; // belongs to the class private final Integer myFinalInteger; // can't be modified or be overridden by any subclasses // Constructor MyClass() { // Initiate the instance of MyClass class. this(\"'this' is used to call alternate constructors from within a constructor\"); // optional code lines... } // Constructor MyClass(String message) { // Initiate the instance of MyClass class. } // Setter for private attribute public void setAttribute1(attribute1) { this.attribute1 = attribute1; // note: \"this\" keyword is needed to disambuguate the variable reference. } // Getter for private attribute public String getAttribute1() { return attribute1; } // Methods or behaviours: void myMethod() { // no modifier so the method is accessible from its package only. this.attribute1 = 'xyz'; } private void myPrivateMethod() { // private so only accessible from this classs } protected void myProtectedMethod() { // protected method so accessible from this class and sub-classes } public void myPublicMethod() { // public method so accessible from anywhere; } // Static method static void myStaticMethod() { // A static method does not rely on any non-static attribute or member. } } Static vs Non-Static members Non-Static Member: is accessible from an instance and belongs to that instance Static Member: is accessible through the class and belongs to that class Static members can be accessed using the class name, example: Enum blueprint // Enum blueprint public enum MyEnum { CONSTANT1, CONSTANT2, ..., CONSTANTN } Main class blueprint // Main class public class Main { public static void main(String[] arg) { MyClass myClass = new MyClass('abc'); // Create an object of type MyClass myClass.myMethod('xyz'); // Call a method from my object } } Princibles Encapsulation allow us to bind together data and related functionality prevent classes from becoming tightly coupled easily modify the inner workings of one class without affecting the rest of the program restrictions we need a clear interface between a given class and the rest of the program everything can’t have direct access make the class attributes hideen from other classes using encapsulation provide a clear interface through public methods benefits clear pathways for classes to communicate less code changes required for a refactoring change less likely for an attribute to be overwritten with an invalid or null value unexpectedly Access Modifiers in Java: private: only visible in class that the member lives in no modifier: only visible in package the member lives in protected: visible to the package and all subclasses public: accessible everywhere within the program Inheritance allow us to create class hierarchies Subclass (child class) inherits properties referred to as the child class Superclass (parent class) is inherited from referred to as the parent class promotes code reusability and scalability leveraging inheritance: a class can only have one superclass but multiple subclasses if multiple super classes is needed then multilevel inheritance is required public class MySuperClass { protected String a1; private String a2; MySuperClass(String a1, String a2) { // Super Class constructor this.a1 = a1; this.a2 = a2; } public void myMethod() { // Method of the Super Class } } public class MySubClass extends MySuperClass { MySubClass(String arg1, String arg2) { super(arg1, arg2); // call superclass with arguments // Sub Class constructor } public String getA1() { return super.a1; // this.a1 would also work for this Superclass protected variable; } @Override public void myMethod() { // Override the Super Class Method within the Sub Class } } Polymorphism the ability for an object or function to take amny different forms Java supports two types of polymorphism: run time and compile-time polymorphism it helps to reduce complexity and write reusable code Abstraction helps us with hide implementation complexity",
    "tags": [],
    "title": "Java Object-Oriented Programming",
    "uri": "/notes/java-object-oriented-programming/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Java Spring Boot 2 Terminology Getting Started Why Spring Boot? Spring Initializr Inversion of Control Proxies Data Access in Spring Spring Data Embeedded DB with Spring Boot Repository with Spring Data Using remote database Service Tier Utilizing IoC Service abstraction Spring Service Object Web pages with Spring Controller Java Spring Boot 2 Terminology POJO : Plain Old Java Object (may have more that setters/getters in Spring world) Java Beans : Simple objects with only setters/getters Spring Beans : POJOs confiugered in the application context DTO : Data Transfer Objects are Java Beans used to move state between layers IOC : Inversion Of Control IoC provides mechanism of dependency injection Application Context wraps the Bean Factory which serves the beans at the runtime of the application Spring Boot provides auto-configuration of the Application Context Getting Started Why Spring Boot? Support rapid development Remove boilerplate of application setup Many uses Cloud Native support but also traditional Key Aspects Embedded tomacat (or others) Auto-configuration of Application Context Automatic Servlet Mappings Database support and Hibermate/JPA dialect Automatic Controller Mappings Auto Config Default opiniated configuration Very to override defaults Configuration on presence Spring Initializr start.spring.io Spring Boot: pick latest released version (ie. 2.5.6) Packaging: Jar Add Dependencies: Pring Web TBD… Generate Now it can build and run as is:\njava -jar target/xyz-0.0.1-SNAPSHOT.jar\nUse Chrome: localhost:8080\nInversion of Control Container mainains your class dependencies Objects injected at runtime or startup time An object accepts the dependencies for construction instead of constructing them Spring IoC Bean Factory Application Context References Analysis of construction order Proxies Beans in Bean Faactory are proxied Annitations drive proxies Annitations are easy extension points, for your own abstracts too Method calling order matters Data Access in Spring Spring Data Provides a common set of interfaces Provides a common naming convention Provides aspected behavior Provides Repository and Data Mapping convention Benefits of Spring Data Remove boilerplate code Allows for swapping datasources easier Allows to focus on buisiness logic Key Components Repository Interface Entity Object DataSource no accessed directly Embeedded DB with Spring Boot Needed dependencies:\norg.springframework.boot:spring-boot-starter-data-jpa com.h2database:h2 Set application.properties:\nlogging.level.org.springframework.jdbc.datasource.init.ScriptUtils=debug : by default is set to info spring.jpa.hibernate.ddl-auto=none : don’t create schema, and just connect to DB Repository with Spring Data Java Persistence API (or JPA)\nMapping Java objects to DB tables and vice versa is called Object-relational Mapping (ORM) JPA permits the developer to works directly with objects rather tahn with SQL statements Based on Annotations Entity\nA class which should be persisted in a database it must be annotated with javax.persistence.Entity JPA uses a database table for every entity Persisted instances of the class will be represented as one row in the table JPA allows to auto-generate the primary key in the database via the @GeneratedValue annotation By default, the table name corresponds to the class name. You can change this with the addition to the annotation @Table(name=\"NEWTABLENAME\") Code Example @Entity @Table(name=\"ROOM\") public class Room { @Id @GeneratedValue(strategy = GenerationType.AUTO) @Column(name=\"ROOM_ID\") private long id; @Column(name=\"NAME\") private String name; @Column(name=\"ROOM_NUMBER\") private String roomNumber; @Column(name=\"BED_INFO\") private String bedInfo; public long getId() { return id; } public void setId(long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getRoomNumber() { return roomNumber; } public void setRoomNumber(String roomNumber) { this.roomNumber = roomNumber; } public String getBedInfo() { return bedInfo; } public void setBedInfo(String bedInfo) { this.bedInfo = bedInfo; } @Override public String toString() { return \"Room{\" + \"id=\" + id + \", name='\" + name + '\\'' + \", roomNumber='\" + roomNumber + '\\'' + \", bedInfo='\" + bedInfo + '\\'' + '}'; } } Create a CrudRepository interface for the created Entity: Code Example @Repository public interface RoomRepository extends CrudRepository\u003cRoom, Long\u003e { // Room is the Entity class // Long is the ID type } Create a Component Event:\nA @Component Annotation is automatically picked by Spring Code Example @Component public class AppStartupEvent implements ApplicationListener\u003cApplicationReadyEvent\u003e { private final RoomRepository roomRepository; public AppStartupEvent(RoomRepository roomRepository) { this.roomRepository = roomRepository; } @Override public void onApplicationEvent(ApplicationReadyEvent event) { Iterable\u003cRoom\u003e rooms = this.roomRepository.findAll(); rooms.forEach(System.out::println); } } Using remote database Replace the H2 database for an other database, example a PostgreSQL:\ndependencies: org.postgresql:postgresql In application.properties:\nspring.jpa.database=postgresql spring.datasource.url=jdbc:postgresql://localhost:5432/dev spring.datasource.username=postgres spring.datasource.password=postgres Service Tier Utilizing IoC Why use IoC?\nAllows you to focus on contracts Develoip business code only, leave constuction to the container Build intermediate abstractions Produce clean code Srping and IoC:\nIoC container is configured by developer Spring maintains handles to objects constucted at startup Spring serves singletons to classes during construction Spring maintains lifecycle of beans Developer only accesses the application context Service abstraction Why building Service Abstractions:\nEncapsulate layers? Abstract 3rd partys APIs Simplify implementations Swap out implementations as runtime (ie. factory pattern) How to build one?\nDefine our interface (or class) Create the API Inject the dependencies Annotate or configure (classes) Code the implemantation Spring Service Object We mark beans with @Service to indicate that they’re holding the business logic. Besides being used in the service layer, there isn’t any other special use for this annotation. Starting with Spring 2.5, the framework introduced annotations-driven Dependency Injection. The main annotation of this feature is @Autowired. It allows Spring to resolve and inject collaborating beans into our bean. Using @Autowired or either properties or setters/getters isn’t a good practice or easy to test; Use final properties with constructors to have immutable object. If more than one constructor is defined then using @Autowired on the default one will make Spring to use it. Code Example @Service public class ReservationService { private final RoomRepository roomRepository; private final GuestRepository guestRepository; private final ReservationRepository reservationRepository; @Autowired // optional if only one constructor public ReservationService(RoomRepository roomRepository, GuestRepository guestRepository, ReservationRepository reservationRepository) { this.roomRepository = roomRepository; this.guestRepository = guestRepository; this.reservationRepository = reservationRepository; } // Business logic here... } Web pages with Spring Controller Model View Controller (or MVC)\nFundamental pattern for Web application development The Model is the data The View is the visual display that is populated The Controller wires the view with the model Spring Controller\nSpring bean Annotated for the servlet mapping Responds to incoming web requests Output a view or raw data Template Engines\nSpring supports several Thymeleaf most popular Provides a DSL for HTML leaving raw html documents Placeholders for dynamic data Rendiring engin allows for final products Code Example @Controller @RequestMapping(\"/reservations\") public class RoomReservationController { private final DateUtils dateUtils; private final ReservationService reservationService; public RoomReservationController(DateUtils dateUtils, ReservationService reservationService) { this.dateUtils = dateUtils; this.reservationService = reservationService; } @RequestMapping(method = RequestMethod.GET) public String getReservations(@RequestParam(value=\"date\", required=false) String dateString, Model model){ Date date = this.dateUtils.createDateFromDateString(dateString); List\u003cRoomReservation\u003e roomReservations = this.reservationService.getRoomReservationsForDate(date); model.addAttribute(\"roomReservations\", roomReservations); return \"roomres\"; } } Can use Thymeleaf to create HTML pages.\nAdd the web page to src/main/resources/templates",
    "description": "Java Spring Boot 2 Terminology Getting Started Why Spring Boot? Spring Initializr Inversion of Control Proxies Data Access in Spring Spring Data Embeedded DB with Spring Boot Repository with Spring Data Using remote database Service Tier Utilizing IoC Service abstraction Spring Service Object Web pages with Spring Controller Java Spring Boot 2 Terminology POJO : Plain Old Java Object (may have more that setters/getters in Spring world) Java Beans : Simple objects with only setters/getters Spring Beans : POJOs confiugered in the application context DTO : Data Transfer Objects are Java Beans used to move state between layers IOC : Inversion Of Control IoC provides mechanism of dependency injection Application Context wraps the Bean Factory which serves the beans at the runtime of the application Spring Boot provides auto-configuration of the Application Context Getting Started Why Spring Boot? Support rapid development Remove boilerplate of application setup Many uses Cloud Native support but also traditional Key Aspects Embedded tomacat (or others) Auto-configuration of Application Context Automatic Servlet Mappings Database support and Hibermate/JPA dialect Automatic Controller Mappings Auto Config Default opiniated configuration Very to override defaults Configuration on presence Spring Initializr start.spring.io Spring Boot: pick latest released version (ie. 2.5.6) Packaging: Jar Add Dependencies: Pring Web TBD… Generate Now it can build and run as is:",
    "tags": [],
    "title": "Java Spring Boot 2",
    "uri": "/notes/java-spring-boot2/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Java with Rest-Assured Pattern API response Deserialize a response Tools Java with Rest-Assured Rest-Assured link: [https://rest-assured.io] can get latest version: [https://mvnrepository.com] jackson databind package can be used for data-binding hamcrest package can be used for matchers org.hamcrest.Matchers.* Pattern using the Given, When and Then pattern\nthe Given specify prerequisites the When describe the action to take the Then describe the expected result using JUnit 5: @Test public void getTest() { String endpoint = \"http://localhost:8888/a/b/c\"; var response = given(). // using easy to read format for doc only queryParam(\"id\", \"2\"). when(). get(endpoint). then(); } @Test public void postTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().post(endpoint).then(); } @Test public void putTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().put(endpoint).then(); } @Test public void deleteTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\" } \"\"\" var response = given().body(body).when().delete(endpoint).then(); } API response validate the status code\nassertThat example: @Test public void getTest() { String endpoint = \"http://localhost:8888/a/b/c\"; given().queryParam(\"key\", \"value\") .when().get(endpoint) .then().assertThat() .statusCode(200) // check status code is OK/200 .body(\"key\", equalTo(\"value\")) // check for response body key = value .body(\"records.size()\", greaterThan(0)) // check for response body record array to have 1+ items .body(\"records.id\", everyItem(notNullValue())) // make sure each records.id item from the array is not null .body(\"records.id[0]\", equalTo(8)) // make sure first records.id item = 0 .header(\"Content-Type\", equalTo(\"application/json\")); // verify the headers content-type field } Deserialize a response a class can be used to deserialize a response\nexample: @Test public void deserializeTest() { String endpoint = \"http://localhost:8888/a/b/c\"; MyResponseClass request = new MyResponseClass(1, 2, 3, \"value\"); MyResponseClass response = given() .queryParam(\"key\",\"value\") .when() .get(endpoint) .as(MyResponseClass.class); assertThat(response, samePropertyValuesAs(request)); // comparing every property of the classes } Tools response.log().body() -\u003e print the response to console",
    "description": "Java with Rest-Assured Pattern API response Deserialize a response Tools Java with Rest-Assured Rest-Assured link: [https://rest-assured.io] can get latest version: [https://mvnrepository.com] jackson databind package can be used for data-binding hamcrest package can be used for matchers org.hamcrest.Matchers.* Pattern using the Given, When and Then pattern\nthe Given specify prerequisites the When describe the action to take the Then describe the expected result using JUnit 5: @Test public void getTest() { String endpoint = \"http://localhost:8888/a/b/c\"; var response = given(). // using easy to read format for doc only queryParam(\"id\", \"2\"). when(). get(endpoint). then(); } @Test public void postTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().post(endpoint).then(); } @Test public void putTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\" var response = given().body(body).when().put(endpoint).then(); } @Test public void deleteTest() { String endpoint = \"http://localhost:8888/a/b/c\"; String body = \"\"\" { \"key1\": \"value1\" } \"\"\" var response = given().body(body).when().delete(endpoint).then(); } API response validate the status code",
    "tags": [],
    "title": "Java with Rest-Assured",
    "uri": "/notes/java-rest-assured/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Best Practices Use const and let, avoid using var Write Modular, Reusable Code Use Arrow Functions (=\u003e) Consistent Naming Conventions Error Handling Use Template Literals for String Concatenation Comment and Document the Code Prefer Destructuring for Objects and Arrays Use Promises and async/await for Asynchronous Code Use Strict Mode Write Unit Tests Use ESLint and Prettier for Code Quality and Formatting Avoid Global Variables Optimize Performance Minimize Use of this Context Avoid Deeply Nested Code Use Default Parameters Write Clean and Readable Code Principles Single Responsibility Principle (SRP) Don’t Repeat Yourself (DRY) Keep it simple, Stupid (KISS) Encapsulation Separation of Concerns (SoC) Modularity Favor Composition Over Inheritance What does TypeScript fix? Encapsulation, Privacy and Minimize this Context Type Safety and const Usage Avoiding Global Scope Pollution Type Safety and Immutability Error Handling and Fail Fast Documentation and Readability Conclusion It is easy to fall in a trap when coding in JavaScript. Thus, it is highly suggested to add types and use Typescript. Nevertheless, there are the Best Practices and Principles to follow with Javascript. Many can be used with other languages like Typescript.\nBest Practices Here are some best practices for javascript coding, in order to maintain clean, efficient and error-free code.\nUse const and let, avoid using var const is used for variables, or constants, whose values shouldn’t change. let is used for variables that may change. Avoid var due to its function-scoping, which can cause unexpected behavior Function Scope vs Block Scope The var is function-scoped, meaning that it is accessible within the function in which it is declared, regardless of the block it’s in, for example, within an if or for statement.\nThe let and const are block-scoped, meaning they are only accessible within the block they are defined in.\nExample:\nfunction testScope() { if (true) { var functionScoped = \"I'm accessible throughout the function\"; let blockScoped = \"I'm accessible only within this block\"; } console.log(functionScoped); // Works console.log(blockScoped); // ReferenceError: blockScoped is not defined } Hoisting (or raised) The var declarations are hoisted to the top of their scope, which means that Javascript moves all var declarations to the top of the function scope at runtime; warning: only declaration is hoisted, not the initialization.\nThis can lead to confusing bugs, especially if the var is used before it’s initialization. In this case, value will be undefined instead of causing an error.\nExample:\nconsole.log(myVar); // undefined (hoisted declaration) var myVar = \"Hello\"; // initialized here By contrast, let and const are also hoisted, but they remain in a temporal dead zone until they are actually declared in code. Accessing them before their declaration will cause a ReferenceError, meaning it easier to catch potential errors in the code.\nExample:\nconsole.log(myLet); // ReferenceError: Cannot access 'myLet' before initialization let myLet = \"Hello\"; Re-declaration Issues The var allows re-declaration within the same scope, which can cause unintended variable overwrites and bugs in the code.\nThe let and const do not allow re-declaration within same scope.\nExample:\nvar message = \"Hello\"; var message = \"World\"; // Re-declaration allowed with var console.log(message); // \"World\" let greeting = \"Hi\"; let greeting = \"Hello\"; // SyntaxError: Identifier 'greeting' has already been declared Write Modular, Reusable Code Break down code into smaller, reusable functions or modules. Use the Single Responsibility Principle (SRP): each function or module should handle one task. Consider using the module pattern or ES6 modules to organize code. Organizing Javascript code using the module pattern or ES6 modules helps manage code structure, making it more maintainable, modular, and reusable:\nModule Pattern The Module Pattern is a design pattern in javascript that allows you to group related functions, variables and objects into a single unit (or a module), usually returning an object that expose only what needs to be accessible externally while keeping the rest private.\nThis pattern relies only on closures to create private and public members within a function, providing encapsulation to avoid global scope pollution and project internal details from external access. Example:\nconst CounterModule = (function () { // Private variables and functions let counter = 0; function increment() { counter++; } function decrement() { counter--; } // Public API: exposed by returning an object return { getCounter: function () { return counter; }, incrementCounter: function () { increment(); }, decrementCounter: function () { decrement(); }, }; })(); // Usage CounterModule.incrementCounter(); console.log(CounterModule.getCounter()); // 1 In this example, counter, increment and decrement are private (can’t be accessed directly outside the module), while the getCounter, incrementCounter and decrementCounter methods are public.\nES5 Modules ES6 Modules (or ECMAScript 2015) introduced a built-in module system for Javascript, designed to support modular code directly in the language. This system provides import and export keywords to allow easy code sharing between files. With ES6 modules, each file is treated as a separated module with its own scope. Variables, functions and classes can be exported from one file and imported into another. This improves code readability and allows for cleaner dependencies without global variable conflicts. Example:\n// matUtils.js // Named exports export function add(a, b) { return a + b; } export function subtract(a, b) { return a - b; } // Default export export default function multiply(a, b) { return a * b; } // app.js // Importing named exports import { add, subtract } from './mathUtils.js'; // Importing default export import multiply from './mathUtils.js'; console.log(add(2, 3)); // 5 console.log(subtract(5, 3)); // 2 console.log(multiply(4, 2)); // 8 Notes:\nES6 modules are automatically strict mode meaning undeclared variables or unsafe operations are disallowed bu default. Unlike the module pattern, ES6 modules are asynchronous, and they use tree-shaking to removed unused exports, while helps optimize performance by reducing bundle size in production. tree-shaking is an optimization technique used in Javascript bundlers (like webpack, rollup and ESBuild) to remove unused code (dead code) from final bundle, making applications faster and more efficient. When? The module patternis ideal if you’re working in an environment where ES6 modules are not supported (e.g., older environments) or if you want to create an immediately usable object with private/public members without importing/exporting between files.\nThe ÈS6 Modules` are recommended for most modern Javascript application, especially for object using build tools like webpack, vite, rollup, etc. ES6 modules are now the standard way of organizing Javascript code in both client-side and server-side applications, including framework like React, Vue, and Node.js.\nUse Arrow Functions (=\u003e) Arrow functions are concise and don’t have their own this context, making them useful for callbacks and simpler functions. Warning: avoid arrow functions in methods if you need a function with its own this context. Use arrows functions for simple callbacks. Arrows functions make callbacks, especially inline ones, more concise and readable:\n// Good const numbers = [1, 2, 3]; const squares = numbers.map(num =\u003e num * num); // Avoid (excessive verbosity for a simple callback) const squaresOld = numbers.map(function(num) { return num * num; }); The practices help to make the code clearer, more predictable, and avoid common pitfalls with this binding.\nAvoid arrow functions for methods in classes Use regular function expressions for defining methods in classes, as arrow functions fo not have their own this context:\nclass Counter { count = 0; // Good (using regular function for method) increment() { this.count += 1; } } // Avoid (arrow functions lack their own `this`) class CounterBad { count = 0; increment = () =\u003e { this.count += 1; } } Use arrow functions to maintain `this“ context in callbacks When working with callbacks in a class, arrow functions can be beneficial for preserving the this context:\nclass Timer { constructor() { this.seconds = 0; } start() { setInterval(() =\u003e { this.seconds += 1; // `this` refers to Timer instance }, 1000); } } Consistent Naming Conventions Use camelCase for variables and functions: const userAge = 25;\nUse PascalCase for classes and constructors: class UserAccount{ ... }\nUse prefix boolean variables with is, has, should and can:\n// Good let isLoggedIn = false; const hasAccess = true; // Avoid let loggedIn = false; const accessGranted = true; Use descriptive names for variables, functions and classes:\n// Good const calculateTotalPrice = (items) =\u003e { /* ... */ }; let itemList = ['apple', 'orange', 'banana']; // Avoid (unclear purpose) const calcTP = (arr) =\u003e { /* ... */ }; let arr = ['apple', 'orange', 'banana']; Use descriptive names for Constants, use UPPER_SNAKE_CASE for constants that represent values unlikely to change (e.g., configuration values), this helps indicate immutability:\n// Good const MAX_USER_COUNT = 100; const API_BASE_URL = 'https://api.example.com'; // Avoid const MaxUserCount = 100; const apiBaseUrl = 'https://api.example.com'; Use consistent naming convention for Asynchronous Functions (async/fetch), when working with async functions, prefix the function names with fetch, get, load or retrieve to indicate the nature of the function:\n// Good async function fetchUserData() { /* ... */ } const loadSettings = async () =\u003e { /* ... */ }; // Avoid async function getData() { /* ... */ } Use handle or on prefixes with event handler functions to make their purpose clear:\n// Good async function fetchUserData() { /* ... */ } const loadSettings = async () =\u003e { /* ... */ }; // Avoid async function getData() { /* ... */ } Error Handling Always add error handling in asynchronous code (try/catch blocks with async/await). Use custom error messages to make debugging easier. Example:\n// Asynchronous function with proper error handling async function fetchUserData(userId) { try { const response = await fetch(`https://api.example.com/users/${userId}`); if (!response.ok) { throw new Error(`Failed to fetch data: ${response.status} ${response.statusText}`); } const data = await response.json(); return data; } catch (error) { // Custom error message for debugging console.error(`Error in fetchUserData: ${error.message}`); throw new Error(`Unable to retrieve user data for ID: ${userId}`); } } // Example usage of the function fetchUserData(123) .then(data =\u003e console.log(data)) .catch(error =\u003e console.log(error.message)); Use Template Literals for String Concatenation Instead of using + to concatenate strings, use template literals: // Good const firstName = \"Alain\"; const lastName = \"Bouchard\"; const greeting = `Hello, ${firstName} ${lastName}! Welcome.`; // Avoid const greetingOld = \"Hello, \" + firstName + \" \" + lastName + \"! Welcome.\"; Comment and Document the Code Write comments for complex or unclear code blocks, but avoid redundant comments. Use JSDoc-style comments for functions and classes to describe their purpose, parameters and return values. Example:\n/** * Calculates the total price of items with tax. * * @param {number[]} prices - Array of item prices. * @param {number} taxRate - Tax rate to be applied (e.g., 0.08 for 8%). * @returns {number} Total price after tax. */ function calculateTotalPrice(prices, taxRate) { const subtotal = prices.reduce((total, price) =\u003e total + price, 0); return subtotal * (1 + taxRate); } // Example usage const items = [10, 20, 30]; const total = calculateTotalPrice(items, 0.08); console.log(total); Prefer Destructuring for Objects and Arrays Destructuring improves readability and reduces repetitive code: Example:\nconst user = { name: 'Alain', age: 30 }; const { name, age } = user; Use Promises and async/await for Asynchronous Code Avoid callback hell bu using Promises or async/await. handle errors with try/catch blocks around async functions. Example:\n// Function using async/await with error handling async function fetchData(url) { try { const response = await fetch(url); const data = await response.json(); return data; } catch (error) { console.error(\"Error fetching data:\", error); throw error; // Re-throw to handle it in calling code if needed } } // Example usage fetchData(\"https://api.example.com/data\") .then(data =\u003e console.log(data)) .catch(error =\u003e console.log(\"Fetch failed:\", error.message)); Use Strict Mode Enable Strict Mode by adding use strict; at the beginning of your scripts. This mode prevents the use of undeclared variables and other common Javascript pitfalls: \"use strict\"; function calculateArea(radius) { // Prevents usage of undeclared variables pi = 3.14159; // ReferenceError: pi is not defined return pi * radius * radius; } calculateArea(5); Note: Strict Mode isn’t required with Typescript as TS inherently enforces many of the same safeguards.\nWrite Unit Tests Write unit tests for functions and components to verify code behavior. Use frameworks like Jest, Mocha, or Cypress (for FE and E2E testing). Use ESLint and Prettier for Code Quality and Formatting ESLint helps catch common errors and enforces a consistent style. Prettier ensures that code is formatted consistently across your project. Avoid Global Variables Avoid polluting the global namespace by declaring variables and functions at the global scope. Use closures aor modules instead. Example:\n// Avoid: Global variables and functions let count = 0; // Global variable function increment() { count++; console.log(\"Count:\", count); } function reset() { count = 0; console.log(\"Count reset.\"); } // Usage increment(); // Count: 1 increment(); // Count: 2 reset(); // Count reset. Example using an Immediately-Invoked Function Expression (IIFE) to create a closure:\n// IIFE to avoid polluting the global namespace const CounterModule = (function () { let count = 0; // Private variable function increment() { count++; console.log(\"Count:\", count); } function reset() { count = 0; console.log(\"Count reset.\"); } return { increment, reset, }; })(); // Using the module CounterModule.increment(); // Count: 1 CounterModule.increment(); // Count: 2 CounterModule.reset(); // Count reset. Note: A closure is a feature where an inner function has access to the outer (enclosing) function’s variables, even after the outer function has finished executing. This allows the inner function to \"remember\" the environment in which it was created. Closures are often used to create private variables and functions, which helps avoid global scope pollution:\nfunction createCounter() { let count = 0; // Private variable return function increment() { count++; // Accessing count from the outer scope console.log(\"Count:\", count); }; } const counter = createCounter(); // createCounter returns the inner function counter(); // Count: 1 counter(); // Count: 2 Optimize Performance Avoid excessive DOM manipulations; batch updates if possible. Use Debouncing and Throttling to handle frequent event calls like scrolling or resizing. Load only necessary libraries and assets, especially for web applications. Debouncing To delay the function execution until a specified time has passed since the last call. Useful for events that file rapidly, like resize:\n// Debounce function function debounce(func, delay) { let timer; return function (...args) { clearTimeout(timer); timer = setTimeout(() =\u003e func.apply(this, args), delay); }; } // Usage: Resize event with debounce window.addEventListener(\"resize\", debounce(() =\u003e { console.log(\"Resized!\"); }, 300)); Throttling To ensure a function is only called once in a specified time interval, regardless of how many times the event occurs. Useful for scroll events:\n// Throttle function function throttle(func, limit) { let inThrottle; return function (...args) { if (!inThrottle) { func.apply(this, args); inThrottle = true; setTimeout(() =\u003e (inThrottle = false), limit); } }; } // Usage: Scroll event with throttle window.addEventListener(\"scroll\", throttle(() =\u003e { console.log(\"Scrolled!\"); }, 300)); Minimize Use of this Context Use arrow functions and .bind() to avoid unexpected this binding. Prefer class for object-oriented code, which clarifies this usage. In Javascript, .bind() is used to explicitly set the this context for a function, ensuring it refers to a specific object regardless of where or how the function is called. This is particularly helpful when passing functions as callbacks, as it prevents unexpected this binding.\nExample of .bind[] to preserve this context:\nconst user = { name: \"Alain\", greet() { console.log(`Hello, ${this.name}!`); }, }; // Without .bind(), `this` is lost in a callback setTimeout(user.greet, 1000); // Output: \"Hello, undefined!\" // Using .bind() to preserve `this` setTimeout(user.greet.bind(user), 1000); // Output: \"Hello, Alain!\" In this specific case, the arrow function =\u003e isn’t suggested, Example:\nconst user = { name: \"Alain\", greet: () =\u003e console.log(`Hello, ${this.name}!`), // Avoid (arrow function here makes `this` undefined) }; Avoid Deeply Nested Code Deeply nested ode can be hard to read and maintain. Refactor nested callbacks and if statements into smaller functions. Use Default Parameters Set default values for function parameters to handle undefined arguments: function greet(name = 'Guest') { console.log(`Hello, ${name}`); } Write Clean and Readable Code Aim for readability over cleverness. Code should be understandable by others without needing additional explanation. Avoid adding comments, use code readability instead. Principles Javascript Principles form the foundational ideas and approaches that guide how code should be structured, organized and maintained. Following these principles leads to more efficient, maintainable and error-free code.\nSingle Responsibility Principle (SRP) Each function, module or class should have one specific responsibility. Bu following SRP, you make your code easier to read, test and maintain. Don’t Repeat Yourself (DRY) Avoid duplicating code; instead, reuse functions, classes, or modules. DRY promotes reusability and helps avoid inconsistent code, especially during updates. Keep it simple, Stupid (KISS) Write simple, clear code, instead of overly complicated solutions. Avoid unnecessary complexity and strive to make your code easy to understand and maintain. Encapsulation Hide implementation details and expose only what’s necessary. Encapsulation helps prevent external code from accidentally interacting with or modifying internal logic. Example:\nclass BankAccount { #balance = 0; // Private field constructor(accountHolder) { this.accountHolder = accountHolder; // Public property } deposit(amount) { if (amount \u003e 0) this.#balance += amount; } withdraw(amount) { if (amount \u003e 0 \u0026\u0026 amount \u003c= this.#balance) this.#balance -= amount; } getBalance() { return this.#balance; } } // Usage const account = new BankAccount(\"Alain\"); account.deposit(100); console.log(account.getBalance()); // 100 account.withdraw(30); console.log(account.getBalance()); // 70 // console.log(account.#balance); // Error: Private field '#balance' is not accessible Separation of Concerns (SoC) Keep different concerns (data handling, UI rendering, business logic) separate. In Javascript, this often means separating HTML/CSS from JavaScript logic and organizing Javascript into cohesive modules. Modularity Break down your code into smaller, independent modules or functions. Modularity improves reusability, testability, and maintainability by keeping related functions grouped logically. Favor Composition Over Inheritance Use composition (combining functions to build objects) rather than inheritance (creating class hierarchies) when possible. Composition is more flexible than inheritance, making it easier to reuse code without creating complex class hierarchies. Example of Composition vs Inheritance:\n// Behavior functions function canFly(obj) { obj.fly = () =\u003e console.log(\"Flying!\"); return obj; } function canSwim(obj) { obj.swim = () =\u003e console.log(\"Swimming!\"); return obj; } // Composing a bird object with specific behaviors const bird = canFly({}); bird.fly(); // Output: \"Flying!\" // Composing a fish object with different behavior const fish = canSwim({}); fish.swim(); // Output: \"Swimming!\" Instead of creating multiple subclasses like FlyingBird or SwimmingFish, we use small, composable functions (e.g., canFly or canSwim). This approach allows flexible combinations of behaviors, making it easier to create objects with varied capabilities without rigid class hierarchies.\nWhat does TypeScript fix? When using TypeScript with JavaScript principles, most concepts remain valid, but TypeScript’s features can modify or enhance certain principles.\nThere are the JavaScript principles that are affected.\nEncapsulation, Privacy and Minimize this Context TypeScript provides private and protected access modifiers for class fields and methods, enabling true encapsulation without relying on JavaScript-specific patterns like closures or the # syntax for private fields, and making it easier to manage privacy without relying heavily on this. Additionally, using arrow functions and bound functions in TYpeScript classes can help avoid unexpected this binding, especially in callbacks.\nThe impact is that it makes encapsulation more intuitive and aligned with other OOP languages. You can avoid closures or the module pattern for encapsulation within classes, as TypeScript’s access modifiers give more straightforward privacy control. Additionally, you can more easily avoid this in context where it’s not needed or expected. Access modifiers and arrow functions reduce reliance of this for accessing private or protected members, which can simplify code and make it easier to follow.\nExample of a TypeScript class:\nclass BankAccount { private balance: number = 0; // private access modifier deposit(amount: number) { if (amount \u003e 0) this.balance += amount; } getBalance(): number { return this.balance; } } Type Safety and const Usage TypeScript enforces types, meaning that developers don’t need to rely as heavily on const to prevent reassignment. The type system ensures that values of specific types remain consistent throughout the code.\nThe impact is that the use of const for immutability is still recommended, but sightly less critical for type safety, as TypeScript will enforce types at compile time.\nAvoiding Global Scope Pollution **TypeScript has its own module system and strict scoping, which naturally avoids global pollution bu requiring explicit imports and exports.\nThe impact is that the emphasis on avoiding global scope pollution is less relevant in TypeScript, as it enforces a module-based approach. Each file is treated as a module by default, so variables and functions aren’t accidentally exposed globally.\nType Safety and Immutability TypeScript provides the readonly keyword for properties. It adds a layer of immutability by preventing properties from being modified after initialization. This is especially helpful in enforcing immutability for objects and class properties.\nThe impact is that the immutability is still important, but TypeScript’s readonly keyword gives more control over immutability directly, reducing the need to use libraries or additional patterns to enforce immutability manually.\nExample:\nclass User { readonly name: string; constructor(name: string) { this.name = name; } } Error Handling and Fail Fast TypeScript’s type checking at compile time reduces certain runtime errors, enforcing stricter code that prevents many errors, enforcing stricter code that prevents many errors before they happen. Additionally, TypeScript modules automatically run in strict mode, enforcing safer coding by default\nThe impact is that while fail fast is still a good principle, TypeScript’s static typing minimizes runtime errors, especially those related to type mismatches or missing properties. This reduces reliance on try/catch for type errors, though runtime errors (like network errors) still require handling.\nJavaScript vs TypeScript example:\n// javascript version with type error handling function divide(a, b) { try { if (typeof a !== 'number' || typeof b !== 'number') { throw new Error(\"Both arguments must be numbers\"); } if (b === 0) { throw new Error(\"Cannot divide by zero\"); } return a / b; } catch (error) { console.error(\"Error:\", error.message); } } // Usage console.log(divide(10, 0)); // Error: Cannot divide by zero console.log(divide(\"10\", 2)); // Error: Both arguments must be numbers // typescript version with no type error handling function divide(a: number, b: number): number | void { try { if (b === 0) { throw new Error(\"Cannot divide by zero\"); } return a / b; } catch (error) { if (error instanceof Error) { console.error(\"Error:\", error.message); } } } // Usage console.log(divide(10, 0)); // Error: Cannot divide by zero console.log(divide(10, 2)); // 5 Documentation and Readability TypeScript types provide a level of self-documentation, as types clarify what a function, variable, or class is expected to handle. For example, seeing getUser(id: number): User clearly communicates what getUser does.\nThe impact is that this principle still relevant, but enums and TypeScript’s type checking make it easier to avoid magic numbers and use constants consistently.\nExample:\nenum Status { Active = 1, Inactive = 0, } Conclusion Most JavaScript principles remain valid in TypeScript, but TypeScript’s features like private, readonly, modules, enums and static typing improve and simplify many best practices. With TypeScript, code can be more organized, self-documenting and type-safe, which can reduce reliance on specific patterns used in pure JavaScript.",
    "description": "Best Practices Use const and let, avoid using var Write Modular, Reusable Code Use Arrow Functions (=\u003e) Consistent Naming Conventions Error Handling Use Template Literals for String Concatenation Comment and Document the Code Prefer Destructuring for Objects and Arrays Use Promises and async/await for Asynchronous Code Use Strict Mode Write Unit Tests Use ESLint and Prettier for Code Quality and Formatting Avoid Global Variables Optimize Performance Minimize Use of this Context Avoid Deeply Nested Code Use Default Parameters Write Clean and Readable Code Principles Single Responsibility Principle (SRP) Don’t Repeat Yourself (DRY) Keep it simple, Stupid (KISS) Encapsulation Separation of Concerns (SoC) Modularity Favor Composition Over Inheritance What does TypeScript fix? Encapsulation, Privacy and Minimize this Context Type Safety and const Usage Avoiding Global Scope Pollution Type Safety and Immutability Error Handling and Fail Fast Documentation and Readability Conclusion It is easy to fall in a trap when coding in JavaScript. Thus, it is highly suggested to add types and use Typescript. Nevertheless, there are the Best Practices and Principles to follow with Javascript. Many can be used with other languages like Typescript.",
    "tags": [],
    "title": "JavaScript Best Practices and Principles",
    "uri": "/notes/javascript-best-practices/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "@ToString The Class @ToString tag will replace the Overriding ToString method automatically.\n@AllArgsConstructor and @NoArgsConstructor The Class @AllArgsConstructor and @NoArgsConstructor tags will automatically replace the constructors.\n@EqualsAndHashCode The Class @EqualsAndHashCode tag will allow us to compare 2 objects.\n@Log4J The Class @Log4J tag can replace the logger initiator in the class. The LOGGER can be replaced by log.\n@Data The Class @Data tag will replace the @ToString, @RequiredArgsConstructor, @EqualsAndHashCode, @Setter (for non-final attributes) and the @Getter.",
    "description": "@ToString The Class @ToString tag will replace the Overriding ToString method automatically.\n@AllArgsConstructor and @NoArgsConstructor The Class @AllArgsConstructor and @NoArgsConstructor tags will automatically replace the constructors.\n@EqualsAndHashCode The Class @EqualsAndHashCode tag will allow us to compare 2 objects.\n@Log4J The Class @Log4J tag can replace the logger initiator in the class. The LOGGER can be replaced by log.\n@Data The Class @Data tag will replace the @ToString, @RequiredArgsConstructor, @EqualsAndHashCode, @Setter (for non-final attributes) and the @Getter.",
    "tags": [],
    "title": "Lombok Java Library",
    "uri": "/notes/java-lombok/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "MongoDB Create a doc Insert to a collection Show documents Searching using regex Sort, limit and skip Operators and arrays Updating a document Update one field Add one field Remove one field Increment a counter by N updating an array of a document Delete a document MongoDB show dbs show collections\njavascript shell\nuse \u003cdb name\u003e db.getName() -\u003e will return current DB Create a doc doc = {\"key\":value, ...} Insert to a collection db.\u003ccollection\u003e.insertOne(doc) Show documents db.\u003ccollection\u003e.find() db.\u003ccollection\u003e.find().pretty() -\u003e json file formated db.\u003ccollection\u003e.find({}, {\"title\": 1}) -\u003e will return only the title for all documents db.\u003ccollection\u003e.find({\"title\": \"tacos\"}) -\u003e will return all documents that match the condiction, ex. “title”:“tacos” Searching using regex db.\u003ccollection\u003e.find(\"title\": {$regex: /taco/i}}, {title: 1}) -\u003e returns the title for all documents that title march regex /taco/i\nSort, limit and skip db.\u003ccollection\u003e.find().count() db.\u003ccollection\u003e.find({}, {\"title\": 1}).sort({\"title\": 1}) where 1 for asc and -1 for desc db.\u003ccollection\u003e.find().limit(2) -\u003e returns 2 results only db.\u003ccollection\u003e.find({}, {\"title\": 1}).skip(1) -\u003e will return the items skipping first one Operators and arrays greater than: $gt less than: $lt less than or equal to: $lte db.\u003ccollection\u003e.find({ \"cook_time\": { $lte: 30 }}, {\"title\":1}) -\u003e find items with cook_time \u003c= 30 db.\u003ccollection\u003e.find({ \"cook_time\": { $lte: 30 }, \"prep_time\": { $lte: 10 } }, {\"title\":1}) -\u003e find items with cook_time \u003c= 30 AND prep_time \u003c= 10 db.\u003ccollection\u003e.find( { $or: [{ \"cook_time\": { $lte: 30 }, \"prep_time\": { $lte: 10 } }]}, {\"title\":1}) -\u003e find items with cook_time \u003c= 30 OR prep_time \u003c= 10 $all operator: require all items in a doc arrays $in operator: require one from the arrays db.\u003ccollection\u003e.find( { \"tags\" : {$all: [\"easy\", \"quick\"]}}, {\"title\":1, \"tag\":1}) -\u003e returns item with both tags “easy” or “quick” db.\u003ccollection\u003e.find( { \"tags\" : {$in: [\"easy\", \"quick\"]}}, {\"title\":1, \"tag\":1}) -\u003e returns item with either tags “easy” or “quick” Updating a document $set $unset $inc -\u003e increment Update one field db.\u003ccollection\u003e.updateOne(find, modification) db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $set: {\"title\":\"thin crust pizza\"}}) Add one field db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $set: {\"vegan\":true}}) Remove one field db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $unset: {\"vegan\":1}}) -\u003e one for true?!?! Increment a counter by N db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $inc: {\"likes_count\":1}}) -\u003e increment likes_count field by 1 updating an array of a document db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $push: {\"likes\":60}}); -\u003e will add value 60 to the likes array db.\u003ccollection\u003e.updateOne({\"title\": \"pizza\"}, { $pull: {\"likes\":60}}); -\u003e will remove value 60 from the likes array Delete a document db.\u003ccollection\u003e.deleteOne({\"_id\":\"....\"})",
    "description": "MongoDB Create a doc Insert to a collection Show documents Searching using regex Sort, limit and skip Operators and arrays Updating a document Update one field Add one field Remove one field Increment a counter by N updating an array of a document Delete a document MongoDB show dbs show collections\njavascript shell\nuse \u003cdb name\u003e db.getName() -\u003e will return current DB Create a doc doc = {\"key\":value, ...} Insert to a collection db.\u003ccollection\u003e.insertOne(doc) Show documents db.\u003ccollection\u003e.find() db.\u003ccollection\u003e.find().pretty() -\u003e json file formated db.\u003ccollection\u003e.find({}, {\"title\": 1}) -\u003e will return only the title for all documents db.\u003ccollection\u003e.find({\"title\": \"tacos\"}) -\u003e will return all documents that match the condiction, ex. “title”:“tacos” Searching using regex db.\u003ccollection\u003e.find(\"title\": {$regex: /taco/i}}, {title: 1}) -\u003e returns the title for all documents that title march regex /taco/i",
    "tags": [],
    "title": "MongoDB",
    "uri": "/notes/mongodb/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Python and PyTest PyTest Why PyTest? requirements.txt content for pytest Run a test pytest.ini configuration file example Tox tox.ini Test file skeleton Test class skeleton Fixtures Python Class file skeleton Commonly used functions and examples Python and PyTest Working with multiple python versions may cause issues and confusion. It is recommended to specify the python version to use.\nWorking with python 3.9:\npy -3.9 -m \u003ccommand\u003e It is recommended to upgrade the pip version:\npy -3.9 -m pip install --upgrade pip It is recommended to install tox, which is a virtual environment (venv) manager for Python.\npy -3.9 -m pip install tox To run tests with tox, the following example assumes that pytest has been configured by the tox.ini commands parameter:\npy -3.9 -m tox --recursive -- \u003cpytest parameters\u003e PyTest Why PyTest? allow to run a standalone test function as its own case easy to read syntax, allowing you to use the standard assert method powerful CLI automates test setup, teardown, and common test scenarios (uses fixtures) Great to use with CI tools like Jenkins, Travis, Circle CI, etc. actively maintained with participatory open-source community requirements.txt content for pytest coverage===xxx pytest===xxx pytest-cov===xxx pytest-flakes===xxx pytest-pep8===xxx pytest-pythonpath==xxx docker pytest-flakes will make pytest use PyFlake and Flake8. It will make pytest and python use a Linter and code style checker. Run a test Get pytest help:\npytest -h Run all tests:\npytest Run test using keyword for filename\npytest -k \u003ctest_name_keyword\u003e Explore test coverage of a script - requires the pytest-cov package to be installed:\npytest --cov scripts pytest.ini configuration file example [pytest] # Configure the logging within PyTest (the configuration won't work if configured in test files) log_cli = 1 log_cli_level = WARNING log_cli_format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s) log_cli_date_format=%Y-%m-%d %H:%M:%S # Filter Warnings filterwarnings = ignore::FutureWarning # Uses classes with prefix Test as test files python_files = Test*.py Tox Tox aims to automate and standardize testing in Python. It is part of a larger vision of easing the packaging, testing and release process of Python software.\ntox.ini Tox may be used along with a tox.ini configuration file.\n[framework] files = tests coverages = --cov=src [tox] envlist = py3.9 skip_missing_interpreters = false skipsdist = true toxworkdir = tmp [flake8] max-line-length = 159 # E501: Ignore max line length # ignore = E501 [pytest] norecursedirs = .cache tmp # pytest-spec configuration spec_header_format = {module_path}: spec_test_format = {result} {name} [testenv] deps = -rrequirements.txt # Only forward the environment variables with the following prefix. passenv = PYTHON_SANDBOX_* commands = flake8 src tests --exclude=__init__.py pytest -p no:cacheprovider --spec --durations=5 --cov-config .coveragerc --cov-report term-missing {posargs} {[framework]coverages} {[framework]files} Test file skeleton from path.to.class.to.test import ClassName from pytest # to use pytest Context Manager def test_name(): obj = ClassName(\"value1\", \"value2\") assert obj.value1 == \"value1\" assert obj.value2 == \"value2\" def test_with_exception_context_manager(): with pytest.raises(ValueError) as ex: # Code that will raise an exception. obj = ClassName(\"value1\", \"value2\") obj.raises_exception() assert str(ex.value) == \"this is an exception!\" Test class skeleton The Test Class will work like the fixtures from the test file.\n# conftest.py import pytest import logging @pytest.fixture(scope=\"session\", autouse=True) def set_logging() -\u003e None: logging.info(\"set_logging on conftest.py\") # TestExample.py import logging class TestExample: @classmethod def setup_class(cls): logging.info(\"starting class: {} execution\".format(cls.__name__)) @classmethod def teardown_class(cls): logging.info(\"starting class: {} execution\".format(cls.__name__)) def setup_method(self, method): logging.info(\"starting execution of tc: {}\".format(method.__name__)) def teardown_method(self, method): logging.info(\"starting execution of tc: {}\".format(method.__name__)) def test_tc1(self): logging.info(\"running tc1\") assert True def test_tc2(self): logging.info(\"running tc2\") assert True The output of this example Test Class will be:\n============================= test session starts ============================= collecting ... collected 2 items TestExample.py::TestExample::test_tc1 ------------------------------- live log setup -------------------------------- 2022-07-18 12:57:45 [ INFO] set_logging on conftest.py (conftest.py:7) 2022-07-18 12:57:45 [ INFO] starting class: TestExample execution (TestExample.py:7) 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc1 (TestExample.py:14) -------------------------------- live log call -------------------------------- 2022-07-18 12:57:45 [ INFO] running tc1 (TestExample.py:20) PASSED [ 50%] ------------------------------ live log teardown ------------------------------ 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc1 (TestExample.py:17) TestExample.py::TestExample::test_tc2 ------------------------------- live log setup -------------------------------- 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc2 (TestExample.py:14) -------------------------------- live log call -------------------------------- 2022-07-18 12:57:45 [ INFO] running tc2 (TestExample.py:24) PASSED [100%] ------------------------------ live log teardown ------------------------------ 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc2 (TestExample.py:17) 2022-07-18 12:57:45 [ INFO] starting class: TestExample execution (TestExample.py:11) ============================== 2 passed in 0.02s ============================== Process finished with exit code 0 A session configuration can be done in the conftest.py as shown in the example. It will be run once only in the session setup.\nFixtures A test fixture is a concept used in both electronics and software. It’s a piece of software or device that sets up a system to satisfy certain preconditions of the process. Its biggest advantage is that it provides consistent results so that the test results can be repeatable. Examples of fixtures could be loading a test set to the database, reading a configuration file, setting up environment variables, etc.\nA pytest fixture has a specific scope. By default, the scope is a function. Pytest fixtures have five different scopes: function, class, module, package, and session. The scope basically controls how often each fixture will be executed.\nOrder of priority:\nsession (higher priority) package module class function (lower priority) Function THe default scope is function: `scope=“function” and therefore may be omitted.\nimport pytest from datetime import datetime @pytest.fixture() def only_used_once(): with open(\"app.json\") as f: config = json.load(f) return config @pytest.fixture() def light_operation(): return \"I'm a constant\" @pytest.fixture() def need_different_value_each_time(): return datetime.now() Class The scope=\"class\" run before any function or test of the Test Class.\n@pytest.fixture(scope=\"class\") def dummy_data(request): request.cls.num1 = 10 request.cls.num2 = 20 logging.info(\"Execute fixture\") @pytest.mark.usefixtures(\"dummy_data\") class TestCalculatorClass: def test_distance(self): logging.info(\"Test distance function\") assert distance(self.num1, self.num2) == 10 def test_sum_of_square(self): logging.info(\"Test sum of square function\") assert sum_of_square(self.num1, self.num2) == 500 Special usage of the keyword yield in a fixture: the code before the yield keyword will be executed before the test functions of the Test Class while the code after the yield keyword will be executed after the test functions of the Test Class.\n@pytest.fixture(scope=\"class\") def prepare_db(request): # pseudo code connection = db.create_connection() request.cls.connection = connection yield connection = db.close() @pytest.mark.usefixtures(\"prepare_db\") class TestDBClass: def test_query1(self): assert self.connection.execute(\"..\") == \"...\" def test_query2(self): assert self.connection.execute(\"..\") == \"...\" Module and package The scope=\"module\" runs the fixture per module while the scope=\"package\" runs by package. The scope module is usually used more often than the scope package. The difference between scope function and scope module is that the scope module will only be run once, even if used in many functions in the module.\n@pytest.fixture(scope=\"module\") def read_config(): with open(\"app.json\") as f: config = json.load(f) logging.info(\"Read config\") return config def test1(read_config): logging.info(\"Test function 1\") assert read_config == {} def test2(read_config): logging.info(\"Test function 2\") assert read_config == {} Session The scope=\"session\" is only run once every time pytest is run. A per-directory conftest.py file will be executed once per pytest execution.\n# test/conftest.py @pytest.fixture(scope=\"session\") def read_config(): with open(\"app.json\") as f: config = json.load(f) logging.info(\"Read config\") return config # test/test_code1.py def test1(read_config): logging.info(\"Test function 1\") assert read_config == {} def test2(read_config): logging.info(\"Test function 2\") assert read_config == {} # test/test_code2.py def test3(read_config): logging.info(\"Test function 3\") assert read_config == {} def test4(read_config): logging.info(\"Test function 4\") assert read_config == {} Using conftest.py for common functions:\nstores common utility test fixtures and extension code often referred to as hooks pytest collects the fixtures in this file so they are globally accessible within the testing directory it must be placed under your /tests directory good practice to cross-reference this file when reading a testing suite conftest.py modularization It is possible to modularize the conftest.py file when it is getting too big.\n# referring to modules: # tests/utils/db.py # tests/utils/network.py # in conftest.py pytest_plugins = [ \"tests.utils.db\", \"tests.utils.network\" ] The autouse=True fixtures must stay in the conftest.py file.\nAutouse The fixture parameter autouse=True will make the fixture used automatically even if the fixture isn’t called by the test function.\n@pytest.fixture(autouse=True) def function_autouse(): logging.info(\"scope function with autouse\") def test_autouse(): assert True Parametrize The [pytest.mark.parametrize] fixture allows the user to run the same test, multiple times, by modifying the input parameters.\n@pytest.mark.parametrize(\"num, output\",[(1,11),(2,22),(3,35),(4,44)]) def test_multiplication_11(num, output): assert 11*num == output Python Class file skeleton class ClassName(): \"\"\" This is a multi-line comment (used for header in this example) This is a second line of comment.. \"\"\" def __init__(self, var1: str, var2: str): # the \"var1: str\" format will require the var1 to be a string type self._var1 = var1 # the _ is to make the variable \"protected\" self._var2 = var2 @property # Using property decorator as a getter function def var1(self) -\u003e str: # the -\u003e specifies the return type return self._var1 @var1.setter # using setter decorator for setter function def var1(self, value: str): self.var1 = value def raises_exception(): raise ValueError(\"this is an exception!\") It is possible to use decorators for getters and setters.\nCommonly used functions and examples Verify variable type if not isinstance(variable, str): raise ValueError(\"wrong type!\") Open file with context manager with open(\"test.txt\", 'w', encoding='utf-8') as f: f.write(\"my first file\\n\") f.write(\"This file\\n\\n\") f.write(\"contains three lines\\n\") Iterate a list data = [\"abc\", \"def\", \"ghi\"] for each_data in data: assert each_data in data try except try: # Some Code except: # Executed if error in the # try block else: # execute if no exception finally: # Some code .....(always executed)",
    "description": "Python and PyTest PyTest Why PyTest? requirements.txt content for pytest Run a test pytest.ini configuration file example Tox tox.ini Test file skeleton Test class skeleton Fixtures Python Class file skeleton Commonly used functions and examples Python and PyTest Working with multiple python versions may cause issues and confusion. It is recommended to specify the python version to use.\nWorking with python 3.9:\npy -3.9 -m \u003ccommand\u003e It is recommended to upgrade the pip version:",
    "tags": [],
    "title": "Python and PyTest",
    "uri": "/notes/python-and-pytest/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "Role Strategic Tactics Role The architect is responsible for two main types of high-level activities:\nStrategic (foundations): take care of the technical vision in the medium to long term (one, two years or even more) Tactics (operations): technical support for teams, problem solving, proof of concept, etc. Strategic Define the “Quality Vision”, guidelines, technical documents (ADR, technical designs, terminology/technical glossary, etc.) while taking into account security rules, PII, etc Evaluate technologies: research, comparisons, proof of concepts, publications, etc Promote best practices for the chosen technologies Mentoring and coaching Promote quality during the six stages of software development, SDLC or “Software Development Life Cycle”: “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” - \u003e “Deployment” Assist or lead working groups for specific topics, define the purpose and deliverables, periodic and monthly events Attend or lead workshops for specific topics, define the goal and deliverables, event over a few hours or a few days Audit of the services for test coverage and correlate the data with the problematic services in production (Data-Driven decisions) Analyze the current testing strategy, test characterization, define a plan, promote good testing practices, follow the test pyramid: Unit (solitary and sociable) testing Component (in-process and out-of-process) testing Contract testing Integration (narrow and broad) testing UI End-to-end (E2E) and API E2E (Subcutaneous) User journey tests (and Synthetic Traffic tests) Non-functional tests: load, scalability, webvital metrics LCP, chaos/robustness, etc Alignment on a test strategy with the deployment procedure (CI/CD) and a branching procedure (gitflow, githubflow, etc), blue/green deployment, etc Participates in the recruitment process for the “quality” aspects Tactics Apply the test strategy: develop test projects, add the projects to the process (build, CI/CD, cluster/kubernetes. etc), document, etc Support test development: support QAs and developers who add tests, PR reviews, etc Add test coverage: JavaScript (e.g., cypress.io, etc), python/pytest, java/rest-assured, etc Addition of Synthetic Traffic tests and verification of customer flows and scenarios (i.e., customer flows) Analyze system performance (e.g., load tests) and according to defined standards (i.e., “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (i.e., Largest Contentful Paint) (e.g., “a page should be considered usable in less than 2.5 seconds”) Using observation tools to understand issues: datadoghq, splunk/signalfx, prometheus, ​​grafana, etc Collaborates horizontally with the different “domaines” of the company and suppliers to improve the overall quality of the company",
    "description": "Role Strategic Tactics Role The architect is responsible for two main types of high-level activities:\nStrategic (foundations): take care of the technical vision in the medium to long term (one, two years or even more) Tactics (operations): technical support for teams, problem solving, proof of concept, etc. Strategic Define the “Quality Vision”, guidelines, technical documents (ADR, technical designs, terminology/technical glossary, etc.) while taking into account security rules, PII, etc Evaluate technologies: research, comparisons, proof of concepts, publications, etc Promote best practices for the chosen technologies Mentoring and coaching Promote quality during the six stages of software development, SDLC or “Software Development Life Cycle”: “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” - \u003e “Deployment” Assist or lead working groups for specific topics, define the purpose and deliverables, periodic and monthly events Attend or lead workshops for specific topics, define the goal and deliverables, event over a few hours or a few days Audit of the services for test coverage and correlate the data with the problematic services in production (Data-Driven decisions) Analyze the current testing strategy, test characterization, define a plan, promote good testing practices, follow the test pyramid: Unit (solitary and sociable) testing Component (in-process and out-of-process) testing Contract testing Integration (narrow and broad) testing UI End-to-end (E2E) and API E2E (Subcutaneous) User journey tests (and Synthetic Traffic tests) Non-functional tests: load, scalability, webvital metrics LCP, chaos/robustness, etc Alignment on a test strategy with the deployment procedure (CI/CD) and a branching procedure (gitflow, githubflow, etc), blue/green deployment, etc Participates in the recruitment process for the “quality” aspects Tactics Apply the test strategy: develop test projects, add the projects to the process (build, CI/CD, cluster/kubernetes. etc), document, etc Support test development: support QAs and developers who add tests, PR reviews, etc Add test coverage: JavaScript (e.g., cypress.io, etc), python/pytest, java/rest-assured, etc Addition of Synthetic Traffic tests and verification of customer flows and scenarios (i.e., customer flows) Analyze system performance (e.g., load tests) and according to defined standards (i.e., “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (i.e., Largest Contentful Paint) (e.g., “a page should be considered usable in less than 2.5 seconds”) Using observation tools to understand issues: datadoghq, splunk/signalfx, prometheus, ​​grafana, etc Collaborates horizontally with the different “domaines” of the company and suppliers to improve the overall quality of the company",
    "tags": [],
    "title": "Quality Architect Role",
    "uri": "/blogs/quality-architect-role/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Blogs",
    "content": "Role Stratégique Tactique Role L’architecte est responsable de deux grands types d’activités à haut niveau:\nStratégique (fondations): s’occuper de la vision technique à moyen à long terme (un, deux ans ou même plus) ; Tactique (opérations): support technique des équipes, résolution de problèmes, preuve de concept, etc ; Stratégique Définir la “Vision Qualité”, les guides (ou “guidelines”), documents techniques (ADR, Technical Design, Terminologie/glossaire technique, etc.) tout en tenant compte des règles de sécuritées, PII, etc. ; Évaluer les technologies : Recherches, comparatifs, preuves de concepts, publications, etc.; Promouvoir les bonnes pratiques pour les technologies choisies; Mentorat et “Coaching”; Promouvoir la qualité lors des six étapes du développement logiciel, SDLC ou “Software Development Life Cycle” : “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” -\u003e “Deployment” ; Assister ou diriger des groupes de travail (ou “Work Groups”) pour des sujets précis, définir le but et les livrables, événements périodiques et sur plusieurs mois ; Assister ou diriger des ateliers de travail (ou “Workshops”) pour des sujets précis, définir le but et les livrables, événement sur quelques heures ou quelques jours ; Audit de la couverture de tests et corrélation des données avec les services problématiques en production (décision basée sur les données ou “Data-Driven”); Analyser la stratégie de tests actuelle, caractérisation des tests, définir un plan, promouvoir les bonnes pratiques de tests, la pyramide de tests “pratique” (en anglais pour faciliter) : Unit (“solitary” et “sociable”) testing ; Component (“in-process” et “out-of-process”) testing ; Contract testing ; Integration (“narrow” et “broad”) testing ; UI End-to-end (E2E) et API E2E (ou Subcutaneous) ; User journey tests (and Synthetic Traffic tests) ; Non functional tests : load, scalability, webvital metrics LCP, chaos/robustness, etc. ; Création de projets archétypes et création de matériel de formation (on-demand training) suivi de projets pilotes ou réels, écriture de tests, documentation, etc. ; Alignment sur une stratégie de tests avec la procédure de déploiement (CI/CD) avec une procédure de “branching” (examples : gitflow ou trunk-based workflows), blue/green deployment, smoke tests, etc. ; Participe au processus de recrutement pour le volet “qualité” ; Tactique Appliquer la stratégie de tests : développer des projets tests, ajouter les projets au processus (build, CI/CD, cluster/kubernetes. etc), documenter, etc. ; Supporter le développement de tests: supporter les QAs et développeurs qui ajoutent des tests, PR reviews, etc. ; Ajouter une couverture de tests : javascript (exemple : cypress.io), python/pytest, java/rest-assured, etc. ; Ajout de “tests de trafic synthétique” et vérification des flux et scénarios clients (ou “customer flows”) ; Analyser les performances systèmes (example: load tests) et selon les standards définis (example: “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (ou “Largest Contentful Paint”, example : “une page devrait être considérée utilisable en moins de 2.5 secondes”) ; Utilisation d’outils d’observation pour comprendre les problèmes : datadoghq, splunk/signalfx, prometheus, grafana, etc. ; Collabore horizontalement avec les différents “domaines” de l’entreprises et des fournisseurs pour améliorer la qualité globale de l’entreprise ;",
    "description": "Role Stratégique Tactique Role L’architecte est responsable de deux grands types d’activités à haut niveau:\nStratégique (fondations): s’occuper de la vision technique à moyen à long terme (un, deux ans ou même plus) ; Tactique (opérations): support technique des équipes, résolution de problèmes, preuve de concept, etc ; Stratégique Définir la “Vision Qualité”, les guides (ou “guidelines”), documents techniques (ADR, Technical Design, Terminologie/glossaire technique, etc.) tout en tenant compte des règles de sécuritées, PII, etc. ; Évaluer les technologies : Recherches, comparatifs, preuves de concepts, publications, etc.; Promouvoir les bonnes pratiques pour les technologies choisies; Mentorat et “Coaching”; Promouvoir la qualité lors des six étapes du développement logiciel, SDLC ou “Software Development Life Cycle” : “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” -\u003e “Deployment” ; Assister ou diriger des groupes de travail (ou “Work Groups”) pour des sujets précis, définir le but et les livrables, événements périodiques et sur plusieurs mois ; Assister ou diriger des ateliers de travail (ou “Workshops”) pour des sujets précis, définir le but et les livrables, événement sur quelques heures ou quelques jours ; Audit de la couverture de tests et corrélation des données avec les services problématiques en production (décision basée sur les données ou “Data-Driven”); Analyser la stratégie de tests actuelle, caractérisation des tests, définir un plan, promouvoir les bonnes pratiques de tests, la pyramide de tests “pratique” (en anglais pour faciliter) : Unit (“solitary” et “sociable”) testing ; Component (“in-process” et “out-of-process”) testing ; Contract testing ; Integration (“narrow” et “broad”) testing ; UI End-to-end (E2E) et API E2E (ou Subcutaneous) ; User journey tests (and Synthetic Traffic tests) ; Non functional tests : load, scalability, webvital metrics LCP, chaos/robustness, etc. ; Création de projets archétypes et création de matériel de formation (on-demand training) suivi de projets pilotes ou réels, écriture de tests, documentation, etc. ; Alignment sur une stratégie de tests avec la procédure de déploiement (CI/CD) avec une procédure de “branching” (examples : gitflow ou trunk-based workflows), blue/green deployment, smoke tests, etc. ; Participe au processus de recrutement pour le volet “qualité” ; Tactique Appliquer la stratégie de tests : développer des projets tests, ajouter les projets au processus (build, CI/CD, cluster/kubernetes. etc), documenter, etc. ; Supporter le développement de tests: supporter les QAs et développeurs qui ajoutent des tests, PR reviews, etc. ; Ajouter une couverture de tests : javascript (exemple : cypress.io), python/pytest, java/rest-assured, etc. ; Ajout de “tests de trafic synthétique” et vérification des flux et scénarios clients (ou “customer flows”) ; Analyser les performances systèmes (example: load tests) et selon les standards définis (example: “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (ou “Largest Contentful Paint”, example : “une page devrait être considérée utilisable en moins de 2.5 secondes”) ; Utilisation d’outils d’observation pour comprendre les problèmes : datadoghq, splunk/signalfx, prometheus, grafana, etc. ; Collabore horizontalement avec les différents “domaines” de l’entreprises et des fournisseurs pour améliorer la qualité globale de l’entreprise ;",
    "tags": [],
    "title": "Rôle de l'Architecte Qualité (FR)",
    "uri": "/blogs/role-architecte-qualite/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  },
  {
    "breadcrumb": "Alain Bouchard's Engineering Notes and Blogs \u003e My Notes",
    "content": "Node.JS Installation Quick Overview Initializing TypeScript Project Setting TypeScript Configuration File Searching and Importing Open Source Defined Types Types Primitive and Built-in Types Creating Custom Types Using Interface Creating Custom Types Using Aliases Enumerable Types Typing a Function Using Specific Types in a Function Defining a Megatype using Generics More Complex Types Using Aliases for Complex Types Definition Working with Record Type Resource Management with using Keyword Extending and modifying existing types The Partial helper type The Omit helper type The Pick helper type The Required helper type Decorators Decorator with no arguments Decorator with no arguments Using modules in typescript Using the globals.d.ts file The declare global {} statement Using or not using the declare global {} statement? TypeScript JavaScript and TypeScript are two popular programming languages for developing web applications. JavaScript is a simple and versatile language that supports dynamic typing. TypeScript extends JavaScript by adding static typing and features like interfaces, enums, and advanced type-checking.\nNode.JS Installation Quick Overview Get current node version:\n\u003e node -v v22.9.0 Installing Node.js using brew:\n\u003e brew install node Initializing TypeScript Project Installing typescript using npm at the project level:\n\u003e npm install typescript --save-dev \u003e ls -1 node_modules package-lock.json package.json \u003e npx tsc -v Version 5.6.3 \u003e cat package.json { \"devDependencies\": { \"typescript\": \"^5.6.3\" } } Setting TypeScript Configuration File A configuration file tsconfig.json is required at the Project root directory:\ntsconfig.json:\n{ \"compilerOptions\": { \"outDir\": \"build\", \"target\": \"ES6\" , \"noEmit\": false, \"experimentalDecorators\": true, \"emitDecoratorMetadata\": true }, \"include\": [\"src/**/*\"] } Details:\ninclude: the directories to search the TypeScript files from compilerOptions.target: the ECMAScript version compatibility compilerOptions.outDir: the built files directory, or where tsc will drop the generated js files compilerOptions.noEmit: true or false, if the js files should be generated. The true value will make tsc to only check the files without generating any js file. compilerOptions.experimentalDecorators: true or false, if the then the decorators (experimental) feature will be enabled. compilerOptions.emitDecoratorMetadata: true or false, This library implements polyfills for another set of proposed ECMAScript features (experimental), requires the reflect-metadata library to be installed : npm i reflect-metadata --save. Searching and Importing Open Source Defined Types All defined types can be found in [DefinitelyTypes] github repository, or more easily on [npnjs.com] package locator:\nFor example, search for @types jquery within npmjs.com search field.\nThe installation command will be:\nnpm install --save @types/jquery The imported library will be located in node_modules/@type directory.\nTypes Primitive and Built-in Types By default, Javascript will infer the variable type. For example, this will create a number type by default:\nlet x = 5 However, TypeScript won’t infer anymore since variable type must be defined:\nlet x: number let y: string let z: boolean let a: Date let b: string[] let c: any The any type will be automatically inferred.\nIt is possible to use casting.\nlet b: string[] b = \"Hello World!\" as any Using as any is however avoiding the whole TypeScript purpose.\nCreating Custom Types Using Interface It is possible to create custom types in TypeScript by using interface, for example:\ninterface Contact { id: number; name: string; birthDate?: Date; // Optional since ? was added } let contact1: Contact; let contact2: Contract = { birthDate: new Date(\"01-01-1999\"), id: 1234, name: \"John Snow\" } let contact3: Contract = { id: 1234, name: \"John Snow\" } When a ? is added to the interface field then it will make this field optional.\nIt is possible to extend an actual interface, for example:\ninterface Contact extend Address { id: number; name: string; birthDate?: Date; // Optional since ? was added } interface Address { line1: string; line2: string; province: string; region: string; postalCode: string; } let contact1: Contract = { birthDate: new Date(\"01-01-1999\"), id: 1234, name: \"John Snow\" postalCode: \"H0H0H0\" ... } Creating Custom Types Using Aliases An Alias is only a replacement name for an existing type, for example:\ntype ContactName = string So ContactName is in reality a string, and both are now interchangeable.\nEnumerable Types Enums allow a developer to define a set of named constants, for example:\nenum ContactStatus { Active = \"active\", // assigned value \"active\" is optional Inactive = \"inactive\", New = \"new\" } interface Contact { id: number; name: string; birthDate?: Date; status: ContactStatus; } let contact1: Contract = { birthDate: new Date(\"01-01-1999\"), id: 1234, name: \"John Snow\", status: ContactStatus.Active } Typing a Function This is a JavaScript function:\nfunction clone(source) { return Object.apply({}, source) // Apply will return an \"any\" type } const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone(contact1); // contact2's type will be \"any\" Using Specific Types in a Function This is the typed version in TypeScript:\nfunction clone(source: Contact): Contact { return Object.apply({}, source) // Apply will return an \"any\" type, but the function's returned type will be Contact } const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone(contact1); // contact2's type will be \"Contact\" Defining a Megatype using Generics function clone\u003cT\u003e(source: T): T { return Object.apply({}, source) } const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone(contact1); // clone() will work for any type, e.g. const dateRange = { startDate: Date.now(), endDate: Date.now() }; const dateRageCopy = clone(dateRange); It is possible to define multiple types using Generics, for example:\nfunction clone\u003cT1, T2\u003e(source: T1): T2 { return Object.apply({}, source) } // T1 and T2 must be specified at the function's call const contact1: Contact = {id: 1, name: \"John Do\"}; const contact2 = clone\u003cContact, Date\u003e(contact1); It is possible to use generics with an interface:\ninterface ExternalContact\u003cTExternalId\u003e { id: number; name: string; birthDate?: Date; externalId: TExternalId; loadExternalId(): Task\u003cTExternalId\u003e; } More Complex Types Using Aliases for Complex Types Definition It is possible to allow multiple types for one field, for example:\ninterface Contact { id: number; name: string; birthDate: Date | number | string; // all 3 types can be used } Now it is possible to use an alias, for example:\ntype ContactBirthDate = Date | number | string; interface Contact { id: number; name: string; birthDate: ; ContactBirthDate; // all 3 types can be used } Creating a new interface or custom type using an alias:\ninterface Contact { id: number; name: string; birthDate: Date; } interface Address { line1: string; line2: string; province: string; region: string; postalCode: string; } type AddressableContact = Contact \u0026 Address; Replacing an Enum type with an alias, for example:\nenum ContactStatus { Active = \"active\", Inactive = \"inactive\", New = \"new\" } let a: ContactStatus = ContactStatus.Active; // Can be replaced by type ContactStatus = \"active\" | \"inactive\" | \"new\"; let a: ContactStatus = \"active\"; Using keyof to get the available fields for a given type:\ninterface Contact { id: number; name: string; birthDate: Date; } type ContactFields = keyof Contact; function getValue(source, propertyName: ContactFields) { return source[propertyName] } // With a generic function getValue\u003cT\u003e(source, propertyName: keyof T) { return source[propertyName] } Working with Record Type In TypeScript, a Record is a utility type that allows you to map keys of a specific type to values of another type. It’s a concise way to define objects with a uniform key-value structure. Here’s a simple example:\n// Syntax: Record\u003cKeyType, ValueType\u003e // Example 1: Record with string keys and number values const scores: Record\u003cstring, number\u003e = { Alice: 85, Bob: 92, Charlie: 88, }; // Example 2: Record with string keys and boolean values const isActive: Record\u003cstring, boolean\u003e = { Alice: true, Bob: false, Charlie: true, }; Explanation:\nRecord\u003cstring, number\u003e: The keys are strings, and the values are numbers. So, each key (like “Alice”) must map to a number (like 85). Record\u003cstring, boolean\u003e: Here, the keys are also strings, but the values are booleans (true or false). It is possible to define a list of types, e.g.:\nconst scores: Record\u003cstring, number | string\u003e = { Alice: 85, Bob: 92, Charlie: \"88\", }; It is possible to limit the keys to a custom type using keyof, e.g.:\nconst contact1: Record\u003ckeyof Contact, number | string\u003e = { id: 85, name: \"Jon Snow\", }; Resource Management with using Keyword As of TypeScript 5.2, a new using keyword has been introduced to work similarly to C#’s using statement. This feature allows you to automatically dispose of resources when they are no longer needed, provided the resource implements a Disposable pattern.\nHere’s a small example of using this feature in TypeScript 5.2+\n// can safely omit the `implements Disposable` statement class Resource { constructor(private name: string) { console.log(`${this.name} is created`); } [Symbol.dispose]() { console.log(`${this.name} is disposed`); } use() { console.log(`Using ${this.name}`); } } function main() { using resource = new Resource(\"MyResource\"); resource.use(); } main(); Explanations:\nResource class: Implements [Symbol.dispose] to clean up resources. using keyword: Automatically calls Symbol.dispose when the block ends. In main(): Creates a resource, uses it, and automatically disposes it when done. The implements Disposable part is optional; the critical part is implementing the Symbol.dispose method for the using keyword to automatically dispose of the resource. Extending and modifying existing types The Partial helper type Partial is a TypeScript utility type that makes all properties of a given type T optional. This allows you to create objects where only some properties of the original type are required. This is an example:\ninterface User { name: string; age: number; email: string; } // Using Partial to make all fields optional const updateUser = (user: Partial\u003cUser\u003e) =\u003e { console.log(user); }; // Example usage updateUser({ name: \"Alice\" }); updateUser({ age: 30, email: \"alice@example.com\" }); Explanation:\nPartial\u003cUser\u003e: Makes all properties in the User interface optional. updateUser: Can accept an object with any subset of User properties (e.g., just name, age, or email). The Omit helper type Omit\u003cT, K\u003e is a TypeScript utility type that constructs a new type by removing the specified keys K from type T. This allows you to create a type without certain properties. This is an example:\ninterface User { name: string; age: number; email: string; address: string; } // Using Omit to exclude 'email' and 'address' const createBasicUser = (user: Omit\u003cUser, 'email' | 'address'\u003e) =\u003e { console.log(user); }; // Example usage createBasicUser({ name: \"Alice\", age: 30 }); Explanation:\nOmit\u003cUser, 'email' | 'address'\u003e: Excludes both email and address from the User type. createBasicUser: Accepts an object with only name and age, omitting email and address. The Pick helper type Pick\u003cT, K\u003e is a TypeScript utility type that creates a new type by selecting specific keys K from type T. It allows you to include only the desired properties. There is an example:\ninterface User { name: string; age: number; email: string; address: string; } // Using Pick to select only 'name' and 'email' const getUserContactInfo = (user: Pick\u003cUser, 'name' | 'email'\u003e) =\u003e { console.log(user); }; // Example usage getUserContactInfo({ name: \"Alice\", email: \"alice@example.com\" }); Explanation:\nPick\u003cUser, 'name' | 'email'\u003e: Selects only name and email from the User type. getUserContactInfo: Accepts an object with just name and email, ignoring other properties like age and address. The Required helper type Required\u003cT\u003e is a TypeScript utility type that makes all properties of a given type T required, removing any optional modifiers. This is an example:\ninterface User { name?: string; age?: number; } // Using Required to make all fields mandatory const createUser = (user: Required\u003cUser\u003e) =\u003e { console.log(user); }; // Example usage (now both 'name' and 'age' are required) createUser({ name: \"Alice\", age: 30 }); Explanation:\nRequired\u003cUser\u003e: Converts all optional properties in User to required. createUser: Now expects both name and age to be provided. Decorators A TypeScript decorator is a special kind of declaration that can be attached to classes, methods, properties, or parameters to modify their behavior. Decorators are essentially functions that take the target (like a class or method) as an argument and allow you to apply additional logic to it. They are commonly used for things like logging, validation, or adding metadata.\nDecorators are written with the @ symbol followed by the decorator function name and can be applied to:\nClasses Methods Properties Accessors Parameters To enable decorators in TypeScript, you need to add the experimentalDecorators option in your tsconfig.json.\nMore details may be found in the Decorators official documentation.\nDecorator with no arguments This is an example:\nfunction log(target: any, propertyKey: string, descriptor: PropertyDescriptor) { const originalMethod = descriptor.value; descriptor.value = function (...args: any[]) { console.log(`Method ${propertyKey} was called with args: ${args}`); return originalMethod.apply(this, args); }; } class Example { @log greet(name: string) { console.log(`Hello, ${name}!`); } } // Example usage const example = new Example(); example.greet(\"Alice\"); In this example, the @log decorator adds logging behavior to the greet method, printing when the method is called and the arguments passed to it.\nDecorator with no arguments This is an example:\nfunction logWithMessage(message: string) { return function (target: any, propertyKey: string, descriptor: PropertyDescriptor) { const originalMethod = descriptor.value; descriptor.value = function (...args: any[]) { console.log(`${message} - Method ${propertyKey} was called with args: ${args}`); return originalMethod.apply(this, args); }; }; } class Example { @logWithMessage(\"Custom log\") greet(name: string) { console.log(`Hello, ${name}!`); } } // Example usage const example = new Example(); example.greet(\"Alice\"); Explanation:\nlogWithMessage(message: string): A decorator factory that takes a message argument and returns a decorator function. @logWithMessage(\"Custom log\"): Applies the decorator with the custom log message to the greet method, enhancing it with additional logging behavior. Using modules in typescript It is recommended to work with modules. Basically, the exported functions and properties require the export keyword in front. The importing module require to define the imported functions and properties.\nThis is an example:\nfile: math.ts (exporting module)\nexport function add(a: number, b: number): number { return a + b; } export const PI = 3.14; file: app.ts (importing module)\nimport { add, PI } from './math'; console.log(add(2, 3)); // Output: 5 console.log(PI); // Output: 3.14 Explanation:\nmath.ts: Exports a function (add) and a constant (PI). app.ts: Imports the add function and PI constant from math.ts and uses them in the code. Using the globals.d.ts file The globals.d.ts file is a TypeScript declaration file used to define global types, interfaces, or variables that can be accessed throughout your project without imports. It’s useful for:\nDeclaring global variables. Extending global objects (e.g., Window). Sharing global types or interfaces. This is an example of globals.d.ts:\ndeclare const API_URL: string; interface Window { myCustomProperty: string; } type UserRole = 'admin' | 'user' | 'guest'; This file allows these types and variables to be used globally across your project and is typically placed in the src or types directories.\nThe declare global {} statement The declare global {} statement in TypeScript is used to extend or modify the global scope by adding new types, interfaces, or variables that will be accessible globally throughout the project. This is useful when you need to add custom properties to global objects (like Window, Document, etc.) or declare new global variables and types.\nIt is typically used in .d.ts files to make these global declarations available project-wide.\nThis is an example:\n// globals.d.ts export {}; declare global { interface Window { myCustomProperty: string; } declare const API_URL: string; } Meaning:\ndeclare global {}: Everything inside this block is added to the global scope, meaning you can use it without importing in other files. Extending Window: Adds a custom property myCustomProperty to the global Window object. Declaring global API_URL: Makes the API_URL constant available globally, with its type specified. This approach is useful when working with global objects or variables that are shared across different parts of your application.\nUsing or not using the declare global {} statement? The difference between using declare global {} and not using it lies in where the declarations are scoped and how they affect the global namespace. Here’s a concise comparison:\n1. Without declare global When you use declare directly without declare global {}, the declared entities are treated as global only within the specific file where the declaration exists.\nExample (without declare global {}):\n// globals.d.ts declare const API_URL: string; interface Window { myCustomProperty: string; } These declarations are global, but only if the .d.ts file is included by TypeScript. You don’t explicitly tell TypeScript that this is meant to modify the global namespace; TypeScript assumes the declarations are part of the global scope.\n2. With declare global By wrapping the declarations inside declare global {}, you are explicitly modifying the global namespace and ensuring that these changes apply project-wide.\nExample (with declare global {}):\n// globals.d.ts export {}; // Ensures the file is treated as a module declare global { interface Window { myCustomProperty: string; } declare const API_URL: string; } declare global {} is used when the file is treated as a module (via export {} or similar) and you still want to modify the global scope. This makes it explicit that you’re extending the global namespace from within a module, ensuring the declarations are globally available, even if the .d.ts file contains export statements. Key Differences Without declare global {}: The file is implicitly considered part of the global namespace. These declarations work fine in traditional .d.ts files with no imports or exports. With declare global {}: Explicitly extends the global namespace when the file is a module (i.e., it contains import or export statements). This is necessary if you want to mix global declarations with modular code. You would typically use declare global {} in module-based projects where you are exporting/importing other things but still want to add or modify the global scope.",
    "description": "Node.JS Installation Quick Overview Initializing TypeScript Project Setting TypeScript Configuration File Searching and Importing Open Source Defined Types Types Primitive and Built-in Types Creating Custom Types Using Interface Creating Custom Types Using Aliases Enumerable Types Typing a Function Using Specific Types in a Function Defining a Megatype using Generics More Complex Types Using Aliases for Complex Types Definition Working with Record Type Resource Management with using Keyword Extending and modifying existing types The Partial helper type The Omit helper type The Pick helper type The Required helper type Decorators Decorator with no arguments Decorator with no arguments Using modules in typescript Using the globals.d.ts file The declare global {} statement Using or not using the declare global {} statement? TypeScript JavaScript and TypeScript are two popular programming languages for developing web applications. JavaScript is a simple and versatile language that supports dynamic typing. TypeScript extends JavaScript by adding static typing and features like interfaces, enums, and advanced type-checking.",
    "tags": [],
    "title": "TypeScript",
    "uri": "/notes/typescript/index.html"
  }
]
