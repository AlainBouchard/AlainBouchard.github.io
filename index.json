[
  {
    "content": "Blogs How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR)  ",
    "description": "",
    "tags": null,
    "title": "My Blogs",
    "uri": "/blogs/"
  },
  {
    "content": "My Engineering Notes and Blogs Welcome on my Engineering Notes and Blogs site!\nMy Blogs How to promote the Quality Best Practices Quality Architect Role Rôle de l'Architecte Qualité (FR) My Notes Apache Kafka (WIP) Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured Lombok Java Library Python and PyTest  ",
    "description": "",
    "tags": null,
    "title": "Alain Bouchard's Engineering Notes and Blogs",
    "uri": "/"
  },
  {
    "content": "My Engineering Notes Apache Kafka (WIP) Gradle Gradle for Java-Based applications and libraries Hugo IntelliJ Cheat Sheet Java Object-Oriented Programming Java Spring Boot 2 Java with Rest-Assured Lombok Java Library Python and PyTest  ",
    "description": "",
    "tags": null,
    "title": "My Notes",
    "uri": "/notes/"
  },
  {
    "content": "  Warning 2022-07-11: This is a WORK IN PROGRESS document (WIP) and need to be reviewed.\n   Apache Kafka  Topics, partitions and offsets Brokers Brokers and topics Topic replication factor Producers Consumers Consumer offsets Kafka broker discovery Zookeeper Kafka guarantees Install Kafka using Docker images Use Kafka Topics CLI Topics CLI Kafka console producer Kafka console consumer Kafka-Client with Java Kafka Connect and Kafka Stream    \r Apache Kafka Why Apache Kafka?\n Created by LinkedIn, now Open Source Project Maintained by Confluent (Apache Stewardship) Distributedm resilient architecture, fault tolerant scales Horizontaly (100s of borkers, millions of messages per seconds, etc.)  Use Cases?\n Messaging system Activity tracking Gather metrics from many different locations Application logs gathering Stream processing (e.g., kafka stream API, Apache Spark, etc.) De-coupliung system dependicies Integration with Spark, Flink, Storm, Hadoop, and other big Data technologies  Topics, partitions and offsets   Topics: a particular stream of data\n similar to a table in a DB (but without the constraints) you can have as many topics as you want a topic is identified by its name    Partitions: spliting topics\n each partiction is orderied each message within a partiction gets an incremental id, called offset   | Partition 0 -\u003e offset: | 0 | 1 | 2 | 3 | 4 | 5 | ...  | Kafka Topic + Partition 1 -\u003e offset: | 0 | 1 | 2 | ... writes  |  | Partition 2 -\u003e offset: | 0 | 1 | 2 | 3 | 4 | ...  offset only have a meaning for a specific partiction (e.g., ofsset 3 in partition 0 != offset 3 in partition 1) order is guaranteed only within a partition (not acresoo partitions) data is kept only for a limited time (default is one week) once the data is written to a partition, it can’t be changed (immutability) data is assigned randomly to a partition unless a key is provided (more on this later)    Brokers  a kafka cluster in composed of multiple brokers (broker = servers) each broker is identified with its ID (integer) each broker contains certain topic partitions after connecting to any broker (called a bootstrap broker), you will be connected to the intire cluster a good number to get started is 3 brokers, but may go over 100 brokers  Brokers and topics  Example of Topic-A with 3 partitions Exmaple of Topic-B with 2 partitions     Broker 101 Broker 102 Broker 103     Topic A partition 0 Topic A partition 2 Topic A partition 1   Topic A partition 1 Topic A partition 0     Topic replication factor  topics should have a replication factor \u003e 1 (usually between 2 and 3) this way if a broker is down, another broker can serve the data example: topic-A with 2 partitions and replications factor of 2     Broker 101 Broker 102 Broker 103     Topic-A Partition 0 Topic-A Partition 1 Topic-A Partition 1    Topic-A Partition 0      example: broker 102 goes down; broker 101 and 103 still up, both partitions still work at any time only one broker can be a leader for a given partition only that leader can receive and serve data for a partition the other brokers will synchronize the data therefore each partion has one leader and multiple ISR (in-sync replica)  Producers  producers write data to topics (which is made of partitions) producers automatically know to which broker and partition to write to in case of broker failures, producers will automatically recover producers can choose to receive acknowledgement of data writes  acks=0: producers won’t wait for acknowledgment (possible data loss) (default) acks=1: producer will wait for leader acknowledgment (limited data loss) acks=all: leaders and replicas acknowledgment (no data loss)    Message keys  producers can choose to send a key with the message (string, number, etc.) if key=null, data is sent round robin (broker 101, 102 then 103, and 101 again…) if a key is sent, then all messages for that key will always goto the same partition a key is basically sent if you need a message ordering for specific field (e.g., truck_id, etc.)  we get this guarantee due to key hashing, which depends on the number of partitions    Consumers  consumers read data from a tipic (identified by name) consumers know which broker to read from in case of broker failures, consumer know how to recover data is read in order within each partitions  Consumers groups  consumers read data in consumer groups each consumer within a group reads from exclusinve partitions if youy have more consumer than partitions, some consumers with be icative if you have more consumers than partitions, some consumers will be inactive  Consumer offsets  kafka stores the offsets at which a consumer group has beed reading the offset committed live in a kafka topic named __consumer_offsets when a consumer in a group has processed data received from kafka, it should be committing the offsets if a consumer dies, it will be able to read back from where it left off (due to the committed consumer offsets)  Delivery semantics for consumers  consumers choose when to commit offsets there are 3 delivery semantics at most once:  offsets are committed as soon as the message is received  offsets are commited as soon as the message is received if the processing goes wring, the message will be lost (it won’t be read again)   at least once (usually preferred)  offsets are committed after the message is processed if the processing goes wring, the message will be read again this can reults in duplicate processing of messages, so make sure the processing is idempotent   exactly once  can be achieved for kafta-to-kafka workflows using kafka streams API for kafka-to-external-system workflows, it requires the consumer to be idempotent      Kafka broker discovery  every kafka broker is also called a bootstrap server that means that you only need to connect to one broker and you will be connected to the entire cluster each broker knows about all brokers, topics and partitions (metadata)  Zookeeper  zookeeper manages brokers (keeps a list of them) zookeeper helps in performing leader election partitions zookeeper sends notifications to kafka in case of changes (e.g., new topic, broker dies, brker comes up, delete topics, etc.) kafka can’t work without zookeeper zookeeper by design operates with an odd number of servers (3, 5, 7, …) zookeeper has a leader (handle writes) and the rest if the servers are followers (handle reads) zookeeper does not store consumer offsets with kafka \u003e v0.10  Kafka guarantees  Messages are appended to a topic-partition in othe order they are sent consumers read messages in the order stored in a topic-partition with a replication factor of N, producers and consumers can tolerate up to N-1 brokers being down this is why a replicator factor of 4 is a good idea:  allows for one broker to be taken down for maintenance allows for another broker to be taken down unexpectedly   as long as the number of partitions remains constant for a topic (no new partitions), the same key will always go to the same partition (i.e., hashed key)  Install Kafka using Docker images Create the docker-compose.yaml file:\nversion: '3.5' services:  zookeeper:  image: confluentinc/cp-zookeeper:latest  environment:  ZOOKEEPER_CLIENT_PORT: 2181  ZOOKEEPER_TICK_TIME: 2000  ports:  - 22181:2181   kafka:  image: confluentinc/cp-kafka:latest  depends_on:  - zookeeper  ports:  - 29092:29092  environment:  KAFKA_BROKER_ID: 1  KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT  KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 From the CLI:\n\u003e docker-compose up -d \u003e docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ab09b5c8bc7b confluentinc/cp-kafka:latest \"/etc/confluent/dock…\" 11 minutes ago Up 11 minutes 9092/tcp, 0.0.0.0:29092-\u003e29092/tcp kafka-cluster_kafka_1 8e7725b7874b confluentinc/cp-zookeeper:latest \"/etc/confluent/dock…\" 11 minutes ago Up 11 minutes 2888/tcp, 3888/tcp, 0.0.0.0:22181-\u003e2181/tcp kafka-cluster_zookeeper_1  \u003e $ netstat -aon | grep 22181  TCP 0.0.0.0:22181 0.0.0.0:0 LISTENING 23792  TCP [::]:22181 [::]:0 LISTENING 23792  TCP [::1]:22181 [::]:0 LISTENING 27612  \u003e netstat -aon | grep 29092  TCP 0.0.0.0:29092 0.0.0.0:0 LISTENING 23792  TCP [::]:29092 [::]:0 LISTENING 23792  TCP [::1]:29092 [::]:0 LISTENING 27612  \u003e docker-compose logs kafka | grep -i started kafka_1 | [2022-05-31 14:49:34,438] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 14:49:34,441] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 14:49:34,445] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,450] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,450] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started socket server acceptors and processors (kafka.network.SocketServer) kafka_1 | [2022-05-31 14:49:34,459] INFO [KafkaServer id=1] started (kafka.server.KafkaServer) kafka_1 | [2022-05-31 18:51:50,791] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 18:51:50,791] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 19:52:52,922] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 19:52:52,923] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 21:02:51,738] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 21:02:51,739] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) kafka_1 | [2022-05-31 23:24:34,226] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -\u003e HashMap() (kafka.controller.ZkReplicaStateMachine) kafka_1 | [2022-05-31 23:24:34,226] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -\u003e HashMap() (kafka.controller.ZkPartitionStateMachine) Note: nc -z localhost \u003cport\u003e can be used on MacOS and linux.\nFollow the Guide to Setting Up Apache Kafka Using Docker instructions.\nInstall the Kafka Offset Explorer to connect to Kafka Cluster and make sure to configure the bootstrap server: localhost:29092\nUse Kafka Topics CLI Use docker to execute the kafka-topics command:\n\u003e docker-compose ps  Name Command State Ports ----------------------------------------------------------------------------------------------------------- kafka-cluster_kafka_1 /etc/confluent/docker/run Up 0.0.0.0:29092-\u003e29092/tcp, 9092/tcp kafka-cluster_zookeeper_1 /etc/confluent/docker/run Up 0.0.0.0:22181-\u003e2181/tcp, 2888/tcp, 3888/tcp  \u003e docker exec -t kafka-cluster_kafka_1 kafka-topics Create, delete, describe, or change a topic. Option Description ------ ----------- --alter Alter the number of partitions,  replica assignment, and/or  configuration for the topic. . . . To use Kafka Container shell:\n\u003e docker exec -it kafka-cluster_kafka_1 sh sh-4.4$ Topics CLI Reference: Apache Kafka CLI commands cheat sheet\nCreating a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --create --topic first_topic --partitions 3 --replication-factor 1  WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both. Created topic first_topic.  sh-4.4$ Note: the --bootstrap-server localhost:9092 replaces the kafka-topics command --zookeeper localhost:9092\nListing current topics sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list  first_topic  sh-4.4$ Describe a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --describe --topic first_topic  Topic: first_topic TopicId: vQ6o8fX1Sx-qNQuUQ5vbAg PartitionCount: 3 ReplicationFactor: 1 Configs:  Topic: first_topic Partition: 0 Leader: 1 Replicas: 1 Isr: 1  Topic: first_topic Partition: 1 Leader: 1 Replicas: 1 Isr: 1  Topic: first_topic Partition: 2 Leader: 1 Replicas: 1 Isr: 1  sh-4.4$ Delete a topic sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list  first-topic first_topic  sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --delete --topic first-topic  sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list  first_topic  sh-4.4$ Kafka console producer Create messages Creating 4 messages using Kafka Broker using the kafka-console-producer command. The CTRL-C command will make the kafka-console-producer command to stop.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic  \u003emessage1 \u003emessage2 \u003emessage3 \u003emessage4 \u003e^C  sh-4.4$ Create messages in a non-existing topic It is possible to create a new topic on-the-fly when adding but it is not recommended since the default values for both PartitionCount and ReplicationFactor are set to 1. Best practices require more partitions and replications.\nDefault replication value can be changed from /etc/kafka/server.properties value num.partitions, default is 1.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic new_topic  \u003eThis is a message to a new topic [2022-06-02 18:56:53,185] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 3 : {new_topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient) \u003eanother message  sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --list  first_topic new_topic  sh-4.4$ kafka-topics --bootstrap-server localhost:9092 --topic new_topic --describe  Topic: new_topic TopicId: FuC1JLRETNCrgUWaEPZCOg PartitionCount: 1 ReplicationFactor: 1 Configs:  Topic: new_topic Partition: 0 Leader: 1 Replicas: 1 Isr: 1  sh-4.4$ Change the producer-property Changing the acks property. Refer to producer above sections for more information about acks=all property.\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic --producer-property acks=all  \u003eacks message1 \u003e^C  sh-4.4$ Kafka console consumer Consuming messages from a topic The kafka-console-consumer won’t consume topic messages from offset=0 by default to avoid consuming millions of existing message. It will consume the upcomming messages only (from now on). To get all messages from offest:0, the --from-beginning option must be specified.\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --from-beginning  message2 acks message1 message3 message1 message4 The order isn’t garanteed when the number of topic partitions is greater than 1, as explained in Topics, partitions and offsets section.\nConsuming messages from a topic with groups The --group define a group of kafka consumers that will share the consumption load for one given topic.\nThe kafka-console-producer example:\nsh-4.4$ kafka-console-producer --broker-list localhost:9092 --topic first_topic  \u003epatate \u003ecarotte \u003epomme \u003eorange \u003e The first kafka-console-consumer example:\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-group  carotte orange The second kafka-console-consumer example:\nsh-4.4$ kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --group my-group  patate pomme Consumer groups command Get all the available consumer groups:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --list  my-group  sh-4.4$ Get consumer group information:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group  GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 3 3 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 1 2 2 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 2 4 4 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer  sh-4.4$ Consumer groups: reseting offsets All messages are currently consumed: current-offsets=log-end-offsets:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group  GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 3 3 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 1 2 2 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer my-group first_topic 2 4 4 0 console-consumer-13be95b9-8704-4471-8211-2660c5ab59b1 /172.19.0.3 console-consumer  sh-4.4$ Reseting the current-offsets to 0 using --to-earliest option:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --reset-offsets --to-earliest --execute --topic first_topic  GROUP TOPIC PARTITION NEW-OFFSET my-group first_topic 0 0 my-group first_topic 1 0 my-group first_topic 2 0  sh-4.4$ Verify the group offsets is not 0:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group  Consumer group 'my-group' has no active members.  GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID my-group first_topic 0 0 3 3 - - - my-group first_topic 1 0 2 2 - - - my-group first_topic 2 0 4 4 - - -  sh-4.4$ Shift the offset by 1 using --shift-by option:\nkafka-consumer-groups --bootstrap-server localhost:9092 --group my-group --reset-offsets --shift-by 1 -- execute --topic first_topic  GROUP TOPIC PARTITION NEW-OFFSET my-group first_topic 0 1 my-group first_topic 1 1 my-group first_topic 2 1  sh-4.4$ Available --reset-offsets options are :\n--to-datetime --by-period --to-earliest --to-latest --shift-by --from-file --to-current Kafka-Client with Java Use Kafka Documentation as much as possible.\nCreate Java project Using IntelliJ:\n create a project search for kafka-client in https://mvnrepository.com/ select the desired version (i.e., the following will use 3.2.0) copy the Gradle implementation information in the dependencies section of the build.gradle file download the dependencies using IntelliJ Gradle window -\u003e Reload... button repeat same steps for slf4j-simple package and set scope to implementation instead of testImplementation create a new package, e.g., com.github.alainbouchard.kafka-demo.demo1  Create a simple producer The following are needed when creating a producer:\n kafka producer properties (or Properties object) kafka producter creation Send data (verification) Close the producer  Reference for Kafka Producer Properties.\npublic class KafkaProducerDemo {   public static void main(String[] args) {  Properties properties = new Properties();   // Set Producer Properties  properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\");  properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());  properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());   // Create Kafka Producer  KafkaProducer\u003cString, String\u003e producer = new KafkaProducer\u003cString, String\u003e(properties);  ProducerRecord\u003cString, String\u003e record = new ProducerRecord\u003c\u003e(\"first_topic\", \"hello world from Java!\");   // Send data  producer.send(record); // asynchronous, need to flush data  producer.flush();   // Tear down Producer  producer.close();  } } Create a producer with a callback public class KafkaProducerDemoWithCallback {   public static void main(String[] args) {  Logger logger = LoggerFactory.getLogger(KafkaProducerDemoWithCallback.class);   Properties properties = new Properties();   // Set Producer Properties  properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\");  properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());  properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());   // Create Kafka Producer  KafkaProducer\u003cString, String\u003e producer = new KafkaProducer\u003cString, String\u003e(properties);  ProducerRecord\u003cString, String\u003e record = new ProducerRecord\u003c\u003e(\"first_topic\", \"hello world from Java!\");   // Send data  producer.send(record, new Callback() {  @Override  public void onCompletion(RecordMetadata metadata, Exception exception) {  // execute everytime a message is sent OR an exception is thrown.  if (exception == null) {  // successfully sent message  logger.info(\"Received metadata - Topic: \"  + metadata.topic() + \" Partition: \"  + metadata.partition() + \" Offset: \"  + metadata.offset() + \" Timestamp: \"  + metadata.timestamp());  } else {  logger.error(\"Error while producer sent a message\", exception);  }  }  }); // asynchronous, need to flush data  producer.flush();   // Tear down Producer  producer.close();  } } Create a simple consumer The following are needed when creating a consumer:\n kafka consumer properties (or Properties object) kafka consumer creation poll data in a loop  Reference for Kafka Consumer Properties.\npublic class KafkaConsumerDemo {  public static void main(String[] args) {  Logger logger = LoggerFactory.getLogger(KafkaConsumerDemo.class.getName());   // Configure the consumer  Properties properties = new Properties();  properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\");  properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());  properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());  properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, \"my_group\");  properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest   // Create consumer  KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003cString, String\u003e(properties);   // Subscribe the consumer to the Topic or Topics  consumer.subscribe(Arrays.asList(\"first_topic\"));   // Poll for new data  while(true) { // Avoid in production - demo purpose only.  ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100));   records.forEach(r -\u003e logger.info(\"Key: \" + r.key()  + \" Value: \" + r.value()  + \" Partition: \" + r.partition()  + \" Offset: \" + r.offset()  + \" Timestamp: \" + r.timestamp()));  }  } } Create a consumer in a thread Note: It is only working if the application does stop gracefully. A Break/Kill signal won’t trigger the shutdown properly. IntelliJ don’t have the exit button on Windows.\n public class KafkaConsumerWithThreadsDemo {  Logger logger = LoggerFactory.getLogger(KafkaConsumerWithThreadsDemo.class.getName());   public KafkaConsumerWithThreadsDemo() { }   public void run() {  String bootstrapServer = \"localhost:29092\";  String groupId = \"my_group\";  String topic = \"first_topic\";   // Latch for dealing with multiple threads;  CountDownLatch latch = new CountDownLatch(1);   // Create Consumer Runnable;  Runnable consumerRunnable = new ConsumerRunnable(bootstrapServer, groupId, topic, latch);   // Starting Consumer Runnable Thread;  Thread thread = new Thread(consumerRunnable);  thread.start();   // Add a shutdown hook;  Runtime.getRuntime().addShutdownHook(new Thread( () -\u003e {  logger.info(\"Received a shutdown hook...\");  ((ConsumerRunnable) consumerRunnable).shutdown();   try {  latch.await();  } catch (InterruptedException e) {  throw new RuntimeException(e);  }   logger.info(\"Consumer application has exited...\");  }));   try {  latch.await();  } catch (InterruptedException e) {  logger.error(\"Consumer application got interrupted\", e);  } finally {  logger.info(\"Consumer application is closing...\");  }   }   public class ConsumerRunnable implements Runnable {  private CountDownLatch latch;  KafkaConsumer\u003cString, String\u003e consumer;   public ConsumerRunnable(String bootstrapServer, String groupId, String topic, CountDownLatch latch) {  this.latch = latch;   // Configure the consumer  Properties properties = new Properties();  properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer);  properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());  properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());  properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId);  properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest   // Create consumer  consumer = new KafkaConsumer\u003cString, String\u003e(properties);   // Subscribe the consumer to the Topic or Topics  consumer.subscribe(Arrays.asList(topic));  }   @Override  public void run() {  // Poll for new data  try {  while (true) { // Avoid in production - demo purpose only.  ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100));   records.forEach(r -\u003e logger.info(\"Key: \" + r.key()  + \" Value: \" + r.value()  + \" Partition: \" + r.partition()  + \" Offset: \" + r.offset()  + \" Timestamp: \" + r.timestamp()));  }  } catch ( WakeupException exception) {  logger.info(\"Received shutdown signal...\");  } finally {  consumer.close();  latch.countDown(); // telling caller code that this thread is done.  }  }   public void shutdown() {  // to interrupt the consumer.poll() method   // and will make consumer.poll() to throw an exception WakeupException  consumer.wakeup();  }  }   public static void main(String[] args) {  new KafkaConsumerWithThreadsDemo().run();  } } Assign and seek consumer The Assign and Seek is mostly used to replay data or fetch a specific message.\npublic class KafkaConsumerWithAssignAndSeekDemo {  public static void main(String[] args) {  Logger logger = LoggerFactory.getLogger(KafkaConsumerWithAssignAndSeekDemo.class.getName());   // Configure the consumer  Properties properties = new Properties();  properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:29092\");  properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());  properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());  // properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, \"my_group\"); No needs for group with assign and seek...  properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // Possible values : none, earliest, latest   // Create consumer  KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003cString, String\u003e(properties);   // The Assign and Seek is mostly used to replay data or fetch a specific message.   // Assign  TopicPartition partition = new TopicPartition(\"first_topic\", 0);  consumer.assign(Arrays.asList(partition));   // Seek  consumer.seek(partition, 1L);   // Poll for new data  while(true) { // Avoid in production - demo purpose only.  ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(100));   records.forEach(r -\u003e logger.info(\"Key: \" + r.key()  + \" Value: \" + r.value()  + \" Partition: \" + r.partition()  + \" Offset: \" + r.offset()  + \" Timestamp: \" + r.timestamp()));  }  } } Kafka Bidirectional Compatibility As Kafka 0.10.2 (July 2017), the client and brokers hava a vapability called bi-directional compatibility (because API calls are nov versioned).\nIt means that the latest client library version should always be used as documented in the confluent documentation Upgrading Apache Kafka Clients Just Got Easier\nCreating a Producer for Twitter messages Needed packages:\ndependencies {  implementation group: 'com.twitter', name: 'twitter-api-java-sdk', version: '1.2.4' } Creating the topic:\nkafka-topics --bootstrap-server localhost:9092 --create --topic twitter_tweets --partitions 6 --replication-factor 1 Adding a TwitterKafkaProducerInterface Interface:\npackage com.github.alainbouchard.kafka_demo.demo2;  public interface TwitterKafkaProducerInterface {  void send(String topic, String message); } Implementing the interface with TwitterKafkaProducer Class:\npublic class TwitterKafkaProducer implements TwitterKafkaProducerInterface {  private final Logger logger = LoggerFactory.getLogger(TwitterKafkaProducer.class.getName());  private KafkaProducer\u003cString, String\u003e producer;   public TwitterKafkaProducer() {  Properties properties = new Properties();   // Set Producer Properties  properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:29092\");  properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());  properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());   // Create Kafka Producer  producer = new KafkaProducer\u003cString, String\u003e(properties);   // Create topic:  // kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_tweets --partitions 6 --replication-factor 1  }   public Logger getLogger() {  return logger;  }   @Override  public void send(String topic, String message) {  ProducerRecord producerRecord = new ProducerRecord\u003c\u003e(topic, null, message);  producer.send(producerRecord, new Callback() {  @Override  public void onCompletion(RecordMetadata metadata, Exception exception) {  if (exception != null) {  getLogger().error(\"Could not send the message to the producer.\", exception);  }  }  });  } } Adding the TwitterListerner Class:\nSome TwitterApi v2 SDK methods have issues at the moment that this document is written.\npublic class TwitterListener {  /*** * Ref: * https://developer.axonivy.com/api-browser?url=/market-cache/twitter/twitter-connector-product/9.3.0/openapi.json#/Tweets/addOrDeleteRules * https://github.com/twitterdev/twitter-api-java-sdk/tree/d0d6a8ce8db16faf4e3e1841c3a43bd5a56aa069 * https://developer.twitter.com/en/docs/twitter-api */   private Logger logger = LoggerFactory.getLogger(TwitterListener.class.getName());  // API V2 uses BEARER token.  // TODO: Use environment variable for BEARER_TOKEN.  private final String BEARER_TOKEN = \"\";  protected TwitterApi twitterApi;   private TwitterKafkaProducerInterface twitterKafkaProducer;   Function\u003cList\u003cRule\u003e, List\u003cString\u003e\u003e GetIdsFromRules = r -\u003e r.stream().map(Rule::getId).collect(Collectors.toList());   public TwitterListener() {  twitterApi = new TwitterApi();  TwitterCredentialsBearer credentials = new TwitterCredentialsBearer(BEARER_TOKEN);   twitterApi.setTwitterCredentials(credentials);  }   public Logger getLogger() {  return logger;  }   public TwitterApi getTwitterApi() {  return twitterApi;  }   public void setTwitterKafkaProducer(TwitterKafkaProducerInterface twitterKafkaProducer) {  this.twitterKafkaProducer = twitterKafkaProducer;  };   private void logApiExceptionToString(String description, ApiException e) {  String text = description  + \" Status code: \" + e.getCode()  + \" Reason: \" + e.getResponseBody()  + \" Response headers: \" + e.getResponseHeaders();  getLogger().error(text, e);  }   private List\u003cRule\u003e addRule(String value, String tag, boolean dryRun) {  // Create rule  RuleNoId ruleNoId = new RuleNoId();  ruleNoId.setValue(value);  ruleNoId.setTag(tag);   // Add rule to list of rules  List\u003cRuleNoId\u003e ruleNoIds = new ArrayList\u003c\u003e();  ruleNoIds.add(ruleNoId);   // Add the list of rules to the request  AddRulesRequest addRulesRequest = new AddRulesRequest();  addRulesRequest.add(ruleNoIds);   AddOrDeleteRulesRequest addOrDeleteRulesRequest = new AddOrDeleteRulesRequest();  addOrDeleteRulesRequest.setActualInstance(addRulesRequest);   List\u003cRule\u003e rules = null;   try {  AddOrDeleteRulesResponse result = getTwitterApi().tweets().addOrDeleteRules(addOrDeleteRulesRequest, dryRun);  getLogger().info(result.toString());   rules = result.getData();  } catch (ApiException e) {  logApiExceptionToString(\"Exception when calling TweetsApi#addOrDeleteRules\", e);  }   return rules;  }   private AddOrDeleteRulesResponse deleteRules(List\u003cRule\u003e rules, boolean dryRun) {  DeleteRulesRequestDelete deleteRulesRequestDelete = new DeleteRulesRequestDelete();  deleteRulesRequestDelete.ids(GetIdsFromRules.apply(rules));   DeleteRulesRequest deleteRulesRequest = new DeleteRulesRequest();  deleteRulesRequest.setDelete(deleteRulesRequestDelete);   AddOrDeleteRulesRequest addOrDeleteRulesRequestForDelete = new AddOrDeleteRulesRequest();  addOrDeleteRulesRequestForDelete.setActualInstance(deleteRulesRequest);   AddOrDeleteRulesResponse result = null;   try {  result = this.getTwitterApi().tweets().addOrDeleteRules(addOrDeleteRulesRequestForDelete, dryRun);  getLogger().info(result.toString());  } catch (ApiException e) {  logApiExceptionToString(\"Exception when calling TweetsApi#addOrDeleteRules\", e);  }   return result;  }   private GetRulesResponse getRules(String paginationToken, List\u003cRule\u003e rules, Integer maxResults) {  List\u003cString\u003e ruleIds = rules != null? GetIdsFromRules.apply(rules) : null;  GetRulesResponse result = null;   try {  result = getTwitterApi().tweets().getRules(ruleIds, maxResults, paginationToken);  getLogger().info(result.toString());  } catch (ApiException e) {  logApiExceptionToString(\"Exception when calling TweetsApi#getRules\", e);  }   return result;  }   private GetRulesResponse getRules(String paginationToken, List\u003cRule\u003e rules) {  return this.getRules(paginationToken, rules, 1000);  }   private GetRulesResponse getRules(String paginationToken) {  return this.getRules(paginationToken, null);  }   private void searchStream() {  Set\u003cString\u003e expansions = new HashSet\u003c\u003e(Arrays.asList());  Set\u003cString\u003e tweetFields = new HashSet\u003c\u003e();  tweetFields.add(\"id\");  tweetFields.add(\"author_id\");  tweetFields.add(\"created_at\");  tweetFields.add(\"text\");  Set\u003cString\u003e userFields = new HashSet\u003c\u003e(Arrays.asList());  Set\u003cString\u003e mediaFields = new HashSet\u003c\u003e(Arrays.asList());  Set\u003cString\u003e placeFields = new HashSet\u003c\u003e(Arrays.asList());  Set\u003cString\u003e pollFields = new HashSet\u003c\u003e(Arrays.asList());  Integer backfillMinutes = null; // There is a bug in the Twitter API v2 where any specified value will cause an error.   InputStream result = null;   try {  result = getTwitterApi().tweets().searchStream(expansions, tweetFields, userFields, mediaFields, placeFields, pollFields, backfillMinutes);   try {  JSON json = new JSON();  Type localVarReturnType = new TypeToken\u003cFilteredStreamingTweet\u003e(){}.getType();  BufferedReader reader = new BufferedReader(new InputStreamReader(result));  String line = reader.readLine();   while (line != null) {  if (line.isEmpty()) {  getLogger().info(\"==\u003e Empty line\");  line = reader.readLine();  continue;  }  Object jsonObject = json.getGson().fromJson(line, localVarReturnType);  String message = jsonObject != null ? jsonObject.toString() : \"Null object\";  getLogger().info(message);  twitterKafkaProducer.send(\"twitter_tweets\", message);   line = reader.readLine();  }  } catch (Exception e) {  e.printStackTrace();  }   } catch (ApiException e) {  logApiExceptionToString(\"Exception when calling TweetsApi#searchStream\", e);  }  }   public static void main(String[] args) {  TwitterListener twitterListener = new TwitterListener();  twitterListener.setTwitterKafkaProducer(new TwitterKafkaProducer());   boolean dryRun = false;   // Delete all existing Rules;  try {  GetRulesResponse rulesResponse = twitterListener.getRules(null);  AddOrDeleteRulesResponse result = twitterListener.deleteRules(rulesResponse.getData(), dryRun);  twitterListener.getLogger().info(\"Deleted rules: \" + twitterListener.GetIdsFromRules.apply(result.getData()));  } catch (Exception e) {  twitterListener.getLogger().error(\"oops!\"); // bug in the TwitterApi SDK.  e.printStackTrace();  }   // Adding Rules;  twitterListener.addRule( \"potus -is:retweet\", \"Non-retweeted potus tweets\", dryRun);  twitterListener.addRule( \"hockey -is:retweet\", \"Non-retweeted hockey tweets\", dryRun);  twitterListener.addRule( \"baseball -is:retweet\", \"Non-retweeted baseball tweets\", dryRun);  twitterListener.addRule( \"bitcoin -is:retweet\", \"Non-retweeted bitcoin tweets\", dryRun);   // Filter twitter stream;  twitterListener.searchStream();  } } Fine-tuning the Kafka producer Producers Acks acks=0  no response is requested may loose data if broker goes offline it is okay when lost of data is acceptable:  metrics collection log collection    Producer Broker 101 partition 0 (leader) -------- -------------------------------  | |  +-----[send data to leader]----\u003e+  | | acks=1 (default as Kafka v2.0)  leader response is requestion (no guarantee of replication) the producer may retry if the tack isn’t received data loss is possible if leader broker goes offline and replcas haven’t replicated the data yet  Producer Broker 101 partition 0 (leader) -------- -------------------------------  | |  +----[send data to leader]-----\u003e+  | |  +\u003c---[respond write reqs]-------+  | | acks=all (replicas acks)  leader and replicas acks are requested adding latency and safety no data loss if enough replicas needed setting to avoid losing data  Producer Broker 101 part0 (leader) Broker 102 part0 (replica) Broker 103 part0 (replica) -------- ------------------------- -------------------------- --------------------------  | | | |  +---[send data to leader]---\u003e+ | |  | | | |  | +-----[send to replica]-----\u003e+ |  | | | |  | +-----[send to replicas]---------------------------------\u003e+  | | | |  | +\u003c--------[ack write]--------+ |  | | | |  | +\u003c--------[ack write]-------------------------------------+  | | | |  +\u003c---[respond write reqs]----+ | |  | | min.insync.replicas  the acks=all must be used along with min.insync.replicas it can be set at the proker or topic level (override) min.insync.replicas=2 means that at least 2 brokers that are ISR (incuding leader) must do the write aknowledgement  e.g., with replication.factor=3, the min.insync.replicas=2 and acks=all then only 1 broker can go down or the producer will receive an exception on the send operation    Producer retries  developers are expected to handle the exceptions or the data may be lost:  e.g., transcient failure: NotEnoughReplicasException   a retries setting is available:  default is 0 no limits to the retries, i.e., Interger.MAX_VALUE in case of retries, there is a change that the messages will be sent in wrong order relying on key-based ordering may be an issue max.in.flight.requests.per.connection (default=5) can be used to help fixing message ordering issue, where max.in.flight.requests.per.connection=1 will ensure ordering, and slow down the throughput. Idempotent producer can be used to help with ordering issues if Kafka version is \u003e= 1.0.0    Idempotent producer  using kafka \u003e= 0.11 - an idempotent producer can be defined, which will solve the duplicates due to network issues (i.e., if ack is lost on the network) idempotent producers are great to guarantee a stable and safe pipeline it comes with when using producerProps.put(\"enable.idempotence\", true):  retries=Integer.MAX_VALUE max.in.flight.requests=1 with Kafka \u003e= 0.11 and \u003c 1.1 max.in.flight.requests=5 with Kafka \u003e= 1.1 for higher performance acks=all   The min.insync.replicas=2 must also be specified since enable.idempotence=true property doesn’t imply this configuration Note: running safe producer might impact the throughput or lantency, and therefore the use case must be verified to make sure the producer is within the NFR expectations  An example of Producer settings to improve safeness:\n // Make a safer producer  properties.setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");  properties.setProperty(ProducerConfig.ACKS_CONFIG, \"all\"); // default value with Idempotence=true  properties.setProperty(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE)); // default value with Idempotence=true  properties.setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"5\"); // with kafka \u003e= 2.0, otherwise use 1. Message compression Some compression benchmarks can be found the this blog: Squeezing the firehose: getting the most from Kafka compression\n producer usually send data text-based (json) messages compression is set at the producer configuration only no needs for consumer or broker configuration the compression.type can be none (default), gzip, lz4 and snappy compression will improve the throughput and performance  +----------------+ +--------------------+ +---------------+ | Producer Batch |------\u003e| Compression of the |-----\u003e| Kafka Cluster | +----------------+ | batch of messages | +---------------+  +--------------------+ Compression advantages:\n smaller producer request size (compression up to 4x) faster to tranfer data over the network better throughput better disk utilisation in Kafka cluster  Compression disavantages:\n producers and consumers must commit CPU time for compression/decompression  Overall:\n snappy and lz4 are optimal for speed/compression ratio recommendations are to try the algorithms in a given use case and environment always use compression in prod should use along with linger.ms and batch.size configuration settings  Configuration: linger.ms and batch.size  Kafka producer default behavior is to send records as soon as possible  it will have up to 5 requests in flight (batch) batching messages are done simultaneously with messages are in-flight (no time wasted)   smart batching allows kafka to increase throughput while maintaining very low latency linger.ms is the number of ms (default=0) that the producer is willing to wait to fully get a batch of messages  by introducing some lag (e.g., linger.ms=5) then we increase the cahnges of messages being sent together in a batch at the cost of introducing a small delay (e.g., 5 ms) then the throughput can be increased, the compression is more efficient and the producer efficiency is better   batch.size is the maximum mumber of bytes (default=16KB) that will be included in a batch increasing a batch size to higher number (32KB or 64KB) can help increasing the ompression, throughput, and efficiency of requests any message tha is bigger than a batch size will not be batched the producer will make or allocate a batch per partition the average batch size metric can be monitored by using the Kafka Producer Metrics  An example of Producer settings to improve efficiency:\n // Improve throughput efficiency of the producer  properties.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"snappy\");  properties.setProperty(ProducerConfig.LINGER_MS_CONFIG, \"20\");  properties.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, \"32768\"); // 32 KB Producer default partition and key hashing  The default key are hashed using murmur2 algorithm it is possible - but maybe not suggested - to override the partitioner behavior using partitioner.class the formula from Kafka code: targetPartition = Utils.abs(Utils.murmur2(record.key())) % numPartitions; therfore the same key will always go to the same partition changing the number of partition will cause key vs partition issues and should be avoided  Max.blocks.ms and buffer.memory These are advanced settings and it is probably better to avoid tweaking them unless necessary.\n when the producer prodeuces faster tahn the broker can handle then the records will be memory buffered the buffer.memory is the size of the buffer (default is 32MB) a full buffer (e.g., full 32MB) will cause the producer.send() method to block (or wait) the max.block.ms (default=60000ms) is the waiting time befre the producer.send() method throw an exception, and causes are:  the producer has filled up the buffer the broker is not accepting any new data 60s has elapsed   An exception may mean that the broker is down or overloaded as it can’t handle the requests  Elastic Search Adding elasticsearch docker container The following must be added in order to add an elastic search container or server to the kafka-cluster:\ndocker-compose.yaml:\n elasticsearch:  image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2  ports:  - 9200:9200  - 9300:9300  environment:  discovery.type: single-node  node.name: es01  cluster.name: kafka-cluster To start the service from docker-compose:\n\u003e docker-compose up elasticsearch -d \u003e docker-compose ps  NAME COMMAND SERVICE STATUS PORTS kafka-cluster-elasticsearch-1 \"/usr/local/bin/dock…\" elasticsearch running 0.0.0.0:9200-\u003e9200/tcp, 0.0.0.0:9300-\u003e9300/tcp kafka-cluster-kafka-1 \"/etc/confluent/dock…\" kafka running 0.0.0.0:29092-\u003e29092/tcp kafka-cluster-zookeeper-1 \"/etc/confluent/dock…\" zookeeper running 0.0.0.0:22181-\u003e2181/tcp  \u003e curl localhost:9200/  {  \"name\" : \"es01\",  \"cluster_name\" : \"kafka-cluster\",  \"cluster_uuid\" : \"-w05UbWdSeOhc4nRGcy8yA\",  \"version\" : {  \"number\" : \"7.5.2\",  \"build_flavor\" : \"default\",  \"build_type\" : \"docker\",  \"build_hash\" : \"8bec50e1e0ad29dad5653712cf3bb580cd1afcdf\",  \"build_date\" : \"2020-01-15T12:11:52.313576Z\",  \"build_snapshot\" : false,  \"lucene_version\" : \"8.3.0\",  \"minimum_wire_compatibility_version\" : \"6.8.0\",  \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"  },  \"tagline\" : \"You Know, for Search\" } Java elasticsearch client The following packages are required:\n implementation group: 'org.elasticsearch.client', name: 'elasticsearch-rest-high-level-client', version: '7.14.0'  implementation group: 'org.apache.logging.log4j', name: 'log4j-core', version: '2.17.2' // Elastic search dependency public class ElasticsearchClient {  Logger logger = LoggerFactory.getLogger(ElasticsearchClient.class.getName());   private RestHighLevelClient restHighLevelClient;   public ElasticsearchClient() {  restHighLevelClient = new RestHighLevelClient(  // TODO: use configuration file or environment variables to set ip and ports, now configured for local docker containers.  RestClient.builder(new HttpHost(\"localhost\", 9200, \"http\"),  new HttpHost(\"localhost\", 9300, \"http\")));  }   public RestHighLevelClient getRestHighLevelClient() {  return restHighLevelClient;  }   public boolean ping() {  boolean result = false;  try {  result = getRestHighLevelClient().ping(RequestOptions.DEFAULT);  } catch (IOException e) {  logger.error(\"Elasticsearch client received an exception.\", e);  } finally {  logger.info(\"Elasticsearch aliveness: \" + result);  }   return result;  }   public IndexResponse toIndex(String index, String jsonSource, String id) {  IndexRequest indexRequest = new IndexRequest(index)  .id(id) // Make the entry idempotent.  .source(jsonSource, XContentType.JSON);   IndexResponse indexResponse = null;  try {  indexResponse = getRestHighLevelClient().index(indexRequest, RequestOptions.DEFAULT);  logger.info(indexResponse.getId());  } catch (IOException e) {  logger.error(\"Caught and exception.\", e);  }   return indexResponse;  }   public void close() {  try {  getRestHighLevelClient().close();  } catch (IOException e) {  logger.error(\"Caught and exception.\", e);  }  }   public static void main(String[] args) {  Logger logger = LoggerFactory.getLogger(ElasticsearchClient.class.getName());   // Setup Elasticsearch  ElasticsearchClient elasticsearchClient = new ElasticsearchClient();   elasticsearchClient.ping();   // Setup Kafka Consumer  TwitterKafkaConsumer twitterKafkaConsumer = new TwitterKafkaConsumer();  twitterKafkaConsumer.subscribe(\"twitter_tweets\");   // Get data using Kafka Consumer and insert data to the elasticsearch  while(true) { // TODO: replace with better logic.  ConsumerRecords\u003cString, String\u003e records = twitterKafkaConsumer.poll(Duration.ofMillis(100));   records.forEach(record -\u003e {  FilteredStreamingTweetResponse tweet = twitterKafkaConsumer.mapTweetToObject(record.value());  elasticsearchClient.toIndex(\"twitter\", record.value(), tweet.getData().getId());   logger.debug(\"Key: \" + record.key()  + \" Value: \" + record.value()  + \" Partition: \" + record.partition()  + \" Offset: \" + record.offset()  + \" Timestamp: \" + record.timestamp());   try {  Thread.sleep(1000);  } catch (InterruptedException e) {  logger.error(\"Error while waiting for sleep\", e);  }  });  }   // Tear-down the Elasticsearch // elasticsearchClient.close();   // Tear-down the Kafka Consumer  } } Kafka delivery semantics The at least once processing should used for most applications along with idempotent strategy.\nAt most once Offsets are committed as soon as the message batch is received. If the processing oes wrong, the message will be lost (it won’t be read again)\nAt least once Offsets are committed after the message is processed. If the processing goes wrong, the message will be read again. This can resilt in duplicate processing of messages. Idempotence of the messages is needed to avoid duplicates.\nExactly once Can be achieved for kafka-to-kafka workflows using kafka streams APIs. For the Kafka-to-Sink workflows, idempotent consumer is needed.\nIdempotentce and offset auto-commit The default configuration (ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG=\"enable.auto.commit\") setting is at-least-once if not specified otherwise.\nTo avoid duplicate entries, the usage of a unique key will be required. An example would be the Twitter ID.\nConsumer Poll Behaviours Default values should be correct in most cases but the following can be adjuested.\n  fetch.min.bytes (default=1):\n controls the minimum data to pull on each request help improving throughput and decreasing request numbr cost is latency    max.poll.records (default=500):\n controls the number of records to receive per poll request increase if the messages are very small and if RAM is available best practices tell to monitor the number of records per poll request and to adjust to increase the value if default value is often reached    max-partitions.fetch.bytes (default=1MB):\n maximum data returned by the broker per partition reading from many partions will require a lot of memory    fetch.max.bytes (default 50MB):\n max data returned for each fetch request (covers multiple partitions) the consumer performs multiple fetches in parallel    Consumer Offset Commits Strategies The two strategies:\n  enable.auto.commit=true and syncrhonous processing of record batches (default)\n  pseudocode:\nwhile(true) {  List\u003cRecords\u003e records = consumer.poll(Duration.ofMillis(100));  doSomethingSunchronous(records); }  offsets get automatically commited at regular interval auto.commit.interval.ms=5000 (default) Note: using asyncrhonous processing would make the delivery sementic to at-most-once since the offset will be committed before the data is processes      enable.auto.commit=false and manual commit of offsets\n  pseudocode:\nwhile(true) {  records += consumer.poll(Duration.ofMillis(100));  if isReady(records) {  doSomethingSynchronous(records);  consumer.commitSync();  } }   offsets commit is controlled according to the expected conditions\n  E.g., accumulation recortds into a buffer and then flushing the buffer to a DB, then offsets are committed\n    Manual commits Settings to be configured to avoid offsets auto commits:\nproperties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,\"false\"); // Will require manual offsets commits properties.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"10\"); // Only retrieve 10 records at the time The syncronous commit command for the consumer - pseudocode:\nwhile(true) {  ConsumerRecords\u003cString, String\u003e records = twitterKafkaConsumer.poll(Duration.ofMillis(100));   logger.info(String.format(\"Received %s records\", records.count()));  records.forEach(record -\u003e {  // do something synchronous here...  });  logger.info(\"Committing offsets\");  twitterKafkaConsumer.commitSync();  logger.info(\"Offsets were committed\"); } Using kafka-console-consumer command:\n looking at the offsets for given group kafka-java-demo-elasticsearch:  kafka-consumer-groups --bootstrap-server localhost:9092 --group kafka-java-demo-elasticsearch --describe  Consumer group 'kafka-java-demo-elasticsearch' has no active members.  GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID kafka-java-demo-elasticsearch twitter_tweets 3 0 39 39 -  - - kafka-java-demo-elasticsearch twitter_tweets 0 0 37 37 -  - - kafka-java-demo-elasticsearch twitter_tweets 4 0 63 63 -  - - kafka-java-demo-elasticsearch twitter_tweets 5 0 43 43 -  - - kafka-java-demo-elasticsearch twitter_tweets 2 0 37 37 -  - - kafka-java-demo-elasticsearch twitter_tweets 1 0 58 58 -  - - sh-4.4$  consuming records using Java consumer and verifying the offsets:  sh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --group kafka-java-demo-elasticsearch --describe  Consumer group 'kafka-java-demo-elasticsearch' has no active members.  GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID kafka-java-demo-elasticsearch twitter_tweets 3 0 39 39 -  - - kafka-java-demo-elasticsearch twitter_tweets 0 0 37 37 -  - - kafka-java-demo-elasticsearch twitter_tweets 4 0 63 63 -  - - kafka-java-demo-elasticsearch twitter_tweets 5 30 43 13 -  - - kafka-java-demo-elasticsearch twitter_tweets 2 0 37 37 -  - - kafka-java-demo-elasticsearch twitter_tweets 1 0 58 58 -  - - sh-4.4$  Note: the current offset for the partition 5 (above example) should be a multiple of ConsumerConfig.MAX_POLL_RECORDS_CONFIG property value, any record (modulo) will be consumed again since the offset wasn’t committed  Consumer offset reset behaviour The offset reset available behaviours are:\n auto.offset.reset=latest which will read from the end of the log auto.offset.reset=earliest which will read from start of the log auto.offset.reset=none which will throw exception if no offset is found  The consumer offsets can be lost:\n if a consumer hasn’t read new data for 1 day (kafka \u003c 2.0) if a consumner hasn’t read new data for 7 days (kafka \u003e= 2.0)  The retention delay can be set using the broker setting offset.retention.minutes. Proper data and offset retention period must be set to ensure no data loss if a server can go down for a while, e.g., 30 days.\nReplaying the data for a consumner group To reset kafka server offsets and replay messages from beginning:\nsh-4.4$ kafka-consumer-groups --bootstrap-server localhost:9092 --topic twitter_tweets --group kafka-java-demo-elasticse arch --reset-offsets --to-earliest --execute  GROUP TOPIC PARTITION NEW-OFFSET kafka-java-demo-elasticsearch twitter_tweets 3 0 kafka-java-demo-elasticsearch twitter_tweets 0 0 kafka-java-demo-elasticsearch twitter_tweets 4 0 kafka-java-demo-elasticsearch twitter_tweets 5 0 kafka-java-demo-elasticsearch twitter_tweets 2 0 kafka-java-demo-elasticsearch twitter_tweets 1 0  sh-4.4$ This allows to restart the consumer and replay the messages; having an idempotent server will make replays safe.\nControlling Consumer Liveliness  consumers in a group talk to a Consumer Group Coordinator there is a heartbeat and a pool mechanism to detect if consumers are down best practices tell that a process should process data fast and poll often  Consumer Heartbeat Thread:\n  session.timeout.ms (default=10s)\n heardbeets are sent periodically to broker if no heartbeat is sent during that period, the consumer is considered dead set low value to faster consumer rebalancing    heartbeat.interval.ms (default=3s)\n wait period between 2 heartbeats best pratices tell to set 1/3rd of the session.timeout.ms value    both settings are set together to detect a dead consumer application (down)\n  Consumer Poll Thread:\n  max.poll.interval.ms (default=5m)\n maximum time between 2 poll() alls before declaring the consumer is dead relevant for Big Data frameworks (e.g., Spark) in case processing takes time    mecahnism to detect a data processing issue with the consumer\n  Kafka Connect and Kafka Stream  There are 4 Kafka use cases:  Source to Kafka =\u003e Producer API =\u003e Kafka Connect Source Kafka to Kafka =\u003e Consumer API/Producer API =\u003e Kafka Streams Kafka =\u003e Sink =\u003e Consumer API =\u003e Kafka Connect Sink Kafka =\u003e Application =\u003e Consumer API =\u003e   Kafka connect and Kafka stream will simplify and improve the in/out of Kafka Kafka connect and Kafka stream will simplify transforming data within kafka withou relying on external libraries  The Kafka connectors can be found on the Confluent Kafka Connectors page.\nWhy Kafka Connect?  developpers always want to import data from the same sources: DB, JDBC, Couchbase, Goldergate, SAP HANA, Blockchain, Cassandra, DynamoDB, FTP, IOT, MongoDB, MQTT, RethinkDB, Salesforce, Solr, SQS, Twitter, etc. developpers always want to store data in the same sinks: S3, ElasticSearch, HDFS, JDBC, SAP HANA, DocumentDB, Cassandra, DynamoDB, HBase, MongoDB, Redis, Solr, Splunk, Twitter, etc. an unexperimented developper may struggle to achieve fault tolerance, idempotence, distribution, ordering, etc. an experimented developper already did the work for others.  Sources/Destinations Connect Cluster Kafka Cluster Stream Apps -------------------- --------------- ------------- -----------  [source1]-------------\u003e[worker] [broker]--------------\u003e[app1]  [worker]------------------\u003e[broker]\u003c--------------  [source2] [worker] [broker]  [worker]\u003c------------------[broker]--------------\u003e[app2]  [sinks]\u003c--------------[worker] \u003c-------------- Kafka Connect: High level  source connectors to get data from Common Data Sources sink connectors to publish data in common data stores make it easy for non-experienced developpers to quickly get data in kafka part of the ETL pipeline (Extract, Transform and Load) scaling made easy from small pipelines to company-wide pipelines re-usable code  Adding a connector to a Java project The connector kafka-connect-twitter will be used as an example to show file structure:\n+ [my-java-project] +-+ [kafka-connect] (to be created with sub folders) | +-+ [connectors] | | +-+ [kafka-connect-twitter] (downloaded connector project from github) | | | +-+ *.jar | | +-+ connector-standalone.properties | | | + run.sh | | | + twitter.properties Connect standalone CLI command sh-4.4$ connect-standalone USAGE: /usr/bin/connect-standalone [-daemon] connect-standalone.properties  sh-4.4$ Creating a new topic for the kafka connector:\n$ kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_status_connect --parti tions 3 --replication-factor 1  WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.  Created topic twitter_status_connect.  $ kafka-topics --bootstrap-server localhost:9092 --create --topic twitter_deletes_connect --part itions 3 --replication-factor 1  WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.  Created topic twitter_deletes_connect. $ ",
    "description": "",
    "tags": null,
    "title": "Apache Kafka (WIP)",
    "uri": "/notes/apache-kafka/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": " Gradle (6.6.1) Terminology Actions Files structure for a “Multi-module build” Commands Example of Typed Task Plugins  \r Gradle (6.6.1)  requires JDK download from [https://gradle.org/releases/] can use either Groovy or Kotlin DSL (Domain-Specific Language)  Terminology  Project: models a software component Build script: contains automation instructions for a project Task: defines executable automation instructions  Ad hoc task: implements one-off, simplistic action code by defining doFirst or doLast, automatically extends DefaultTaslk without having to declare it Typed task: Explicitly declares type (for example, Copy); does not need to define actions as they are already provided by type   Wrapper:  set of files checked into SCM alongside source code standardizes compatible Gradle version for a project automatically downloads the Gradle distribution with defined version    Actions  doLast, doFirst, …  Files structure for a “Multi-module build”  root  + build.gradle  + moduleA/  | + build.gradle  | + src/  + moduleB/  + build.gradle  + src/ Commands   gradle \u003ctask name\u003e -\u003e run the task from the build.gradle\n  gradle wrapper -\u003e create the wrapper files\n  creates gradlew, gradlew.bat and the gradle directories…\n  gradle/wrapper/gradle-wrapper.properties\n  example:\n distributionBase=GRADLE_USER_HOME  distributionPath=wrapper/dists  distributionUrl=https\\://services.gradle.org/distributions/gradle-6.6.1-bin.zip  zipStoreBase=GRADLE_USER_HOME  zipStorePath=wrapper/dists   ./gradlew \u003ctask name\u003e -\u003e download the wrapper gradle version and execute the task with it\n    gradle projects -\u003e create settings for the project\n  Use settings.gradle do define information, example:\n rootProject.name = \"gradle-training\"     gradle \u003ctask name\u003e --dry-run -\u003e show the tasks from the build.gradle but does not execute\n  gradle tasks --all -\u003e show the full list of available tasks\n  Example of Typed Task   example:\n task copyFiles(type: Copy) {  from \"sourceFiles\"  into \"target\"  include \"**/*md\"  includeEmptyDirs = false  }   task createZip(type: Zip) {  from \"build/docs\"  archiveFileName = \"docs.zip\"  destinationDirectory = file(\"build/dist\")  dependsOn CopyFiles  }   Plugins  create a reusable or sharable \u003cplugin-name\u003e.gradle file with defined tasks apply the plugin to the local project build.gradle file, example:  plugin with tasks: myPlugin.gradle in build.gradle:     apply from: \"myPlugin.gradle\"   Notes:\n available plugins are Core Plugins (from Gradle) and Community Plugins (not from Gradle) to apply Core plugin Base (for example), in build.gradle:   apply plugin: 'base'   ",
    "description": "",
    "tags": null,
    "title": "Gradle",
    "uri": "/notes/gradle/"
  },
  {
    "content": " Project structure for gradle project in Java In build.gradle  JAVA plugin APPLICATION plugin   Commands Maven Dependencies Testing with Gradle  JUnit 5 dependencies    \r Project structure for gradle project in Java   single Java project with Gradle\n example:   root  + src  + main  + java  + resources   + test  + java  + resources  + build  + classes -\u003e compiled class files  + libs -\u003e generated JAR files   A multi-modules project file structure:\n example:   root/  + appA/  + build.gradle  + src/  + main/  + java/  + resources/  + test/  + java/  + resources/  + build/  + classes/ -\u003e compiled class files  + libs/ -\u003e generated JAR files  + appB/  + build.gradle  + src/  ... (same as above subproject)   In build.gradle JAVA plugin   add plugins:\n plugins {  id 'java'  }   set the Java compatibility:\n java {  sourceCompatibility = JavaVersion.VERSION_11  targetCompatibility = JavaVersion.VERSION_11  }   compiler arguments:\n compileJava {  // example: to terminate compilation if a warning occures)  options.compilerArgs \u003c\u003c '-Werror'  }   set JAR filename explicitly:\n version = '1.0.0'   jar {  archiveBaseName = '\u003cyour-project-name\u003e'  }   Set javadoc options:\njavadoc {  options.header = '\u003cyour java doc title here\u003e'  options.verbose() } APPLICATION plugin   in build.gradle file:\n plugins {  id 'application'  }   set base class to run for the application as needed by plugin:\n application {  mainClass = 'com.\u003corganization\u003e.\u003cproject\u003e.Main'  }   in settings.gradle file:\n rootProject.name = '\u003cproject_name\u003e'  include ':appA', ':appB'   Commands   gradle wrapper -\u003e Create Gradle Wrapper\n  gradle clean -\u003e Clean dist output?\n  with java plugin:\n ./gradlew compileJava --console=verbose ./gradlew processResources --console=verbose    or both above command agragated command:\n ./gradlew classes --console=verbose    see dependencies tree (and if it can be found)\n ./gradlew dependencies    generate JAR file\n ./gradlew jar -\u003e dropped JAR in build/libs directory    with application plugin:\n ./gradlew run --args=\"add 1 2\" -\u003e specifies arguments ./gradlew installDist -\u003e generate shippable application with scripts ./gradlew distZip distTar -\u003e bundle distributhe appliation ./gradlew javadocs -\u003e genetate java doc in build/docs/javadoc (index.html)    project or multi-modules project:\n ./gradlew project -\u003e show project and show projects structure    Maven Dependencies   where to find? [https://search.maven.org]\n  dependency coordinates:\n example: commons-cli:commons-cli:1.4 -\u003e \u003cgroup\u003e:\u003cartifact\u003e:\u003cversion\u003e or GAV    set the repositories (where to get from) and dependencies:\n repositories {  mavenCentral()  }   dependencies {  implementation 'commons-cli:commons-cli:1.4' // get from maven repo search results  implementation project(':appA') // add project dependencies (other modules from this project)  }   Testing with Gradle JUnit 5 dependencies   from search.maven.org:\n  search org.junit.jupiter (latest version is 5)\n minimum needed is junit-jupiter-api and junit-jupiter-engine    declaring test dependencies:\n testImplementation -\u003e work on compilation and test execution testRuntime -\u003e work on runtime only    adding dependencies:\n copy \u0026 paste Gradle Groovy DSL path for both depenencies   dependencies {  implementation 'commons-cli:commons-cli:1.4' // not a testImplementation!  testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.0' // test dependency  testRuntime 'org.junit.jupiter:junit-jupiter-engine:5.7.0' // test dependency on run time only  }   commands:\n ./gradlew compileTestJava -\u003e compile tests ./gradlew test -\u003e run the tests (see useJUnitPlatform comment in order to use JUnit 5) go to build/reports/tests/test the automaticaly gradle generated HTML report (index.html) go to build/test-reults/test for the autmaticaly gradle generated XML test report    adding test task in build.gradle:\n test {  useJUnitPlatform() // Java plugin expect Java 4 by default. Add useJUnitPlatform to indicates to use JUnit5 instead   testLogging {  events 'started', 'skipped', 'failed' // will display specified events on run time  exceptionFormat 'full' // show stack trace on failures  }  }   ",
    "description": "",
    "tags": null,
    "title": "Gradle for Java-Based applications and libraries",
    "uri": "/notes/gradle-for-java-based-applications-and-libraries/"
  },
  {
    "content": " How to promote the Quality Best Practices Influence or evangelize  Data-Driven decisions   Enforce  \r How to promote the Quality Best Practices Two ways to promote the quality best practices are: Influencing the teams or to enforce the best practices. A periodical team reality check about the current practices is the beginning of the process. A team can easily live in their own Ivory Tower or to be biased by this statement: “We always did it that way so why changing it?”.\nInfluence or evangelize  Discussions: gather information about current pain points (retrospectives and lessons learned) Involve QA Specialists in all SDLC (or Software Development Life Cycle) 1:1 meetings to get information and expose point of views, share ideas and explain the best practices: it may be easier to convince on person than a group Talk to the teams Provide existing articles and blog posts Tech talk to expose the vision Create training material: guidelines, processes, videos (e.g.: Pluralsight, etc.) Involve people in quality solutions:  Organize Workshops (on short term) so that the targeted public becomes active participants Organize Work Groups (on long term) so that the concerned people become active participants   To use data as leverage; more information is available in the data-driven decision section  Data-Driven decisions To show metrics of the current situation (or challenge the “no metrics” or lack of metrics). It is the also called “Shift-Right” approach in the Software Quality world.\n End-users or Customers survey can be used (e.g.: use a post-transaction experience survey on the mobile app or web site) White-Label partner surveys Use production dashboards (examples: Datadog or SignalFX metrics) and performance tools (e.g.: WebVitals, etc.) to monitor APIs, SQLs, etc.  Daily performance emails can be sent to the engineering teams   NPS and CSAT surveys: NPS vs CSAT definitions Real Customers Journey (traces of the end-users through the applications) Use support CIM dashboards: customer tickets, production tickets, ticket aging/bug fix time, tickets by services or teams, RCA (find the issues caused by quality process)  CIM stands for Customer Interaction Management A way to generate CIM data is to use a tool like eazy bi   Correlation between test coverage (static code) and production issues  It is also possible to get test coverage from a live service using a tool like Sealights   Compile the functional testers (manual tests) created Tickets/JIRA vs current team or service test coverage Create Success Stories: start with one team and use the before/after statistics GIT statistics (e.g. with a tool like GitPrime (now Pluralsight Flow)) in order to correlate bugs/tickets and developers stats (e.g.: number of commits, active days, etc.)  I.e.: low stats may be caused by developers not working on features or test automations because they are working on manual tests or production support   Correlate the release cycle (how often the team releases or time to market) and test coverage; better/reliable test coverage and test regression suite will shorten the release cycle. Gather statistic about how much time it takes to refactor an existing feature? Better test coverage will result in faster refactoring  Enforce  Contact the best practices stakeholders and ask for support (I.e.: to push the best practices downward) To fix deadlines (must be adopted by specific dates) Enforce with Gates, checklists, DoD, etc. Set Quality specific OKRs Assign Quality trainings  ",
    "description": "",
    "tags": null,
    "title": "How to promote the Quality Best Practices",
    "uri": "/blogs/how-to-promote-the-quality-best-practices/"
  },
  {
    "content": " Create Hugo Site Install Hugo from [https://gohugo.io/]  From Mac From Windows   Create GitHub Page Create or Clone exiting Hugo repository  Clone the GitHub repo to publish on the hugo site Create a new Hugo site Clone the Theme directory Update the Hugo config.toml file content with team Build Verify if the configuration is good Update the logo Use github documentation repo as content   Try locally Build for GitHub Page Links and references  Training References    \r Create Hugo Site Install Hugo from [https://gohugo.io/] From Mac  brew install hugo\n From Windows  choco install hugo -confirm\n Reference: [https://gohugo.io/getting-started/installing#chocolatey-windows]\nCreate GitHub Page From GitHub.com -\u003e Create a new Repository, e.g., AlainBouchard.github.io\nCreate or Clone exiting Hugo repository In this example, we’ll use project engineering-notes-site\nClone the GitHub repo to publish on the hugo site engineering-notes-site\u003e git clone git@github.com:AlainBouchard/engineering-notes-site.git Create a new Hugo site engineering-notes-site\u003e hugo new site --force engineering-notes-site\u003e ls archetypes config.toml content data layouts public static themes Clone the Theme directory Example of theme, many can be found on Hugo site.\nHugo Theme Source: [https://themes.gohugo.io/themes/hugo-theme-relearn/]\ncd themes themes\u003e git clone git@github.com:McShelby/hugo-theme-relearn.git Update the Hugo config.toml file content with team \u003e vi config.toml baseURL = \"https://AlainBouchard.github.io/\" languageCode = \"en-us\" title = \"Alain Bouchard's Engineering Notes\" theme = \"hugo-theme-relearn\"  [outputs]  home = [ \"HTML\", \"RSS\", \"JSON\"]  [module]   [[module.imports]]  path = \"github.com/alain-bouchard-quality/engineering-notes\"   [[module.imports.mounts]]  source = \"content\"  target = \"content\" Build \u003e hugo -t hugo-theme-relearn Verify if the configuration is good \u003e hugo server Update the logo   Create statics/logo.png\n  Create layouts/partials/logo.html\n\u003cimg src=\"logo.png\"\u003e   Link (git submodule) the documents into the GitHub Page\n\u003e rm -Rf public \u003e git submodule add -b master git@github.com:AlainBouchard/AlainBouchard.github.io.git public \u003e git remote -v   Expect the public directory to get created with the repo content\n  Use github documentation repo as content In document source github repo Source repo example: github.com/AlainBouchard/engineering-notes\n\u003e hugo mod init github.com/AlainBouchard/engineering-notes In hugo project github repo Hugo repo example: github.com/AlainBouchard/engineering-notes-site\n\u003e hugo mod init github.com/AlainBouchard/engineering-notes-site Verify if the module work:\n\u003e hugo mod get github.com/AlainBouchard/engineering-notes Expect go.sum to get created:\ngithub.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6 h1:0s4tNjEN0+jGCkNEObTbnW+akRJD5RdjJ8pPsUU5ROU= github.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6/go.mod h1:Z6BTmpjCul+gI1Za7PY2E0LgyfIJpyeII/zcRE3e654= Expect go.mod to get updated:\nmodule engineering-notes-site  go 1.18  require github.com/AlainBouchard/engineering-notes v0.0.0-20220518202018-bd023ee889d6 // indirect Try locally   Build\n\u003e hugo \u003e hugo server --disableFastRender --ignoreCache   Expect Hugo to run on [http://localhost:1313/]\n  Build for GitHub Page   build for theme\n\u003e hugo -t hugo-theme-relearn \u003e go the `/public` \u003e git status \u003e git add . \u003e git commit -m \"xyz\" \u003e git push   expect the GitHub Page repo to be updated.\n  Links and references Training    (10 min)\nReferences  Master Hugo Modules: Handle Content Or Assets As Modules/ \r Working with Hugo Module Locally \r Hugo Relearn Theme \u003e Content \u003e Pages organization \r Learn Theme for Hugo \u003e Shortcodes \u003e Children \r How to Add Table Of Contents to a Hugo Blog \r  ",
    "description": "",
    "tags": null,
    "title": "Hugo",
    "uri": "/notes/hugo/"
  },
  {
    "content": "IntelliJ cheat sheet  IntelliJ cheat sheet Create Main Method  \r Create Main Method Create the main method: write psvm and select the method to create:\n public static void main(String[] args) {   } ",
    "description": "",
    "tags": null,
    "title": "IntelliJ Cheat Sheet",
    "uri": "/notes/intellij-ide/"
  },
  {
    "content": " Java Object-Oriented Programming Class blueprint  Static vs Non-Static members   Enum blueprint Main class blueprint Princibles  Encapsulation Inheritance Polymorphism Abstraction   Usefull build-in JAVA commands and other information  Method Reference Operator (or ::) The Arbitrary Number of Arguments (or ... as a function argument) Predicate The final keyword Generics    \r Java Object-Oriented Programming Class blueprint // Class blueprint public class MyClass {  // Can have attributes...  private String attribute1; // private attribute member, getter/setter are needed  MyEnum myEnum;  static string staticAttribute1 = \"Static Attribute\"; // belongs to the class  private final Integer myFinalInteger; // can't be modified or be overridden by any subclasses   // Constructor  MyClass() {  // Initiate the instance of MyClass class.  this(\"'this' is used to call alternate constructors from within a constructor\");  // optional code lines...  }   // Constructor  MyClass(String message) {  // Initiate the instance of MyClass class.  }   // Setter for private attribute  public void setAttribute1(attribute1) {  this.attribute1 = attribute1; // note: \"this\" keyword is needed to disambuguate the variable reference.  }   // Getter for private attribute  public String getAttribute1() {  return attribute1;  }   // Methods or behaviours:  void myMethod() {  // no modifier so the method is accessible from its package only.  this.attribute1 = 'xyz';  }   private void myPrivateMethod() {  // private so only accessible from this classs  }   protected void myProtectedMethod() {  // protected method so accessible from this class and sub-classes  }   public void myPublicMethod() {  // public method so accessible from anywhere;  }   // Static method  static void myStaticMethod() {  // A static method does not rely on any non-static attribute or member.  } } Static vs Non-Static members  Non-Static Member: is accessible from an instance and belongs to that instance Static Member: is accessible through the class and belongs to that class  Static members can be accessed using the class name, example:    Enum blueprint // Enum blueprint public enum MyEnum {  CONSTANT1,  CONSTANT2,  ...,  CONSTANTN } Main class blueprint // Main class public class Main {  public static void main(String[] arg) {  MyClass myClass = new MyClass('abc'); // Create an object of type MyClass   myClass.myMethod('xyz'); // Call a method from my object  }  } Princibles Encapsulation  allow us to bind together data and related functionality prevent classes from becoming tightly coupled easily modify the inner workings of one class without affecting the rest of the program restrictions  we need a clear interface between a given class and the rest of the program everything can’t have direct access   make the class attributes hideen from other classes using encapsulation provide a clear interface through public methods benefits  clear pathways for classes to communicate less code changes required for a refactoring change less likely for an attribute to be overwritten with an invalid or null value unexpectedly   Access Modifiers in Java:  private: only visible in class that the member lives in no modifier: only visible in package the member lives in protected: visible to the package and all subclasses public: accessible everywhere within the program    Inheritance  allow us to create class hierarchies Subclass (child class)  inherits properties referred to as the child class   Superclass (parent class)  is inherited from referred to as the parent class   promotes code reusability and scalability leveraging inheritance:  a class can only have one superclass but multiple subclasses if multiple super classes is needed then multilevel inheritance is required    public class MySuperClass {  protected String a1;  private String a2;   MySuperClass(String a1, String a2) {  // Super Class constructor  this.a1 = a1;  this.a2 = a2;  }   public void myMethod() {  // Method of the Super Class  } }  public class MySubClass extends MySuperClass {  MySubClass(String arg1, String arg2) {  super(arg1, arg2); // call superclass with arguments  // Sub Class constructor  }   public String getA1() {  return super.a1; // this.a1 would also work for this Superclass protected variable;  }   @Override  public void myMethod() {  // Override the Super Class Method within the Sub Class  } } Polymorphism  the ability for an object or function to take amny different forms Java supports two types of polymorphism: run time and compile-time polymorphism it helps to reduce complexity and write reusable code  Abstraction   helps us with hide implementation complexity\n  Java supports abstact classes and interfaces\n  helps by fixing inputs and outputs and giving general idea of what the system does\n  an abstract class:\n almost like a template can’t be instencied other classes can extend the abstract calss and implement the appropriate functionality    an example of an abstract class:\npublic abstract class myAbstractClass {  // The class requires the `abstract` keyword since it contains an abstract method.   private final String myString; // final String (aka constant)   protected abstract void myAbstractMethod(); // an abstract method is not implemented! }  public class myOtherClass extends MyAbstractClass {  // variables, constructors... etc.   @Override  protected abstract void myAbstractMethod() {  // The abstract method from the abstract class must be implemented by the sub-class.  } }   an interface is:\n a set of method signatures for to-be-implemented functionality a specification for a set of behaviors without implementation can’t be instencied   public interface MyInterface {  Long myMethod1(); // No method implementation  void myMethod2();  }   public class myClassImplementingMyInterface implements MyInterface {  @Override  public Long myMethod1() {  // the implementation for myMethod1 method  }   @Override  public void myMethod2() {  // the implementation for myMethod2 method  }  }   Consider using abstract classes if any of these statements apply to your situation:\n In the java application, there are some related classes that need to share some lines of code then you can put these lines of code within the abstract class and this abstract class should be extended by all these related classes. You can define the non-static or non-final field(s) in the abstract class so that via a method you can access and modify the state of the Object to which they belong. You can expect that the classes that extend an abstract class have many common methods or fields, or require access modifiers other than public (such as protected and private).    Consider using interfaces if any of these statements apply to your situation:\n It is a total abstraction, All methods declared within an interface must be implemented by the class(es) that implements this interface. A class can implement more than one interface. It is called multiple inheritances. You want to specify the behaviour of a particular data type, but not concerned about who implements its behaviour.    Usefull build-in JAVA commands and other information  System.out.println(“string…”);  Method Reference Operator (or ::)   a method reference operator (or ::) is used to call a method by referring to it with the help of its class directly\n like using a lambda expression, example:   // Get the stream  Stream\u003cString\u003e stream  = Stream.of(\"Geeks\", \"For\",  \"Geeks\", \"A\",  \"Computer\",  \"Portal\");   // Print the stream using lambda method:  stream.forEach(s -\u003e System.out.println(s));   // Print the stream using double colon operator  stream.forEach(System.out::println);   // Both lambda and :: will do the same thing.   The Arbitrary Number of Arguments (or ... as a function argument)  it means that zero or more String objects (or a single array of them) may be passed as the argument(s) for that method Reference : [http://java.sun.com/docs/books/tutorial/java/javaOO/arguments.html#varargs] important note:  the argument(s) passed in this way is always an array - even if there’s just one. Make sure you treat it that way in the method body the argument that gets the ... must be the last in the method signature. So, myMethod(int i, String… strings) is okay, but myMethod(String… strings, int i) is not okay   example:   public static int myFunction (int ... a) {  int sum = 0;  for (int i : a)  sum += i;   return sum;  }  public static void main( String args[] ) {  int ans = myFunction(1,1,1); // could have any number of arguments  System.out.println( \"Result is \"+ ans );  } Predicate   a Predicate in general meaning is a statement about something that is either true or false. In programming, predicates represent single argument functions that return a boolean value\n  example:\n @FunctionalInterface  public interface Predicate\u003cT\u003e {  boolean test(T t);  }   An example with filter() since it does accept a Predicate as parameter:\n // With lambda function:  List\u003cInteger\u003e list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  List\u003cInteger\u003e collect = list.stream().filter(x -\u003e x \u003e 5).collect(Collectors.toList());  System.out.println(collect); // [6, 7, 8, 9, 10]   // With predicate:  Predicate\u003cInteger\u003e noGreaterThan5 = x -\u003e x \u003e 5;  List\u003cInteger\u003e list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  List\u003cInteger\u003e collect = list.stream().filter(noGreaterThan5).collect(Collectors.toList());  System.out.println(collect); // [6, 7, 8, 9, 10]   More examples with Java 8 Predicate Examples\n  The final keyword  Java final keyword is a non-access specifier that is used to restrict a class, variable, and method. If we initialize a variable with the final keyword, then we cannot modify its value if we declare a method as final, then it cannot be overridden by any subclasses if we declare a class as final, we restrict the other classes to inherit or extend it example:  final variables: to create constants final classes: to prevent inheritance final methods: to prevent method overriding    Generics   using generics enable types (classes and interfaces) to be parameters when defining classes, interfaces and methods.\n  using generics give many benefits over using non-generic code\n  stronger type checks at compile time:\n Java compiler applies strong type checking to generic code and issues errors if the code violates type safety. Fixing compile-time errors is easier than fixing runtime errors, which can be difficult to find example:   // Without Generics  List list = new ArrayList();  list.add(\"hello\");   // With Generics  List\u003cInteger\u003e list = new ArrayList\u003cInteger\u003e();  list.add(\"hello\"); // will not compile   enabling programmers to implement generic algorithms\n by using generics, programmers can implement generic algorithms that work on collections of different types, can be customized, and are type safe and easier to read.    elimination of casts\n example:   // Without Generics:  List list = new ArrayList();  list.add(\"hello\");  String s = (String) list.get(0); // Need to cast return value to String   // With Generics:  List\u003cString\u003e list = new ArrayList\u003cString\u003e();  list.add(\"hello\");  String s = list.get(0); // no cast needed   Generics type parameters:\n T: Type E: Element K: Key (used in Map) N: Number V: Value (used in Map)    Reference: [https://www.journaldev.com/1663/java-generics-example-method-class-interface]\n  ",
    "description": "",
    "tags": null,
    "title": "Java Object-Oriented Programming",
    "uri": "/notes/java-object-oriented-programming/"
  },
  {
    "content": " Java Spring Boot 2 Terminology Getting Started  Why Spring Boot? Spring Initializr Inversion of Control Proxies   Data Access in Spring  Spring Data Embeedded DB with Spring Boot Repository with Spring Data Using remote database   Service Tier  Utilizing IoC Service abstraction Spring Service Object   Web pages with Spring  Controller    \r Java Spring Boot 2 Terminology  POJO : Plain Old Java Object (may have more that setters/getters in Spring world) Java Beans : Simple objects with only setters/getters Spring Beans : POJOs confiugered in the application context DTO : Data Transfer Objects are Java Beans used to move state between layers IOC : Inversion Of Control  IoC provides mechanism of dependency injection Application Context wraps the Bean Factory which serves the beans at the runtime of the application Spring Boot provides auto-configuration of the Application Context    Getting Started Why Spring Boot?  Support rapid development Remove boilerplate of application setup Many uses Cloud Native support but also traditional  Key Aspects  Embedded tomacat (or others) Auto-configuration of Application Context Automatic Servlet Mappings Database support and Hibermate/JPA dialect Automatic Controller Mappings  Auto Config  Default opiniated configuration Very to override defaults Configuration on presence  Spring Initializr  start.spring.io Spring Boot: pick latest released version (ie. 2.5.6)  Packaging: Jar   Add Dependencies:  Pring Web TBD…   Generate  Now it can build and run as is:\n java -jar target/xyz-0.0.1-SNAPSHOT.jar\n Use Chrome: localhost:8080\nInversion of Control  Container mainains your class dependencies Objects injected at runtime or startup time An object accepts the dependencies for construction instead of constructing them  Spring IoC  Bean Factory Application Context References Analysis of construction order  Proxies  Beans in Bean Faactory are proxied Annitations drive proxies Annitations are easy extension points, for your own abstracts too Method calling order matters  Data Access in Spring Spring Data  Provides a common set of interfaces Provides a common naming convention Provides aspected behavior Provides Repository and Data Mapping convention  Benefits of Spring Data  Remove boilerplate code Allows for swapping datasources easier Allows to focus on buisiness logic  Key Components  Repository Interface Entity Object DataSource no accessed directly  Embeedded DB with Spring Boot Needed dependencies:\norg.springframework.boot:spring-boot-starter-data-jpa com.h2database:h2 Set application.properties:\n logging.level.org.springframework.jdbc.datasource.init.ScriptUtils=debug : by default is set to info spring.jpa.hibernate.ddl-auto=none : don’t create schema, and just connect to DB  Repository with Spring Data   Java Persistence API (or JPA)\n Mapping Java objects to DB tables and vice versa is called Object-relational Mapping (ORM) JPA permits the developer to works directly with objects rather tahn with SQL statements Based on Annotations    Entity\n A class which should be persisted in a database it must be annotated with javax.persistence.Entity JPA uses a database table for every entity Persisted instances of the class will be represented as one row in the table JPA allows to auto-generate the primary key in the database via the @GeneratedValue annotation By default, the table name corresponds to the class name. You can change this with the addition to the annotation @Table(name=\"NEWTABLENAME\")     Code Example @Entity @Table(name=\"ROOM\") public class Room {  @Id  @GeneratedValue(strategy = GenerationType.AUTO)  @Column(name=\"ROOM_ID\")  private long id;  @Column(name=\"NAME\")  private String name;  @Column(name=\"ROOM_NUMBER\")  private String roomNumber;  @Column(name=\"BED_INFO\")  private String bedInfo;   public long getId() {  return id;  }   public void setId(long id) {  this.id = id;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public String getRoomNumber() {  return roomNumber;  }   public void setRoomNumber(String roomNumber) {  this.roomNumber = roomNumber;  }   public String getBedInfo() {  return bedInfo;  }   public void setBedInfo(String bedInfo) {  this.bedInfo = bedInfo;  }   @Override  public String toString() {  return \"Room{\" +  \"id=\" + id +  \", name='\" + name + '\\'' +  \", roomNumber='\" + roomNumber + '\\'' +  \", bedInfo='\" + bedInfo + '\\'' +  '}';  } }   Create a CrudRepository interface for the created Entity:   Code Example @Repository public interface RoomRepository extends CrudRepository\u003cRoom, Long\u003e {  // Room is the Entity class  // Long is the ID type }    Create a Component Event:\n A @Component Annotation is automatically picked by Spring     Code Example @Component public class AppStartupEvent implements ApplicationListener\u003cApplicationReadyEvent\u003e {  private final RoomRepository roomRepository;   public AppStartupEvent(RoomRepository roomRepository) {  this.roomRepository = roomRepository;  }   @Override  public void onApplicationEvent(ApplicationReadyEvent event) {  Iterable\u003cRoom\u003e rooms = this.roomRepository.findAll();  rooms.forEach(System.out::println);  } }  Using remote database Replace the H2 database for an other database, example a PostgreSQL:\ndependencies: org.postgresql:postgresql In application.properties:\nspring.jpa.database=postgresql spring.datasource.url=jdbc:postgresql://localhost:5432/dev spring.datasource.username=postgres spring.datasource.password=postgres Service Tier Utilizing IoC Why use IoC?\n Allows you to focus on contracts Develoip business code only, leave constuction to the container Build intermediate abstractions Produce clean code  Srping and IoC:\n IoC container is configured by developer Spring maintains handles to objects constucted at startup Spring serves singletons to classes during construction Spring maintains lifecycle of beans Developer only accesses the application context  Service abstraction Why building Service Abstractions:\n Encapsulate layers? Abstract 3rd partys APIs Simplify implementations Swap out implementations as runtime (ie. factory pattern)  How to build one?\n Define our interface (or class) Create the API Inject the dependencies Annotate or configure (classes) Code the implemantation  Spring Service Object  We mark beans with @Service to indicate that they’re holding the business logic. Besides being used in the service layer, there isn’t any other special use for this annotation. Starting with Spring 2.5, the framework introduced annotations-driven Dependency Injection. The main annotation of this feature is @Autowired. It allows Spring to resolve and inject collaborating beans into our bean.  Using @Autowired or either properties or setters/getters isn’t a good practice or easy to test; Use final properties with constructors to have immutable object. If more than one constructor is defined then using @Autowired on the default one will make Spring to use it.     Code Example @Service public class ReservationService {  private final RoomRepository roomRepository;  private final GuestRepository guestRepository;  private final ReservationRepository reservationRepository;   @Autowired // optional if only one constructor  public ReservationService(RoomRepository roomRepository, GuestRepository guestRepository, ReservationRepository reservationRepository) {  this.roomRepository = roomRepository;  this.guestRepository = guestRepository;  this.reservationRepository = reservationRepository;  }   // Business logic here... }  Web pages with Spring Controller Model View Controller (or MVC)\n Fundamental pattern for Web application development The Model is the data The View is the visual display that is populated The Controller wires the view with the model  Spring Controller\n Spring bean Annotated for the servlet mapping Responds to incoming web requests Output a view or raw data  Template Engines\n Spring supports several Thymeleaf most popular Provides a DSL for HTML leaving raw html documents Placeholders for dynamic data Rendiring engin allows for final products   Code Example @Controller @RequestMapping(\"/reservations\") public class RoomReservationController {   private final DateUtils dateUtils;  private final ReservationService reservationService;   public RoomReservationController(DateUtils dateUtils, ReservationService reservationService) {  this.dateUtils = dateUtils;  this.reservationService = reservationService;  }   @RequestMapping(method = RequestMethod.GET)  public String getReservations(@RequestParam(value=\"date\", required=false) String dateString, Model model){  Date date = this.dateUtils.createDateFromDateString(dateString);  List\u003cRoomReservation\u003e roomReservations = this.reservationService.getRoomReservationsForDate(date);  model.addAttribute(\"roomReservations\", roomReservations);  return \"roomres\";  } }  Can use Thymeleaf to create HTML pages.\n Add the web page to src/main/resources/templates  ",
    "description": "",
    "tags": null,
    "title": "Java Spring Boot 2",
    "uri": "/notes/java-spring-boot2/"
  },
  {
    "content": " Java with Rest-Assured Pattern API response Deserialize a response Tools  \r Java with Rest-Assured  Rest-Assured link: [https://rest-assured.io] can get latest version: [https://mvnrepository.com] jackson databind package can be used for data-binding hamcrest package can be used for matchers  org.hamcrest.Matchers.*    Pattern   using the Given, When and Then pattern\n the Given specify prerequisites the When describe the action to take the Then describe the expected result using JUnit 5:   @Test  public void getTest() {  String endpoint = \"http://localhost:8888/a/b/c\";  var response = given(). // using easy to read format for doc only  queryParam(\"id\", \"2\").  when().  get(endpoint).  then();  }   @Test  public void postTest() {  String endpoint = \"http://localhost:8888/a/b/c\";  String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\"   var response = given().body(body).when().post(endpoint).then();  }   @Test  public void putTest() {  String endpoint = \"http://localhost:8888/a/b/c\";  String body = \"\"\" { \"key1\": \"value1\", \"key2\": \"value2\" } \"\"\"   var response = given().body(body).when().put(endpoint).then();  }   @Test  public void deleteTest() {  String endpoint = \"http://localhost:8888/a/b/c\";  String body = \"\"\" { \"key1\": \"value1\" } \"\"\"  var response = given().body(body).when().delete(endpoint).then();  }   API response   validate the status code\n assertThat example:   @Test  public void getTest() {  String endpoint = \"http://localhost:8888/a/b/c\";   given().queryParam(\"key\", \"value\")  .when().get(endpoint)  .then().assertThat()  .statusCode(200) // check status code is OK/200  .body(\"key\", equalTo(\"value\")) // check for response body key = value  .body(\"records.size()\", greaterThan(0)) // check for response body record array to have 1+ items  .body(\"records.id\", everyItem(notNullValue())) // make sure each records.id item from the array is not null  .body(\"records.id[0]\", equalTo(8)) // make sure first records.id item = 0  .header(\"Content-Type\", equalTo(\"application/json\")); // verify the headers content-type field  }   Deserialize a response   a class can be used to deserialize a response\n example:   @Test  public void deserializeTest() {  String endpoint = \"http://localhost:8888/a/b/c\";  MyResponseClass request = new MyResponseClass(1, 2, 3, \"value\");   MyResponseClass response =  given()  .queryParam(\"key\",\"value\")  .when()  .get(endpoint)  .as(MyResponseClass.class);   assertThat(response, samePropertyValuesAs(request)); // comparing every property of the classes  }   Tools  response.log().body() -\u003e print the response to console  ",
    "description": "",
    "tags": null,
    "title": "Java with Rest-Assured",
    "uri": "/notes/java-rest-assured/"
  },
  {
    "content": "@ToString The Class @ToString tag will replace the Overriding ToString method automatically.\n@AllArgsConstructor and @NoArgsConstructor The Class @AllArgsConstructor and @NoArgsConstructor tags will automatically replace the constructors.\n@EqualsAndHashCode The Class @EqualsAndHashCode tag will allow us to compare 2 objects.\n@Log4J The Class @Log4J tag can replace the logger initiator in the class. The LOGGER can be replaced by log.\n@Data The Class @Data tag will replace the @ToString, @RequiredArgsConstructor, @EqualsAndHashCode, @Setter (for non-final attributes) and the @Getter.\n",
    "description": "",
    "tags": null,
    "title": "Lombok Java Library",
    "uri": "/notes/java-lombok/"
  },
  {
    "content": " Python and PyTest PyTest  Why PyTest? requirements.txt content for pytest Run a test pytest.ini configuration file example   Tox  tox.ini Test file skeleton Test class skeleton Fixtures   Python  Class file skeleton Commonly used functions and examples    \r Python and PyTest Working with multiple python versions may cause issues and confusion. It is recommended to specify the python version to use.\nWorking with python 3.9:\npy -3.9 -m \u003ccommand\u003e It is recommended to upgrade the pip version:\npy -3.9 -m pip install --upgrade pip It is recommended to install tox, which is a virtual environment (venv) manager for Python.\npy -3.9 -m pip install tox To run tests with tox, the following example assumes that pytest has been configured by the tox.ini commands parameter:\npy -3.9 -m tox --recursive -- \u003cpytest parameters\u003e PyTest Why PyTest?  allow to run a standalone test function as its own case easy to read syntax, allowing you to use the standard assert method powerful CLI automates test setup, teardown, and common test scenarios (uses fixtures) Great to use with CI tools like Jenkins, Travis, Circle CI, etc. actively maintained with participatory open-source community  requirements.txt content for pytest coverage===xxx pytest===xxx pytest-cov===xxx pytest-flakes===xxx pytest-pep8===xxx pytest-pythonpath==xxx docker  pytest-flakes will make pytest use PyFlake and Flake8. It will make pytest and python use a Linter and code style checker.  Run a test Get pytest help:\npytest -h Run all tests:\npytest Run test using keyword for filename\npytest -k \u003ctest_name_keyword\u003e Explore test coverage of a script - requires the pytest-cov package to be installed:\npytest --cov scripts pytest.ini configuration file example [pytest] # Configure the logging within PyTest (the configuration won't work if configured in test files) log_cli = 1 log_cli_level = WARNING log_cli_format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s) log_cli_date_format=%Y-%m-%d %H:%M:%S  # Filter Warnings filterwarnings =ignore::FutureWarning  # Uses classes with prefix Test as test files python_files = Test*.py Tox Tox aims to automate and standardize testing in Python. It is part of a larger vision of easing the packaging, testing and release process of Python software.\ntox.ini Tox may be used along with a tox.ini configuration file.\n[framework] files = tests coverages = --cov=src  [tox] envlist = py3.9 skip_missing_interpreters = false skipsdist = true toxworkdir = tmp  [flake8] max-line-length = 159 # E501: Ignore max line length # ignore = E501  [pytest] norecursedirs = .cache tmp  # pytest-spec configuration spec_header_format = {module_path}: spec_test_format = {result} {name}  [testenv] deps = -rrequirements.txt  # Only forward the environment variables with the following prefix. passenv = PYTHON_SANDBOX_* commands =flake8 src tests --exclude=__init__.py pytest -p no:cacheprovider --spec --durations=5 --cov-config .coveragerc --cov-report term-missing {posargs} {[framework]coverages} {[framework]files} Test file skeleton from path.to.class.to.test import ClassName from pytest # to use pytest Context Manager   def test_name():  obj = ClassName(\"value1\", \"value2\")  assert obj.value1 == \"value1\"  assert obj.value2 == \"value2\"  def test_with_exception_context_manager():  with pytest.raises(ValueError) as ex:  # Code that will raise an exception.  obj = ClassName(\"value1\", \"value2\")  obj.raises_exception()   assert str(ex.value) == \"this is an exception!\" Test class skeleton The Test Class will work like the fixtures from the test file.\n# conftest.py import pytest import logging   @pytest.fixture(scope=\"session\", autouse=True) def set_logging() -\u003e None:  logging.info(\"set_logging on conftest.py\")   # TestExample.py import logging   class TestExample:  @classmethod  def setup_class(cls):  logging.info(\"starting class: {}execution\".format(cls.__name__))   @classmethod  def teardown_class(cls):  logging.info(\"starting class: {}execution\".format(cls.__name__))   def setup_method(self, method):  logging.info(\"starting execution of tc: {}\".format(method.__name__))   def teardown_method(self, method):  logging.info(\"starting execution of tc: {}\".format(method.__name__))   def test_tc1(self):  logging.info(\"running tc1\")  assert True   def test_tc2(self):  logging.info(\"running tc2\")  assert True The output of this example Test Class will be:\n============================= test session starts ============================= collecting ... collected 2 items TestExample.py::TestExample::test_tc1 ------------------------------- live log setup -------------------------------- 2022-07-18 12:57:45 [ INFO] set_logging on conftest.py (conftest.py:7) 2022-07-18 12:57:45 [ INFO] starting class: TestExample execution (TestExample.py:7) 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc1 (TestExample.py:14) -------------------------------- live log call -------------------------------- 2022-07-18 12:57:45 [ INFO] running tc1 (TestExample.py:20) PASSED [ 50%] ------------------------------ live log teardown ------------------------------ 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc1 (TestExample.py:17) TestExample.py::TestExample::test_tc2 ------------------------------- live log setup -------------------------------- 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc2 (TestExample.py:14) -------------------------------- live log call -------------------------------- 2022-07-18 12:57:45 [ INFO] running tc2 (TestExample.py:24) PASSED [100%] ------------------------------ live log teardown ------------------------------ 2022-07-18 12:57:45 [ INFO] starting execution of tc: test_tc2 (TestExample.py:17) 2022-07-18 12:57:45 [ INFO] starting class: TestExample execution (TestExample.py:11) ============================== 2 passed in 0.02s ============================== Process finished with exit code 0 A session configuration can be done in the conftest.py as shown in the example. It will be run once only in the session setup.\nFixtures A test fixture is a concept used in both electronics and software. It’s a piece of software or device that sets up a system to satisfy certain preconditions of the process. Its biggest advantage is that it provides consistent results so that the test results can be repeatable. Examples of fixtures could be loading a test set to the database, reading a configuration file, setting up environment variables, etc.\nA pytest fixture has a specific scope. By default, the scope is a function. Pytest fixtures have five different scopes: function, class, module, package, and session. The scope basically controls how often each fixture will be executed.\nOrder of priority:\n session (higher priority) package module class function (lower priority)  Function THe default scope is function: `scope=“function” and therefore may be omitted.\nimport pytest from datetime import datetime  @pytest.fixture() def only_used_once():  with open(\"app.json\") as f:  config = json.load(f)  return config  @pytest.fixture() def light_operation():  return \"I'm a constant\"  @pytest.fixture() def need_different_value_each_time():  return datetime.now() Class The scope=\"class\" run before any function or test of the Test Class.\n@pytest.fixture(scope=\"class\") def dummy_data(request):  request.cls.num1 = 10  request.cls.num2 = 20  logging.info(\"Execute fixture\")  @pytest.mark.usefixtures(\"dummy_data\") class TestCalculatorClass:  def test_distance(self):  logging.info(\"Test distance function\")  assert distance(self.num1, self.num2) == 10   def test_sum_of_square(self):  logging.info(\"Test sum of square function\")  assert sum_of_square(self.num1, self.num2) == 500 Special usage of the keyword yield in a fixture: the code before the yield keyword will be executed before the test functions of the Test Class while the code after the yield keyword will be executed after the test functions of the Test Class.\n@pytest.fixture(scope=\"class\") def prepare_db(request):  # pseudo code  connection = db.create_connection()  request.cls.connection = connection  yield  connection = db.close()  @pytest.mark.usefixtures(\"prepare_db\") class TestDBClass:  def test_query1(self):  assert self.connection.execute(\"..\") == \"...\"   def test_query2(self):  assert self.connection.execute(\"..\") == \"...\" Module and package The scope=\"module\" runs the fixture per module while the scope=\"package\" runs by package. The scope module is usually used more often than the scope package. The difference between scope function and scope module is that the scope module will only be run once, even if used in many functions in the module.\n@pytest.fixture(scope=\"module\") def read_config():  with open(\"app.json\") as f:  config = json.load(f)  logging.info(\"Read config\")  return config  def test1(read_config):  logging.info(\"Test function 1\")  assert read_config == {}  def test2(read_config):  logging.info(\"Test function 2\")  assert read_config == {} Session The scope=\"session\" is only run once every time pytest is run. A per-directory conftest.py file will be executed once per pytest execution.\n# test/conftest.py @pytest.fixture(scope=\"session\") def read_config():  with open(\"app.json\") as f:  config = json.load(f)  logging.info(\"Read config\")  return config  # test/test_code1.py def test1(read_config):  logging.info(\"Test function 1\")  assert read_config == {}  def test2(read_config):  logging.info(\"Test function 2\")  assert read_config == {}  # test/test_code2.py def test3(read_config):  logging.info(\"Test function 3\")  assert read_config == {}  def test4(read_config):  logging.info(\"Test function 4\")  assert read_config == {} Using conftest.py for common functions:\n stores common utility test fixtures and extension code often referred to as hooks pytest collects the fixtures in this file so they are globally accessible within the testing directory it must be placed under your /tests directory good practice to cross-reference this file when reading a testing suite  conftest.py modularization It is possible to modularize the conftest.py file when it is getting too big.\n# referring to modules: # tests/utils/db.py # tests/utils/network.py  # in conftest.py pytest_plugins = [  \"tests.utils.db\",  \"tests.utils.network\" ] The autouse=True fixtures must stay in the conftest.py file.\nAutouse The fixture parameter autouse=True will make the fixture used automatically even if the fixture isn’t called by the test function.\n@pytest.fixture(autouse=True) def function_autouse():  logging.info(\"scope function with autouse\")  def test_autouse():  assert True Parametrize The [pytest.mark.parametrize] fixture allows the user to run the same test, multiple times, by modifying the input parameters.\n@pytest.mark.parametrize(\"num, output\",[(1,11),(2,22),(3,35),(4,44)]) def test_multiplication_11(num, output):  assert 11*num == output Python Class file skeleton class ClassName():  \"\"\" This is a multi-line comment (used for header in this example) This is a second line of comment.. \"\"\"  def __init__(self, var1: str, var2: str): # the \"var1: str\" format will require the var1 to be a string type  self._var1 = var1 # the _ is to make the variable \"protected\"  self._var2 = var2   @property # Using property decorator as a getter function  def var1(self) -\u003e str: # the -\u003e specifies the return type  return self._var1   @var1.setter # using setter decorator for setter function  def var1(self, value: str):  self.var1 = value   def raises_exception():  raise ValueError(\"this is an exception!\") It is possible to use decorators for getters and setters.\nCommonly used functions and examples Verify variable type if not isinstance(variable, str):  raise ValueError(\"wrong type!\") Open file with context manager with open(\"test.txt\", 'w', encoding='utf-8') as f:  f.write(\"my first file\\n\")  f.write(\"This file\\n\\n\")  f.write(\"contains three lines\\n\") Iterate a list data = [\"abc\", \"def\", \"ghi\"]  for each_data in data:  assert each_data in data try except try:  # Some Code except:  # Executed if error in the  # try block else:  # execute if no exception finally:  # Some code .....(always executed) ",
    "description": "",
    "tags": null,
    "title": "Python and PyTest",
    "uri": "/notes/python-and-pytest/"
  },
  {
    "content": " Role Strategic Tactics  \r Role The architect is responsible for two main types of high-level activities:\n Strategic (foundations): take care of the technical vision in the medium to long term (one, two years or even more) Tactics (operations): technical support for teams, problem solving, proof of concept, etc.  Strategic  Define the “Quality Vision”, guidelines, technical documents (ADR, technical designs, terminology/technical glossary, etc.) while taking into account security rules, PII, etc Evaluate technologies: research, comparisons, proof of concepts, publications, etc Promote best practices for the chosen technologies Mentoring and coaching Promote quality during the six stages of software development, SDLC or “Software Development Life Cycle”: “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” - \u003e “Deployment” Assist or lead working groups for specific topics, define the purpose and deliverables, periodic and monthly events Attend or lead workshops for specific topics, define the goal and deliverables, event over a few hours or a few days Audit of the services for test coverage and correlate the data with the problematic services in production (Data-Driven decisions) Analyze the current testing strategy, test characterization, define a plan, promote good testing practices, follow the test pyramid:  Unit (solitary and sociable) testing Component (in-process and out-of-process) testing Contract testing Integration (narrow and broad) testing UI End-to-end (E2E) and API E2E (Subcutaneous) User journey tests (and Synthetic Traffic tests) Non-functional tests: load, scalability, webvital metrics LCP, chaos/robustness, etc   Alignment on a test strategy with the deployment procedure (CI/CD) and a branching procedure (gitflow, githubflow, etc), blue/green deployment, etc Participates in the recruitment process for the “quality” aspects  Tactics  Apply the test strategy: develop test projects, add the projects to the process (build, CI/CD, cluster/kubernetes. etc), document, etc Support test development: support QAs and developers who add tests, PR reviews, etc Add test coverage: JavaScript (e.g., cypress.io, etc), python/pytest, java/rest-assured, etc Addition of Synthetic Traffic tests and verification of customer flows and scenarios (i.e., customer flows) Analyze system performance (e.g., load tests) and according to defined standards (i.e., “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (i.e., Largest Contentful Paint) (e.g., “a page should be considered usable in less than 2.5 seconds”) Using observation tools to understand issues: datadoghq, splunk/signalfx, prometheus, ​​grafana, etc Collaborates horizontally with the different “domaines” of the company and suppliers to improve the overall quality of the company  ",
    "description": "",
    "tags": null,
    "title": "Quality Architect Role",
    "uri": "/blogs/quality-architect-role/"
  },
  {
    "content": " Role Stratégique Tactique  \r Role L’architecte est responsable de deux grands types d’activités à haut niveau:\n Stratégique (fondations): s’occuper de la vision technique à moyen à long terme (un, deux ans ou même plus) ; Tactique (opérations): support technique des équipes, résolution de problèmes, preuve de concept, etc ;  Stratégique  Définir la “Vision Qualité”, les guides (ou “guidelines”), documents techniques (ADR, Technical Design, Terminologie/glossaire technique, etc.) tout en tenant compte des règles de sécuritées, PII, etc. ; Évaluer les technologies : Recherches, comparatifs, preuves de concepts, publications, etc.; Promouvoir les bonnes pratiques pour les technologies choisies; Mentorat et “Coaching”; Promouvoir la qualité lors des six étapes du développement logiciel, SDLC ou “Software Development Life Cycle” : “Planning” -\u003e “Defining” -\u003e “Designing” -\u003e “Building” -\u003e “Testing” -\u003e “Deployment” ; Assister ou diriger des groupes de travail (ou “Work Groups”) pour des sujets précis, définir le but et les livrables, événements périodiques et sur plusieurs mois ; Assister ou diriger des ateliers de travail (ou “Workshops”) pour des sujets précis, définir le but et les livrables, événement sur quelques heures ou quelques jours ; Audit de la couverture de tests et corrélation des données avec les services problématiques en production (décision basée sur les données ou “Data-Driven”); Analyser la stratégie de tests actuelle, caractérisation des tests, définir un plan, promouvoir les bonnes pratiques de tests, la pyramide de tests “pratique” (en anglais pour faciliter) :  Unit (“solitary” et “sociable”) testing ; Component (“in-process” et “out-of-process”) testing ; Contract testing ; Integration (“narrow” et “broad”) testing ; UI End-to-end (E2E) et API E2E (ou Subcutaneous) ; User journey tests (and Synthetic Traffic tests) ; Non functional tests : load, scalability, webvital metrics LCP, chaos/robustness, etc. ;   Création de projets archétypes et création de matériel de formation (on-demand training) suivi de projets pilotes ou réels, écriture de tests, documentation, etc. ; Alignment sur une stratégie de tests avec la procédure de déploiement (CI/CD) avec une procédure de “branching” (examples : gitflow ou trunk-based workflows), blue/green deployment, smoke tests, etc. ; Participe au processus de recrutement pour le volet “qualité” ;  Tactique  Appliquer la stratégie de tests : développer des projets tests, ajouter les projets au processus (build, CI/CD, cluster/kubernetes. etc), documenter, etc. ; Supporter le développement de tests: supporter les QAs et développeurs qui ajoutent des tests, PR reviews, etc. ; Ajouter une couverture de tests : javascript (exemple : cypress.io), python/pytest, java/rest-assured, etc. ; Ajout de “tests de trafic synthétique” et vérification des flux et scénarios clients (ou “customer flows”) ; Analyser les performances systèmes (example: load tests) et selon les standards définis (example: “Given the data load X and the number of users Y, an API should respond within Z seconds”), LCP (ou “Largest Contentful Paint”, example : “une page devrait être considérée utilisable en moins de 2.5 secondes”) ; Utilisation d’outils d’observation pour comprendre les problèmes : datadoghq, splunk/signalfx, prometheus, grafana, etc. ; Collabore horizontalement avec les différents “domaines” de l’entreprises et des fournisseurs pour améliorer la qualité globale de l’entreprise ;  ",
    "description": "",
    "tags": null,
    "title": "Rôle de l'Architecte Qualité (FR)",
    "uri": "/blogs/role-architecte-qualite/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  }
]
